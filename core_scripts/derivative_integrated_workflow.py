#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ´¾ç”Ÿç‰ˆãƒ¡ãƒ¼ãƒ«ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- å…ƒã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ç‹¬ç«‹ã—ãŸæ´¾ç”Ÿç‰ˆ
- æ–°æ©Ÿèƒ½é–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆç”¨é€”
- ãƒ¡ãƒ¼ãƒ«æŠ½å‡º â†’ ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æ â†’ ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã®çµ±åˆå‡¦ç†
- å…ƒã‚·ã‚¹ãƒ†ãƒ ã¨ã®å¹²æ¸‰ã‚’å®Œå…¨ã«å›é¿
"""

import subprocess
import sys
import os
import logging
import argparse
import time
import csv
import glob
import datetime
import pandas as pd
import tempfile
import shutil
import gc
import psutil
import threading

# [DERIVATIVE] æ´¾ç”Ÿç‰ˆä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå®‰å®šåŒ–æ©Ÿèƒ½
def stabilize_working_directory():
    """æ´¾ç”Ÿç‰ˆä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å®‰å®šåŒ–ã™ã‚‹"""
    target_dir = r"c:\Users\Raxus\Desktop\email_extraction_project\email_marketing_derivative_system"
    current_dir = os.getcwd()

    # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒç•°å¸¸ãªå ´åˆã®ä¿®å¾©
    if current_dir == "//" or not os.path.exists(current_dir) or "email_marketing_derivative_system" not in current_dir:
        print(f"[WARNING] Derivative working directory issue detected: {current_dir}")
        print(f"[REPAIR] Attempting repair: {target_dir}")

        try:
            if os.path.exists(target_dir):
                os.chdir(target_dir)
                new_dir = os.getcwd()
                print(f"[SUCCESS] Derivative working directory repaired: {new_dir}")
                return True
            else:
                print(f"[ERROR] Derivative target directory not found: {target_dir}")
                return False
        except Exception as e:
            print(f"[ERROR] Derivative working directory repair failed: {e}")
            return False
    else:
        print(f"[OK] Derivative working directory is normal: {current_dir}")
        return True

# æ´¾ç”Ÿç‰ˆä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå®‰å®šåŒ–ã‚’æœ€åˆã«å®Ÿè¡Œ
if not stabilize_working_directory():
    print("ğŸ’¥ æ´¾ç”Ÿç‰ˆä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä¿®å¾©ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ å†èµ·å‹•ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
    sys.exit(1)

# æ´¾ç”Ÿç‰ˆãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
os.makedirs("logs/derivative_dashboard", exist_ok=True)

# æ´¾ç”Ÿç‰ˆãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("logs/derivative_dashboard/derivative_integrated_workflow.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger('derivative_integrated_workflow')

# æ´¾ç”Ÿç‰ˆã‚·ã‚¹ãƒ†ãƒ ç”¨ã®è¨­å®š
INPUT_FILE = 'data/derivative_input.csv'
SCRIPTS = {
    'email_extraction': 'core_scripts/derivative_email_extractor.py',
    'website_analysis': 'core_scripts/derivative_website_analyzer.py',
    'email_sending': 'core_scripts/derivative_email_sender.py',
    'bounce_processing': 'core_scripts/derivative_bounce_processor.py'
}

# ãƒ¡ãƒ¢ãƒªç›£è¦–ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ©Ÿèƒ½
class MemoryManager:
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, max_memory_mb=2048):
        self.max_memory_mb = max_memory_mb
        self.process = psutil.Process()
        self.initial_memory = self.get_memory_usage()

    def get_memory_usage(self):
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’MBã§å–å¾—"""
        try:
            memory_info = self.process.memory_info()
            return memory_info.rss / 1024 / 1024  # MB
        except:
            return 0

    def check_memory_usage(self):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å¿…è¦ã«å¿œã˜ã¦è­¦å‘Š"""
        current_memory = self.get_memory_usage()
        memory_increase = current_memory - self.initial_memory

        logger.info(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {current_memory:.1f}MB (å¢—åŠ : +{memory_increase:.1f}MB)")

        if current_memory > self.max_memory_mb:
            logger.warning(f"âš ï¸ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒä¸Šé™ã‚’è¶…ãˆã¾ã—ãŸ: {current_memory:.1f}MB > {self.max_memory_mb}MB")
            return False

        return True

    def cleanup_memory(self):
        """ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ"""
        before_memory = self.get_memory_usage()

        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        collected = gc.collect()

        after_memory = self.get_memory_usage()
        freed_memory = before_memory - after_memory

        logger.info(f"ğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {collected}å€‹ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å›å, {freed_memory:.1f}MBè§£æ”¾")

        return freed_memory > 0

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
memory_manager = MemoryManager(max_memory_mb=2048)  # 2GBåˆ¶é™ã«å¤‰æ›´ï¼ˆå®‰å®šæ€§é‡è¦–ï¼‰

# æ”¹å–„ã•ã‚ŒãŸãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã‚¯ãƒ©ã‚¹
class EnhancedProcessMonitor:
    """æ”¹å–„ã•ã‚ŒãŸãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã¨ã‚µã‚¤ãƒ¬ãƒ³ãƒˆå¤±æ•—æ¤œå‡º"""

    def __init__(self):
        self.start_time = None
        self.last_output_time = None
        self.silent_timeout_minutes = 10  # 10åˆ†é–“ç„¡éŸ³ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆçŸ­ç¸®ï¼‰
        self.max_total_time_hours = 2     # æœ€å¤§2æ™‚é–“ã§å¼·åˆ¶çµ‚äº†ï¼ˆçŸ­ç¸®ï¼‰
        self.heartbeat_interval = 30      # 30ç§’é–“éš”ã§ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆ
        self.last_heartbeat = None
        self.process_health_checks = []
        self.error_count = 0
        self.max_errors = 3               # æœ€å¤§ã‚¨ãƒ©ãƒ¼æ•°

    def start_monitoring(self, expected_duration_minutes=None):
        """ç›£è¦–é–‹å§‹ï¼ˆæœŸå¾…å®Ÿè¡Œæ™‚é–“ä»˜ãï¼‰"""
        self.start_time = time.time()
        self.last_output_time = time.time()
        self.last_heartbeat = time.time()
        self.error_count = 0

        if expected_duration_minutes:
            self.expected_end_time = self.start_time + (expected_duration_minutes * 60)
        else:
            self.expected_end_time = None

        logger.info(f"ğŸ” æ”¹å–„ã•ã‚ŒãŸãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–é–‹å§‹:")
        logger.info(f"   - ã‚µã‚¤ãƒ¬ãƒ³ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {self.silent_timeout_minutes}åˆ†")
        logger.info(f"   - æœ€å¤§å®Ÿè¡Œæ™‚é–“: {self.max_total_time_hours}æ™‚é–“")
        logger.info(f"   - ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆé–“éš”: {self.heartbeat_interval}ç§’")
        if expected_duration_minutes:
            logger.info(f"   - æœŸå¾…å®Ÿè¡Œæ™‚é–“: {expected_duration_minutes}åˆ†")

    def update_heartbeat(self, message=""):
        """ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆæ›´æ–°"""
        current_time = time.time()
        self.last_heartbeat = current_time
        self.last_output_time = current_time

        if message:
            logger.info(f"ğŸ’“ ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆ: {message}")

    def check_health(self):
        """ãƒ—ãƒ­ã‚»ã‚¹å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        current_time = time.time()

        # ã‚µã‚¤ãƒ¬ãƒ³ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
        if self.last_output_time:
            silent_duration = (current_time - self.last_output_time) / 60
            if silent_duration > self.silent_timeout_minutes:
                logger.error(f"ğŸš¨ ã‚µã‚¤ãƒ¬ãƒ³ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ¤œå‡º: {silent_duration:.1f}åˆ†é–“ç„¡å¿œç­”")
                return False

        # ç·å®Ÿè¡Œæ™‚é–“ãƒã‚§ãƒƒã‚¯
        if self.start_time:
            total_duration = (current_time - self.start_time) / 3600
            if total_duration > self.max_total_time_hours:
                logger.error(f"ğŸš¨ æœ€å¤§å®Ÿè¡Œæ™‚é–“è¶…é: {total_duration:.1f}æ™‚é–“")
                return False

        # æœŸå¾…æ™‚é–“ã¨ã®æ¯”è¼ƒ
        if self.expected_end_time and current_time > self.expected_end_time * 1.5:
            logger.warning(f"âš ï¸ æœŸå¾…å®Ÿè¡Œæ™‚é–“ã®1.5å€ã‚’è¶…é")

        return True

    def record_error(self, error_message):
        """ã‚¨ãƒ©ãƒ¼è¨˜éŒ²"""
        self.error_count += 1
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼è¨˜éŒ² ({self.error_count}/{self.max_errors}): {error_message}")

        if self.error_count >= self.max_errors:
            logger.error(f"ğŸš¨ æœ€å¤§ã‚¨ãƒ©ãƒ¼æ•°ã«åˆ°é”ã€‚å‡¦ç†ã‚’åœæ­¢ã—ã¾ã™ã€‚")
            return False
        return True

# ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã‚¯ãƒ©ã‚¹ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰
class ProcessMonitor:
    """é•·æ™‚é–“å‡¦ç†ã®ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã¨ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç®¡ç†"""

    def __init__(self):
        self.start_time = None
        self.last_output_time = None
        self.silent_timeout_minutes = 15  # 15åˆ†é–“ç„¡éŸ³ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        self.max_total_time_hours = 3     # æœ€å¤§3æ™‚é–“ã§å¼·åˆ¶çµ‚äº†

    def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        self.start_time = time.time()
        self.last_output_time = time.time()
        logger.info(f"ğŸ” ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–é–‹å§‹: ã‚µã‚¤ãƒ¬ãƒ³ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ {self.silent_timeout_minutes}åˆ†, æœ€å¤§å®Ÿè¡Œæ™‚é–“ {self.max_total_time_hours}æ™‚é–“")

    def update_activity(self):
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ›´æ–°ï¼ˆå‡ºåŠ›ãŒã‚ã£ãŸæ™‚ã«å‘¼ã³å‡ºã—ï¼‰"""
        self.last_output_time = time.time()

    def check_timeout(self):
        """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯"""
        if not self.start_time:
            return False, ""

        current_time = time.time()
        total_elapsed = (current_time - self.start_time) / 3600  # æ™‚é–“
        silent_elapsed = (current_time - self.last_output_time) / 60  # åˆ†

        # æœ€å¤§å®Ÿè¡Œæ™‚é–“ãƒã‚§ãƒƒã‚¯
        if total_elapsed > self.max_total_time_hours:
            return True, f"æœ€å¤§å®Ÿè¡Œæ™‚é–“ï¼ˆ{self.max_total_time_hours}æ™‚é–“ï¼‰ã‚’è¶…éã—ã¾ã—ãŸ"

        # ã‚µã‚¤ãƒ¬ãƒ³ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
        if silent_elapsed > self.silent_timeout_minutes:
            return True, f"ã‚µã‚¤ãƒ¬ãƒ³ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ{self.silent_timeout_minutes}åˆ†é–“ç„¡éŸ³ï¼‰ãŒç™ºç”Ÿã—ã¾ã—ãŸ"

        return False, ""

    def get_status(self):
        """ç¾åœ¨ã®ç›£è¦–çŠ¶æ³ã‚’å–å¾—"""
        if not self.start_time:
            return "ç›£è¦–æœªé–‹å§‹"

        current_time = time.time()
        total_elapsed = (current_time - self.start_time) / 60  # åˆ†
        silent_elapsed = (current_time - self.last_output_time) / 60  # åˆ†

        return f"å®Ÿè¡Œæ™‚é–“: {total_elapsed:.1f}åˆ†, æœ€çµ‚å‡ºåŠ›ã‹ã‚‰: {silent_elapsed:.1f}åˆ†"

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ï¼ˆæ”¹å–„ç‰ˆï¼‰
enhanced_monitor = EnhancedProcessMonitor()
process_monitor = ProcessMonitor()  # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™

def run_command_with_monitoring(command, description, timeout_minutes=10):
    """ãƒ¡ãƒ¢ãƒªç›£è¦–ã¨ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ä»˜ãã§ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ"""
    # å®Ÿè¡Œå‰ã®ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
    memory_manager.check_memory_usage()

    # ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–é–‹å§‹
    process_monitor.start_monitoring()

    # ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œï¼ˆç›£è¦–ä»˜ãï¼‰
    result = run_command_with_process_monitoring(command, description, timeout_minutes)

    # å®Ÿè¡Œå¾Œã®ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if not memory_manager.check_memory_usage():
        logger.warning("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã„ãŸã‚ã€ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™")
        memory_manager.cleanup_memory()

    return result

def run_command_with_process_monitoring(command, description, timeout_minutes=10):
    """ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ä»˜ãã§ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ"""
    logger.info(f"ğŸš€ é–‹å§‹: {description}")
    logger.info(f"ğŸ“‹ å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {command}")
    logger.info(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š: {timeout_minutes}åˆ†")

    start_time = time.time()
    timeout_seconds = timeout_minutes * 60

    try:
        # ãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹
        process = subprocess.Popen(
            command,
            shell=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=False,
            bufsize=1
        )

        output_lines = []
        last_log_time = time.time()
        log_interval = 30  # 30ç§’ã”ã¨ã«ãƒ­ã‚°å‡ºåŠ›

        while True:
            # ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†ãƒã‚§ãƒƒã‚¯
            if process.poll() is not None:
                break

            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
            elapsed = time.time() - start_time
            if elapsed > timeout_seconds:
                logger.error(f"âš ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {description} ({timeout_minutes}åˆ†)")
                process.terminate()
                time.sleep(5)
                if process.poll() is None:
                    process.kill()
                return False

            # ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ãƒã‚§ãƒƒã‚¯
            is_timeout, timeout_reason = process_monitor.check_timeout()
            if is_timeout:
                logger.error(f"âš ï¸ ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {timeout_reason}")
                process.terminate()
                time.sleep(5)
                if process.poll() is None:
                    process.kill()
                return False

            # å‡ºåŠ›èª­ã¿å–ã‚Š
            try:
                output = process.stdout.readline()
                if output:
                    try:
                        line = output.decode('utf-8', errors='replace').strip()
                        if line:
                            output_lines.append(line)
                            process_monitor.update_activity()  # ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ›´æ–°

                            # å®šæœŸçš„ã«ãƒ­ã‚°å‡ºåŠ›
                            current_time = time.time()
                            if current_time - last_log_time > log_interval:
                                logger.info(f"ğŸ“Š {process_monitor.get_status()}")
                                logger.info(f"ğŸ’¬ æœ€æ–°å‡ºåŠ›: {line[:100]}...")
                                last_log_time = current_time
                    except UnicodeDecodeError:
                        pass
            except:
                pass

            time.sleep(0.1)  # CPUä½¿ç”¨ç‡ã‚’ä¸‹ã’ã‚‹

        # ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†å¾Œã®å‡¦ç†
        return_code = process.returncode
        end_time = time.time()
        duration = end_time - start_time

        # å‡ºåŠ›ã‚’çµåˆ
        full_output = '\n'.join(output_lines)

        if return_code == 0:
            logger.info(f"âœ… å®Œäº†: {description} (å®Ÿè¡Œæ™‚é–“: {duration:.1f}ç§’)")
            if len(output_lines) > 0:
                logger.info(f"ğŸ“ å‡ºåŠ›è¡Œæ•°: {len(output_lines)}è¡Œ")
                if len(full_output) > 2000:
                    logger.info(f"ğŸ“„ å‡ºåŠ›ï¼ˆå…ˆé ­500æ–‡å­—ï¼‰:\n{full_output[:500]}...")
                    logger.info(f"ğŸ“„ å‡ºåŠ›ï¼ˆæœ«å°¾500æ–‡å­—ï¼‰:\n...{full_output[-500:]}")
                else:
                    logger.info(f"ğŸ“„ å‡ºåŠ›:\n{full_output}")
            return True
        else:
            logger.error(f"âŒ å¤±æ•—: {description}")
            logger.error(f"ğŸ”¢ ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰: {return_code}")
            if full_output:
                logger.error(f"ğŸ“„ ã‚¨ãƒ©ãƒ¼å‡ºåŠ›:\n{full_output}")
            return False

    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        logger.error(f"ğŸ’¥ ä¾‹å¤–ç™ºç”Ÿ: {description} (å®Ÿè¡Œæ™‚é–“: {duration:.1f}ç§’)")
        logger.error(f"ğŸ” ä¾‹å¤–è©³ç´°: {e}")
        return False

def run_command(command, description, timeout_minutes=10):
    """ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã™ï¼ˆã‚µã‚¤ãƒ¬ãƒ³ãƒˆå¤±æ•—æ¤œå‡ºæ©Ÿèƒ½ä»˜ãï¼‰"""
    logger.info(f"é–‹å§‹: {description}")
    logger.info(f"å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {command}")
    logger.info(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š: {timeout_minutes}åˆ†")

    start_time = time.time()
    timeout_seconds = timeout_minutes * 60

    try:
        # Windowsã§ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œã‚’å›é¿
        result = subprocess.run(
            command,
            shell=True,
            capture_output=True,
            text=False,  # ãƒã‚¤ãƒŠãƒªãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ
            timeout=timeout_seconds  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
        )

        # å‡ºåŠ›ã‚’UTF-8ã§ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã¯ç„¡è¦–ï¼‰
        stdout = ""
        stderr = ""

        if result.stdout:
            try:
                stdout = result.stdout.decode('utf-8', errors='replace')
            except:
                try:
                    stdout = result.stdout.decode('shift_jis', errors='replace')
                except:
                    stdout = str(result.stdout)

        if result.stderr:
            try:
                stderr = result.stderr.decode('utf-8', errors='replace')
            except:
                try:
                    stderr = result.stderr.decode('shift_jis', errors='replace')
                except:
                    stderr = str(result.stderr)

        # çµæœã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ›´æ–°
        result.stdout = stdout
        result.stderr = stderr

        end_time = time.time()
        duration = end_time - start_time

        # ã‚µã‚¤ãƒ¬ãƒ³ãƒˆå¤±æ•—ã®æ¤œå‡º
        if duration < 30 and not result.stdout and not result.stderr:
            logger.warning(f"âš ï¸ ã‚µã‚¤ãƒ¬ãƒ³ãƒˆå¤±æ•—ã®å¯èƒ½æ€§: {description}")
            logger.warning(f"å®Ÿè¡Œæ™‚é–“ãŒçŸ­ã™ãã¾ã™: {duration:.1f}ç§’")
            logger.warning("å‡ºåŠ›ãŒå…¨ãã‚ã‚Šã¾ã›ã‚“")
            return False

        if result.returncode == 0:
            logger.info(f"å®Œäº†: {description} (å®Ÿè¡Œæ™‚é–“: {duration:.1f}ç§’)")
            if result.stdout:
                # å‡ºåŠ›ãŒé•·ã„å ´åˆã¯æœ€åˆã¨æœ€å¾Œã®éƒ¨åˆ†ã®ã¿è¡¨ç¤º
                if len(result.stdout) > 2000:
                    logger.info(f"å‡ºåŠ›ï¼ˆå…ˆé ­500æ–‡å­—ï¼‰:\n{result.stdout[:500]}...")
                    logger.info(f"å‡ºåŠ›ï¼ˆæœ«å°¾500æ–‡å­—ï¼‰:\n...{result.stdout[-500:]}")
                else:
                    logger.info(f"å‡ºåŠ›:\n{result.stdout}")
            return True
        else:
            logger.error(f"å¤±æ•—: {description}")
            logger.error(f"ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰: {result.returncode}")
            if result.stderr:
                logger.error(f"ã‚¨ãƒ©ãƒ¼å‡ºåŠ›:\n{result.stderr}")
            if result.stdout:
                logger.error(f"æ¨™æº–å‡ºåŠ›:\n{result.stdout}")
            return False

    except subprocess.TimeoutExpired:
        logger.error(f"âš ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {description} ({timeout_minutes}åˆ†)")
        logger.error("ãƒ—ãƒ­ã‚»ã‚¹ãŒæŒ‡å®šæ™‚é–“å†…ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸ")
        return False
    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        logger.error(f"ä¾‹å¤–ç™ºç”Ÿ: {description} (å®Ÿè¡Œæ™‚é–“: {duration:.1f}ç§’)")
        logger.error(f"ä¾‹å¤–è©³ç´°: {e}")
        return False

def integrate_email_extraction_results(start_id, end_id):
    """ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºçµæœã‚’æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã«çµ±åˆï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾å¿œç‰ˆï¼‰"""
    try:
        import signal
        import time

        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šï¼ˆ30ç§’ï¼‰
        def timeout_handler(signum, frame):
            raise TimeoutError("ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆå‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")

        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(30)  # 30ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ

        try:
            # æ´¾ç”Ÿç‰ˆãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ä½¿ç”¨ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
            patterns = [
                f"derivative_email_extraction_results_id{start_id}-{end_id}_*.csv",
                f"email_extraction_results_id{start_id}-{end_id}_*.csv"
            ]

            # è¶…é«˜é€Ÿå‡¦ç†ã®åˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚æ¤œç´¢ï¼ˆä¼æ¥­ID 1901-1920ã®å ´åˆï¼‰
            if start_id >= 1901 and end_id <= 1920:
                patterns.extend([
                    f"ultra_fast_email_extraction_results_id{start_id}-{start_id+9}_*.csv",
                    f"ultra_fast_email_extraction_results_id{start_id+10}-{end_id}_*.csv"
                ])

            extraction_files = []
            for pattern in patterns:
                files = glob.glob(pattern)
                if files:
                    extraction_files.extend(files)
                    logger.info(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ '{pattern}' ã§ {len(files)} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

            # é‡è¤‡ã‚’é™¤å»
            extraction_files = list(set(extraction_files))

            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾å¿œãƒ„ãƒ¼ãƒ«ã®è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆå‡¦ç†
            timeout_files = [f for f in extraction_files if 'timeout_safe_extraction' in f]
            if timeout_files:
                logger.info(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ« {len(timeout_files)} å€‹ã‚’ç™ºè¦‹ã€çµ±åˆå‡¦ç†ã‚’å®Ÿè¡Œ")

            if not extraction_files:
                logger.error(f"ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è©¦ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³: {patterns}")
                # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
                all_files = [f for f in os.listdir('.') if f.endswith('.csv') and f'id{start_id}-{end_id}' in f]
                logger.info(f"è©²å½“ã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«: {all_files}")
                return False

            # æ´¾ç”Ÿç‰ˆçµ±åˆãƒ•ã‚¡ã‚¤ãƒ«
            latest_file = "data/derivative_email_extraction_results_latest.csv"

            # æ–°ã—ã„æŠ½å‡ºçµæœã‚’èª­ã¿è¾¼ã¿ï¼ˆè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰
            new_results = {}

            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆçš„ã«å‡¦ç†
            timeout_files = [f for f in extraction_files if 'timeout_safe_extraction' in f]
            other_files = [f for f in extraction_files if 'timeout_safe_extraction' not in f]

            files_to_process = timeout_files if timeout_files else other_files

            for file_path in files_to_process:
                logger.info(f"å‡¦ç†ä¸­: {file_path}")
                try:
                    with open(file_path, 'r', encoding='utf-8-sig') as f:
                        reader = csv.DictReader(f)
                        for row in reader:
                            company_id = int(row['ä¼æ¥­ID'])
                            if start_id <= company_id <= end_id:
                                new_results[company_id] = row
                except Exception as e:
                    logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
                    continue

            logger.info(f"æ–°ã—ã„æŠ½å‡ºçµæœèª­ã¿è¾¼ã¿: {len(new_results)}ç¤¾")

            # æ—¢å­˜ã®çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯æ–°è¦ä½œæˆï¼‰
            existing_data = []
            if os.path.exists(latest_file):
                with open(latest_file, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    existing_data = list(reader)
                logger.info(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(existing_data)}è¡Œ")
            else:
                logger.info("çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€æ–°è¦ä½œæˆã—ã¾ã™")

            # ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆï¼ˆæŒ‡å®šç¯„å›²ã®IDã‚’æ–°ã—ã„çµæœã§ç½®ãæ›ãˆï¼‰
            updated_data = []
            replaced_count = 0

            for row in existing_data:
                company_id = int(row.get('ä¼æ¥­ID', 0))

                if start_id <= company_id <= end_id and company_id in new_results:
                    # æ–°ã—ã„çµæœã§ç½®ãæ›ãˆ
                    updated_data.append(new_results[company_id])
                    replaced_count += 1
                else:
                    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ä¿æŒ
                    updated_data.append(row)

            # æ–°ã—ãè¿½åŠ ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°è¿½åŠ 
            for company_id, new_row in new_results.items():
                if not any(int(row.get('ä¼æ¥­ID', 0)) == company_id for row in updated_data):
                    updated_data.append(new_row)
                    replaced_count += 1

            # ä¼æ¥­IDã§ã‚½ãƒ¼ãƒˆ
            updated_data.sort(key=lambda x: int(x.get('ä¼æ¥­ID', 0)))

            logger.info(f"ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {replaced_count}ç¤¾ã‚’æ›´æ–°/è¿½åŠ ")

            # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ï¼ˆå®‰å…¨ãªæ›¸ãè¾¼ã¿ï¼‰
            if updated_data:
                fieldnames = list(updated_data[0].keys())
                success = safe_csv_write(latest_file, updated_data, fieldnames)
                if success:
                    logger.info(f"çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã—ã¾ã—ãŸ: {latest_file} (ç·ä»¶æ•°: {len(updated_data)})")
                    return True
                else:
                    logger.error("ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
                    return False
            else:
                logger.warning("çµ±åˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return False
        finally:
            signal.alarm(0)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè§£é™¤

    except Exception as e:
        logger.error(f"ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºçµæœã®çµ±åˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return False

def integrate_website_analysis_results(start_id, end_id):
    """ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æçµæœã‚’çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã«åæ˜ """
    try:
        # è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
        patterns = [
            f"new_website_analysis_results_id{start_id}-{end_id}_*.csv",
            f"website_analysis_results_id{start_id}-{end_id}_*.csv",
            f"*website_analysis_results_id{start_id}-{end_id}_*.csv"
        ]

        analysis_files = []
        for pattern in patterns:
            files = glob.glob(pattern)
            if files:
                analysis_files.extend(files)
                logger.info(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ '{pattern}' ã§ {len(files)} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

        # é‡è¤‡ã‚’é™¤å»
        analysis_files = list(set(analysis_files))

        if not analysis_files:
            logger.error(f"åˆ†æçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è©¦ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³: {patterns}")
            # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
            all_files = [f for f in os.listdir('.') if f.endswith('.csv') and f'id{start_id}-{end_id}' in f]
            logger.info(f"è©²å½“ã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«: {all_files}")
            return False

        # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
        latest_analysis_file = max(analysis_files, key=os.path.getmtime)
        logger.info(f"çµ±åˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {latest_analysis_file}")

        # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«
        latest_file = "new_website_analysis_results_latest.csv"

        # æ–°ã—ã„åˆ†æçµæœã‚’èª­ã¿è¾¼ã¿
        new_results = {}
        new_fieldnames = None
        with open(latest_analysis_file, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            new_fieldnames = reader.fieldnames
            for row in reader:
                company_id = int(row['ä¼æ¥­ID'])
                new_results[company_id] = row

        logger.info(f"æ–°ã—ã„åˆ†æçµæœèª­ã¿è¾¼ã¿: {len(new_results)}ç¤¾")

        # æ—¢å­˜ã®çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯æ–°è¦ä½œæˆï¼‰
        existing_data = []
        existing_fieldnames = None
        if os.path.exists(latest_file):
            with open(latest_file, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                existing_fieldnames = reader.fieldnames
                existing_data = list(reader)
            logger.info(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(existing_data)}è¡Œ")
        else:
            logger.info("çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€æ–°è¦ä½œæˆã—ã¾ã™")
            # æ–°è¦ä½œæˆã®å ´åˆã¯æ–°ã—ã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã‚’ä½¿ç”¨
            existing_fieldnames = new_fieldnames

        # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ–°ã—ã„å½¢å¼ã‹ã‚‰æ—¢å­˜å½¢å¼ã¸ï¼‰
        def convert_new_to_existing_format(new_row, target_fieldnames):
            """æ–°ã—ã„åˆ†æçµæœã‚’æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›"""
            converted = {}

            # åŸºæœ¬ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒãƒƒãƒ”ãƒ³ã‚°
            basic_mapping = {
                'ä¼æ¥­ID': 'ä¼æ¥­ID',
                'ä¼æ¥­å': 'ä¼æ¥­å',
                'URL': 'URL',
                'ç·åˆã‚¹ã‚³ã‚¢': 'ç·åˆã‚¹ã‚³ã‚¢',
                'ãƒ©ãƒ³ã‚¯': 'ãƒ©ãƒ³ã‚¯'
            }

            # ã‚¹ã‚³ã‚¢ã®åˆ†æ•£ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ–°ã—ã„3é …ç›®ã‚’æ—¢å­˜9é …ç›®ã«åˆ†æ•£ï¼‰
            ux_score = float(new_row.get('UXã‚¹ã‚³ã‚¢', 0)) if new_row.get('UXã‚¹ã‚³ã‚¢') else 0
            design_score = float(new_row.get('ãƒ‡ã‚¶ã‚¤ãƒ³ã‚¹ã‚³ã‚¢', 0)) if new_row.get('ãƒ‡ã‚¶ã‚¤ãƒ³ã‚¹ã‚³ã‚¢') else 0
            tech_score = float(new_row.get('æŠ€è¡“ã‚¹ã‚³ã‚¢', 0)) if new_row.get('æŠ€è¡“ã‚¹ã‚³ã‚¢') else 0

            # è©³ç´°é …ç›®ã¸ã®åˆ†æ•£ï¼ˆ100ç‚¹æº€ç‚¹ã«èª¿æ•´ï¼‰
            score_distribution = {
                'ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£': ux_score * 3.33,  # UXã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹
                'ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£': ux_score * 3.33,
                'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ª': ux_score * 3.33,
                'ãƒ‡ã‚¶ã‚¤ãƒ³å“è³ª': design_score * 2.5,  # ãƒ‡ã‚¶ã‚¤ãƒ³ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹
                'è¦–è¦šçš„éšå±¤': design_score * 2.5,
                'ãƒ–ãƒ©ãƒ³ãƒ‰ä¸€è²«æ€§': design_score * 2.5,
                'SEOæœ€é©åŒ–': tech_score * 3.33,  # æŠ€è¡“ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹
                'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹': tech_score * 3.33,
                'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£': tech_score * 3.33
            }

            # æ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã«åŸºã¥ã„ã¦å¤‰æ›
            for target_field in target_fieldnames:
                if target_field in new_row:
                    # ç›´æ¥ãƒãƒƒãƒ”ãƒ³ã‚°
                    converted[target_field] = new_row[target_field]
                elif target_field in basic_mapping:
                    # åŸºæœ¬ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
                    source_field = basic_mapping[target_field]
                    converted[target_field] = new_row.get(source_field, '')
                elif target_field in score_distribution:
                    # ã‚¹ã‚³ã‚¢åˆ†æ•£ãƒãƒƒãƒ”ãƒ³ã‚°
                    converted[target_field] = round(score_distribution[target_field], 1)
                elif target_field in ['å¼·ã¿', 'å¼±ã¿', 'æ”¹å–„ææ¡ˆ']:
                    # ã‚³ãƒ¡ãƒ³ãƒˆç³»ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ç©ºã§åˆæœŸåŒ–
                    converted[target_field] = ''
                else:
                    converted[target_field] = ''

            return converted

        # ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆï¼ˆæŒ‡å®šç¯„å›²ã®IDã‚’æ–°ã—ã„çµæœã§ç½®ãæ›ãˆï¼‰
        updated_data = []
        replaced_count = 0

        for row in existing_data:
            company_id = int(row.get('ä¼æ¥­ID', 0))

            if start_id <= company_id <= end_id and company_id in new_results:
                # æ–°ã—ã„çµæœã‚’æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ã—ã¦ç½®ãæ›ãˆ
                converted_row = convert_new_to_existing_format(new_results[company_id], existing_fieldnames)
                updated_data.append(converted_row)
                replaced_count += 1
            else:
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ä¿æŒ
                updated_data.append(row)

        # æ–°ã—ãè¿½åŠ ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°è¿½åŠ 
        for company_id, new_row in new_results.items():
            if not any(int(row.get('ä¼æ¥­ID', 0)) == company_id for row in updated_data):
                converted_row = convert_new_to_existing_format(new_row, existing_fieldnames)
                updated_data.append(converted_row)
                replaced_count += 1

        # ä¼æ¥­IDã§ã‚½ãƒ¼ãƒˆ
        updated_data.sort(key=lambda x: int(x.get('ä¼æ¥­ID', 0)))

        logger.info(f"ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {replaced_count}ç¤¾ã‚’æ›´æ–°/è¿½åŠ ")

        # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
        if updated_data:
            # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã‚’åŸºæº–ã«ã™ã‚‹
            if os.path.exists(latest_file):
                with open(latest_file, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    existing_fieldnames = reader.fieldnames
                    if existing_fieldnames:
                        fieldnames = existing_fieldnames
                    else:
                        fieldnames = list(updated_data[0].keys())
            else:
                fieldnames = list(updated_data[0].keys())

            # ãƒ‡ãƒ¼ã‚¿ã‚’æ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã«åˆã‚ã›ã¦èª¿æ•´
            adjusted_data = []
            for row in updated_data:
                adjusted_row = {}
                for field in fieldnames:
                    adjusted_row[field] = row.get(field, '')
                adjusted_data.append(adjusted_row)

            success = safe_csv_write(latest_file, adjusted_data, fieldnames)
            if success:
                logger.info(f"çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã—ã¾ã—ãŸ: {latest_file} (ç·ä»¶æ•°: {len(adjusted_data)})")
                return True
            else:
                logger.error("ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
        else:
            logger.warning("çµ±åˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return False

    except Exception as e:
        logger.error(f"ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æçµæœã®çµ±åˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return False

def check_prerequisites():
    """å‰ææ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯"""
    logger.info("å‰ææ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã„ã¾ã™...")

    # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(INPUT_FILE):
        logger.error(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {INPUT_FILE}")
        return False

    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    for script_name, script_file in SCRIPTS.items():
        if not os.path.exists(script_file):
            logger.error(f"ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {script_file}")
            return False

    logger.info("å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯å®Œäº†")
    return True

def run_enhanced_batch_processing(start_id, end_id, test_mode=False):
    """æ”¹å–„ã•ã‚ŒãŸãƒãƒƒãƒå‡¦ç†ï¼ˆã‚µã‚¤ãƒ¬ãƒ³ãƒˆå¤±æ•—å¯¾ç­–å¼·åŒ–ç‰ˆï¼‰"""
    total_companies = end_id - start_id + 1

    # ä¿å®ˆçš„ãªãƒãƒƒãƒã‚µã‚¤ã‚ºæ±ºå®šï¼ˆå®‰å®šæ€§é‡è¦–ï¼‰
    if total_companies <= 5:
        batch_size = total_companies
    elif total_companies <= 20:
        batch_size = 5  # å°è¦æ¨¡ã¯5ç¤¾å˜ä½
    elif total_companies <= 50:
        batch_size = 8  # ä¸­è¦æ¨¡ã¯8ç¤¾å˜ä½
    else:
        batch_size = 10  # å¤§è¦æ¨¡ã¯10ç¤¾å˜ä½ï¼ˆæœ€å¤§ï¼‰

    logger.info(f"ğŸ”„ æ”¹å–„ã•ã‚ŒãŸãƒãƒƒãƒå‡¦ç†é–‹å§‹: {total_companies}ç¤¾ã‚’{batch_size}ç¤¾å˜ä½ã§å‡¦ç†")
    logger.info(f"ğŸ“Š å®‰å®šæ€§é‡è¦–ã®ä¿å®ˆçš„ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¡ç”¨")

    successful_batches = 0
    failed_batches = 0
    total_batches = (total_companies + batch_size - 1) // batch_size

    # æœŸå¾…å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—ï¼ˆ1ç¤¾ã‚ãŸã‚Š30ç§’ï¼‰
    expected_duration_per_batch = batch_size * 0.5  # åˆ†

    # æ”¹å–„ã•ã‚ŒãŸãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã‚’é–‹å§‹
    enhanced_monitor.start_monitoring(expected_duration_per_batch * total_batches)

    for batch_start in range(start_id, end_id + 1, batch_size):
        batch_end = min(batch_start + batch_size - 1, end_id)
        batch_num = (batch_start - start_id) // batch_size + 1

        logger.info(f"ğŸ“¦ ãƒãƒƒãƒ {batch_num}/{total_batches}: ä¼æ¥­ID {batch_start}-{batch_end}")
        enhanced_monitor.update_heartbeat(f"ãƒãƒƒãƒ {batch_num} é–‹å§‹")

        # ãƒãƒƒãƒå‡¦ç†å‰ã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
        if not enhanced_monitor.check_health():
            logger.error("ğŸš¨ ãƒ—ãƒ­ã‚»ã‚¹å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯å¤±æ•—ã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
            break

        # ãƒãƒƒãƒå‡¦ç†å‰ã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        memory_manager.cleanup_memory()

        try:
            # ãƒãƒƒãƒé–‹å§‹æ™‚åˆ»è¨˜éŒ²
            batch_start_time = time.time()

            # çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
            success = run_full_workflow_with_monitoring(batch_start, batch_end, test_mode)

            # ãƒãƒƒãƒå®Ÿè¡Œæ™‚é–“ãƒã‚§ãƒƒã‚¯
            batch_duration = time.time() - batch_start_time
            expected_max_duration = expected_duration_per_batch * 60 * 2  # æœŸå¾…æ™‚é–“ã®2å€

            if batch_duration > expected_max_duration:
                logger.warning(f"âš ï¸ ãƒãƒƒãƒå®Ÿè¡Œæ™‚é–“ç•°å¸¸: {batch_duration:.1f}ç§’ (æœŸå¾…æœ€å¤§: {expected_max_duration:.1f}ç§’)")

            if success:
                successful_batches += 1
                logger.info(f"âœ… ãƒãƒƒãƒ {batch_num} å®Œäº† (å®Ÿè¡Œæ™‚é–“: {batch_duration:.1f}ç§’)")
                enhanced_monitor.update_heartbeat(f"ãƒãƒƒãƒ {batch_num} æˆåŠŸ")
            else:
                failed_batches += 1
                error_msg = f"ãƒãƒƒãƒ {batch_num} å¤±æ•—"
                logger.error(f"âŒ {error_msg}")

                # ã‚¨ãƒ©ãƒ¼è¨˜éŒ²ã¨ç¶™ç¶šå¯å¦åˆ¤å®š
                if not enhanced_monitor.record_error(error_msg):
                    logger.error("ğŸš¨ æœ€å¤§ã‚¨ãƒ©ãƒ¼æ•°ã«åˆ°é”ã€‚å‡¦ç†ã‚’åœæ­¢ã—ã¾ã™ã€‚")
                    break

            # ãƒãƒƒãƒé–“ã®ä¼‘æ†©ï¼ˆå®‰å®šåŒ–ï¼‰
            if batch_end < end_id:
                rest_time = 3 if successful_batches > 0 else 5  # æˆåŠŸæ™‚ã¯çŸ­ç¸®
                logger.info(f"â±ï¸ ãƒãƒƒãƒé–“ä¼‘æ†©ï¼ˆ{rest_time}ç§’ï¼‰...")
                time.sleep(rest_time)

        except Exception as e:
            error_msg = f"ãƒãƒƒãƒ {batch_num} ä¾‹å¤–: {str(e)}"
            logger.error(f"ğŸ’¥ {error_msg}")

            if not enhanced_monitor.record_error(error_msg):
                break

            failed_batches += 1

    # å‡¦ç†çµæœã‚µãƒãƒªãƒ¼
    success_rate = (successful_batches / total_batches) * 100 if total_batches > 0 else 0
    logger.info(f"ğŸ“Š ãƒãƒƒãƒå‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼:")
    logger.info(f"   - æˆåŠŸ: {successful_batches}/{total_batches} ({success_rate:.1f}%)")
    logger.info(f"   - å¤±æ•—: {failed_batches}")
    logger.info(f"   - ã‚¨ãƒ©ãƒ¼æ•°: {enhanced_monitor.error_count}")

    return successful_batches > 0 and success_rate >= 80  # 80%ä»¥ä¸Šã®æˆåŠŸç‡ã‚’è¦æ±‚

def run_optimized_batch_processing(start_id, end_id, test_mode=False):
    """æœ€é©åŒ–ã•ã‚ŒãŸãƒãƒƒãƒå‡¦ç†ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–ï¼‰"""
    total_companies = end_id - start_id + 1

    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‹•çš„ã«æ±ºå®š
    if total_companies <= 20:
        batch_size = total_companies
    elif total_companies <= 50:
        batch_size = 10
    else:
        batch_size = 15  # å¤§è¦æ¨¡å‡¦ç†ã§ã¯15ç¤¾å˜ä½

    logger.info(f"ğŸ”„ æœ€é©åŒ–ãƒãƒƒãƒå‡¦ç†é–‹å§‹: {total_companies}ç¤¾ã‚’{batch_size}ç¤¾å˜ä½ã§å‡¦ç†")

    successful_batches = 0
    failed_batches = 0

    for batch_start in range(start_id, end_id + 1, batch_size):
        batch_end = min(batch_start + batch_size - 1, end_id)
        batch_num = (batch_start - start_id) // batch_size + 1
        total_batches = (total_companies + batch_size - 1) // batch_size

        logger.info(f"ğŸ“¦ ãƒãƒƒãƒ {batch_num}/{total_batches}: ä¼æ¥­ID {batch_start}-{batch_end}")

        # ãƒãƒƒãƒå‡¦ç†å‰ã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        memory_manager.cleanup_memory()

        try:
            # çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
            success = run_full_workflow(batch_start, batch_end, test_mode)

            if success:
                successful_batches += 1
                logger.info(f"âœ… ãƒãƒƒãƒ {batch_num} å®Œäº†")
            else:
                failed_batches += 1
                logger.error(f"âŒ ãƒãƒƒãƒ {batch_num} å¤±æ•—")

                # å¤±æ•—æ™‚ã¯å‡¦ç†ã‚’åœæ­¢
                logger.error("ãƒãƒƒãƒå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚ã€å‡¦ç†ã‚’åœæ­¢ã—ã¾ã™")
                break

            # ãƒãƒƒãƒé–“ã®ä¼‘æ†©ï¼ˆãƒ¡ãƒ¢ãƒªå®‰å®šåŒ–ï¼‰
            if batch_end < end_id:
                logger.info("â±ï¸ ãƒãƒƒãƒé–“ä¼‘æ†©ï¼ˆ5ç§’ï¼‰...")
                time.sleep(5)

        except Exception as e:
            failed_batches += 1
            logger.error(f"âŒ ãƒãƒƒãƒ {batch_num} ã§ä¾‹å¤–ç™ºç”Ÿ: {e}")
            break

    # çµæœã‚µãƒãƒªãƒ¼
    logger.info(f"ğŸ“Š ãƒãƒƒãƒå‡¦ç†å®Œäº†: æˆåŠŸ {successful_batches}, å¤±æ•— {failed_batches}")

    return failed_batches == 0

def run_email_extraction(start_id=None, end_id=None, test_mode=False):
    """ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºã‚’å®Ÿè¡Œï¼ˆç¢ºå®Ÿæ€§é‡è¦–ç‰ˆï¼‰"""
    # è¶…é«˜é€Ÿå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã‚’ç„¡åŠ¹åŒ–ã—ã€ç¢ºå®Ÿæ€§ã‚’é‡è¦–ã—ãŸæ¨™æº–å‡¦ç†ã«å¤‰æ›´
    if start_id is not None and end_id is not None:
        range_size = end_id - start_id + 1

        # 100ç¤¾ä»¥ä¸Šã®å‡¦ç†æ™‚ã¯æœ€é©åŒ–ãƒãƒƒãƒå‡¦ç†ã‚’ä½¿ç”¨
        if range_size >= 100:
            logger.info(f"å¤§è¦æ¨¡å‡¦ç†ï¼ˆ{range_size}ç¤¾ï¼‰ã®ãŸã‚æœ€é©åŒ–ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™")
            return run_optimized_batch_processing(start_id, end_id, test_mode)
        elif range_size > 20:
            logger.info(f"ä¸­è¦æ¨¡å‡¦ç†ï¼ˆ{range_size}ç¤¾ï¼‰ã®ãŸã‚æ¨™æº–å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™")

    # ç¢ºå®Ÿæ€§é‡è¦–ã®æ¨™æº–å‡¦ç†ã‚³ãƒãƒ³ãƒ‰ï¼ˆæ­£ã—ã„å¼•æ•°ã®ã¿ä½¿ç”¨ï¼‰
    command = f"python {SCRIPTS['email_extraction']} --input-file {INPUT_FILE}"

    # prioritized_email_extractor.pyãŒã‚µãƒãƒ¼ãƒˆã™ã‚‹å¼•æ•°ã®ã¿ä½¿ç”¨
    if start_id and end_id:
        command += f" --start-id {start_id} --end-id {end_id}"

    # å‹•çš„ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºã‚’æœ‰åŠ¹åŒ–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    # --no-dynamic ãƒ•ãƒ©ã‚°ã¯ä½¿ç”¨ã—ãªã„ï¼ˆå‹•çš„æŠ½å‡ºã‚’æœ‰åŠ¹ã«ã™ã‚‹ãŸã‚ï¼‰

    # å®Ÿè¡Œå‰ã®çŠ¶æ…‹ã‚’è¨˜éŒ²
    before_files = set(glob.glob("*email_extraction_results*.csv"))

    success = run_command_with_monitoring(command, "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºï¼ˆç¢ºå®Ÿæ€§é‡è¦–ï¼‰", timeout_minutes=20)

    # å®Ÿè¡Œå¾Œã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
    if success and start_id and end_id:
        after_files = set(glob.glob("*email_extraction_results*.csv"))
        new_files = after_files - before_files

        if new_files:
            logger.info(f"âœ… æ–°ã—ã„å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ: {list(new_files)}")
        else:
            logger.warning(f"âš ï¸ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆID {start_id}-{end_id}ï¼‰")
            # æœŸå¾…ã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¢ºèª
            expected_patterns = [
                f"email_extraction_results_id{start_id}-{end_id}_*.csv",
                f"new_email_extraction_results_id{start_id}-{end_id}_*.csv"
            ]
            for pattern in expected_patterns:
                matching_files = glob.glob(pattern)
                if matching_files:
                    logger.info(f"æœŸå¾…ã•ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {matching_files}")
                    break
            else:
                logger.error("æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                success = False

    # æŠ½å‡ºãŒæˆåŠŸã—ãŸå ´åˆã€çµæœã‚’æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã«çµ±åˆ
    if success and start_id and end_id and not test_mode:
        logger.info("ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºçµæœã‚’æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã«çµ±åˆã—ã¦ã„ã¾ã™...")
        integrate_success = integrate_email_extraction_results(start_id, end_id)
        if integrate_success:
            logger.info("ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºçµæœã®çµ±åˆãŒå®Œäº†ã—ã¾ã—ãŸ")
        else:
            logger.warning("ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºçµæœã®çµ±åˆã«å¤±æ•—ã—ã¾ã—ãŸ")

    return success

def run_website_analysis(start_id=None, end_id=None, test_mode=False):
    """ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æã‚’å®Ÿè¡Œï¼ˆè©³ç´°åˆ†æç‰ˆï¼‰"""
    # é«˜é€Ÿå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã‚’ç„¡åŠ¹åŒ–ã—ã€è©³ç´°åˆ†æãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´
    if start_id is not None and end_id is not None:
        range_size = end_id - start_id + 1

        # 100ç¤¾ä»¥ä¸Šã®å‡¦ç†æ™‚ã¯è‡ªå‹•çš„ã«20ç¤¾å˜ä½ã®ãƒãƒƒãƒã«åˆ†å‰²
        if range_size >= 100:
            logger.info(f"å¤§è¦æ¨¡å‡¦ç†ï¼ˆ{range_size}ç¤¾ï¼‰ã®ãŸã‚20ç¤¾å˜ä½ã®ãƒãƒƒãƒã«è‡ªå‹•åˆ†å‰²ã—ã¾ã™")
            return run_batch_website_analysis(start_id, end_id, test_mode)
        elif range_size > 20:
            logger.info(f"ä¸­è¦æ¨¡å‡¦ç†ï¼ˆ{range_size}ç¤¾ï¼‰ã®ãŸã‚è©³ç´°åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™")

    # è©³ç´°åˆ†æãƒ¢ãƒ¼ãƒ‰ã®ã‚³ãƒãƒ³ãƒ‰ï¼ˆå®Ÿéš›ã«ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹å¼•æ•°ã®ã¿ä½¿ç”¨ï¼‰
    command = f"python {SCRIPTS['website_analysis']}"

    if test_mode:
        command += " --test"
    elif start_id and end_id:
        command += f" --start-id {start_id} --end-id {end_id}"

    # å®Ÿè¡Œå‰ã®çŠ¶æ…‹ã‚’è¨˜éŒ²
    before_files = set(glob.glob("*website_analysis_results*.csv"))

    success = run_command_with_monitoring(command, "ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æï¼ˆè©³ç´°åˆ†æï¼‰", timeout_minutes=25)

    # å®Ÿè¡Œå¾Œã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
    if success and start_id and end_id:
        after_files = set(glob.glob("*website_analysis_results*.csv"))
        new_files = after_files - before_files

        if new_files:
            logger.info(f"âœ… æ–°ã—ã„å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ: {list(new_files)}")
        else:
            logger.warning(f"âš ï¸ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆID {start_id}-{end_id}ï¼‰")
            # æœŸå¾…ã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¢ºèª
            expected_patterns = [
                f"website_analysis_results_id{start_id}-{end_id}_*.csv",
                f"new_website_analysis_results_id{start_id}-{end_id}_*.csv"
            ]
            for pattern in expected_patterns:
                matching_files = glob.glob(pattern)
                if matching_files:
                    logger.info(f"æœŸå¾…ã•ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {matching_files}")
                    break
            else:
                logger.error("æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                success = False

    # åˆ†æãŒæˆåŠŸã—ãŸå ´åˆã€çµæœã‚’çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã«åæ˜ 
    if success and start_id and end_id and not test_mode:
        logger.info("ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æçµæœã‚’çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã«åæ˜ ã—ã¦ã„ã¾ã™...")
        integrate_success = integrate_website_analysis_results(start_id, end_id)
        if integrate_success:
            logger.info("ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æçµæœã®çµ±åˆãŒå®Œäº†ã—ã¾ã—ãŸ")
        else:
            logger.warning("ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æçµæœã®çµ±åˆã«å¤±æ•—ã—ã¾ã—ãŸ")

    return success

def run_batch_email_extraction(start_id, end_id, test_mode=False):
    """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ç”¨ãƒãƒƒãƒãƒ¡ãƒ¼ãƒ«æŠ½å‡ºï¼ˆ20ç¤¾å˜ä½ï¼‰"""
    logger.info(f"ğŸ”„ ãƒãƒƒãƒãƒ¡ãƒ¼ãƒ«æŠ½å‡ºé–‹å§‹: ID {start_id}-{end_id}")

    batch_size = 20
    total_batches = (end_id - start_id + 1 + batch_size - 1) // batch_size
    successful_batches = 0
    failed_batches = []

    for batch_num in range(total_batches):
        batch_start = start_id + (batch_num * batch_size)
        batch_end = min(batch_start + batch_size - 1, end_id)

        logger.info(f"ğŸ“¦ ãƒãƒƒãƒ {batch_num + 1}/{total_batches}: ID {batch_start}-{batch_end}")

        try:
            # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
            success = run_email_extraction(batch_start, batch_end, test_mode)

            if success:
                successful_batches += 1
                logger.info(f"[SUCCESS] Batch {batch_num + 1} completed")
            else:
                failed_batches.append((batch_start, batch_end))
                logger.error(f"[FAILED] Batch {batch_num + 1} failed")

            # ãƒãƒƒãƒé–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆ3-5ç§’ï¼‰
            if batch_num < total_batches - 1:  # æœ€å¾Œã®ãƒãƒƒãƒã§ãªã„å ´åˆ
                wait_time = 4  # 4ç§’å¾…æ©Ÿ
                logger.info(f"[WAIT] Batch interval wait: {wait_time} seconds")
                time.sleep(wait_time)

                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                cleanup_memory()

        except Exception as e:
            logger.error(f"[ERROR] Batch {batch_num + 1} error: {e}")
            failed_batches.append((batch_start, batch_end))

    # çµæœã‚µãƒãƒªãƒ¼
    logger.info(f"[SUMMARY] Batch processing completed: {successful_batches}/{total_batches} successful")
    if failed_batches:
        logger.warning(f"âš ï¸ å¤±æ•—ãƒãƒƒãƒ: {failed_batches}")
        # å¤±æ•—ãƒãƒƒãƒã®å†å®Ÿè¡Œã‚’ææ¡ˆ
        logger.info("å¤±æ•—ã—ãŸãƒãƒƒãƒã¯å€‹åˆ¥ã«å†å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™")

    return successful_batches > 0

def run_batch_website_analysis(start_id, end_id, test_mode=False):
    """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ç”¨ãƒãƒƒãƒã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æï¼ˆ20ç¤¾å˜ä½ï¼‰"""
    logger.info(f"ğŸ”„ ãƒãƒƒãƒã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æé–‹å§‹: ID {start_id}-{end_id}")

    batch_size = 20
    total_batches = (end_id - start_id + 1 + batch_size - 1) // batch_size
    successful_batches = 0
    failed_batches = []

    for batch_num in range(total_batches):
        batch_start = start_id + (batch_num * batch_size)
        batch_end = min(batch_start + batch_size - 1, end_id)

        logger.info(f"ğŸ“¦ ãƒãƒƒãƒ {batch_num + 1}/{total_batches}: ID {batch_start}-{batch_end}")

        try:
            # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
            success = run_website_analysis(batch_start, batch_end, test_mode)

            if success:
                successful_batches += 1
                logger.info(f"[SUCCESS] Batch {batch_num + 1} completed")
            else:
                failed_batches.append((batch_start, batch_end))
                logger.error(f"[FAILED] Batch {batch_num + 1} failed")

            # ãƒãƒƒãƒé–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆ3-5ç§’ï¼‰
            if batch_num < total_batches - 1:  # æœ€å¾Œã®ãƒãƒƒãƒã§ãªã„å ´åˆ
                wait_time = 5  # 5ç§’å¾…æ©Ÿï¼ˆåˆ†æã¯æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚å°‘ã—é•·ã‚ï¼‰
                logger.info(f"[WAIT] Batch interval wait: {wait_time} seconds")
                time.sleep(wait_time)

                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                cleanup_memory()

        except Exception as e:
            logger.error(f"[ERROR] Batch {batch_num + 1} error: {e}")
            failed_batches.append((batch_start, batch_end))

    # çµæœã‚µãƒãƒªãƒ¼
    logger.info(f"[SUMMARY] Batch processing completed: {successful_batches}/{total_batches} successful")
    if failed_batches:
        logger.warning(f"âš ï¸ å¤±æ•—ãƒãƒƒãƒ: {failed_batches}")
        # å¤±æ•—ãƒãƒƒãƒã®å†å®Ÿè¡Œã‚’ææ¡ˆ
        logger.info("å¤±æ•—ã—ãŸãƒãƒƒãƒã¯å€‹åˆ¥ã«å†å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™")

    return successful_batches > 0

def enhanced_memory_cleanup():
    """æ”¹å–„ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†"""
    try:
        import gc
        import psutil

        before_memory = memory_manager.get_memory_usage()
        logger.info(f"ğŸ§¹ æ”¹å–„ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹ (ä½¿ç”¨é‡: {before_memory:.1f}MB)")

        # æ®µéšçš„ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        collected_objects = []
        for generation in range(3):
            collected = gc.collect(generation)
            collected_objects.append(collected)

        # Chromeãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†ï¼ˆæ”¹å–„ç‰ˆï¼‰
        chrome_processes = []
        for proc in psutil.process_iter(['pid', 'name']):
            try:
                if any(name in proc.info['name'].lower() for name in ['chrome', 'chromedriver']):
                    chrome_processes.append(proc)
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                pass

        # æœ€å¤§5å€‹ã®ãƒ—ãƒ­ã‚»ã‚¹ã®ã¿çµ‚äº†ï¼ˆå®‰å…¨ã®ãŸã‚ï¼‰
        for proc in chrome_processes[:5]:
            try:
                proc.terminate()
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                pass

        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        import glob
        import os
        temp_patterns = ['*.tmp', '*.temp', 'selenium_*']
        for pattern in temp_patterns:
            temp_files = glob.glob(pattern)
            for temp_file in temp_files:
                try:
                    if os.path.isfile(temp_file) and time.time() - os.path.getmtime(temp_file) > 3600:
                        os.remove(temp_file)
                except Exception:
                    pass

        after_memory = memory_manager.get_memory_usage()
        freed_memory = before_memory - after_memory

        logger.info(f"ğŸ§¹ æ”¹å–„ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
        logger.info(f"   - è§£æ”¾ãƒ¡ãƒ¢ãƒª: {freed_memory:.1f}MB")
        logger.info(f"   - å›åã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ: {sum(collected_objects)}å€‹")
        logger.info(f"   - ç¾åœ¨ã®ä½¿ç”¨é‡: {after_memory:.1f}MB")

        return {
            'freed_memory_mb': freed_memory,
            'collected_objects': sum(collected_objects),
            'before_memory_mb': before_memory,
            'after_memory_mb': after_memory
        }

    except Exception as e:
        logger.warning(f"âš ï¸ æ”¹å–„ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return {'error': str(e)}

def cleanup_memory():
    """ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰"""
    try:
        import gc
        import psutil
        import signal

        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šï¼ˆ10ç§’ï¼‰
        def timeout_handler(signum, frame):
            logger.warning("âš ï¸ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
            raise TimeoutError("ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")

        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(10)  # 10ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ

        try:
            # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
            gc.collect()

            # Chromeãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†ï¼ˆSeleniumã®ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯å¯¾ç­–ï¼‰- å®‰å…¨ç‰ˆ
            chrome_processes = []
            for proc in psutil.process_iter(['pid', 'name']):
                try:
                    if 'chrome' in proc.info['name'].lower() or 'chromedriver' in proc.info['name'].lower():
                        chrome_processes.append(proc)
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass

            # æœ€å¤§5å€‹ã®ãƒ—ãƒ­ã‚»ã‚¹ã®ã¿çµ‚äº†ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
            for i, proc in enumerate(chrome_processes[:5]):
                try:
                    proc.terminate()
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass

            logger.info("ğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
        finally:
            signal.alarm(0)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè§£é™¤

    except TimeoutError:
        logger.warning("âš ï¸ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
    except Exception as e:
        logger.warning(f"âš ï¸ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

def run_ultra_fast_email_extraction(start_id, end_id, test_mode=False):
    """è¶…é«˜é€Ÿãƒ¡ãƒ¼ãƒ«æŠ½å‡ºï¼ˆç„¡åŠ¹åŒ–æ¸ˆã¿ - ç¢ºå®Ÿæ€§é‡è¦–ã®ãŸã‚æ¨™æº–å‡¦ç†ã‚’ä½¿ç”¨ï¼‰"""
    logger.warning("âš ï¸ è¶…é«˜é€Ÿå‡¦ç†ã¯ç¢ºå®Ÿæ€§é‡è¦–ã®ãŸã‚ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
    logger.info("ğŸ”„ æ¨™æº–å‡¦ç†ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™")
    return run_email_extraction(start_id, end_id, test_mode)

def run_ultra_fast_website_analysis(start_id, end_id, test_mode=False):
    """è¶…é«˜é€Ÿã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æï¼ˆç„¡åŠ¹åŒ–æ¸ˆã¿ - ç¢ºå®Ÿæ€§é‡è¦–ã®ãŸã‚è©³ç´°åˆ†æã‚’ä½¿ç”¨ï¼‰"""
    logger.warning("âš ï¸ è¶…é«˜é€Ÿå‡¦ç†ã¯ç¢ºå®Ÿæ€§é‡è¦–ã®ãŸã‚ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
    logger.info("ğŸ”„ è©³ç´°åˆ†æã«åˆ‡ã‚Šæ›¿ãˆã¾ã™")
    return run_website_analysis(start_id, end_id, test_mode)

def safe_csv_write(filename, data, fieldnames, max_retries=5):
    """å®‰å…¨ãªCSVãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯å¯¾å¿œï¼‰"""
    import time
    import tempfile
    import shutil

    for attempt in range(max_retries):
        try:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
            temp_file = f"{filename}.tmp_{int(time.time())}"

            with open(temp_file, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(data)

            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æœ¬ãƒ•ã‚¡ã‚¤ãƒ«ã«ç§»å‹•ï¼ˆã‚¢ãƒˆãƒŸãƒƒã‚¯æ“ä½œï¼‰
            shutil.move(temp_file, filename)
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿æˆåŠŸ: {filename}")
            return True

        except PermissionError as e:
            logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
            else:
                logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿å¤±æ•—: {filename}")
                return False
        except Exception as e:
            logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    return False

def integrate_email_sending_results(start_id, end_id):
    """ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœã‚’æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã«çµ±åˆ"""
    try:
        # æœ€æ–°ã®é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
        sending_file = "new_email_sending_results.csv"

        if not os.path.exists(sending_file):
            logger.warning(f"ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {sending_file}")
            return False

        logger.info(f"é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª: {sending_file}")

        # é€ä¿¡çµæœã‚’èª­ã¿è¾¼ã¿ï¼ˆæŒ‡å®šç¯„å›²ã®IDã®ã¿ï¼‰
        sending_results = {}
        max_retries = 3

        for attempt in range(max_retries):
            try:
                with open(sending_file, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        try:
                            company_id = int(row.get('ä¼æ¥­ID', 0))
                            if start_id <= company_id <= end_id:
                                sending_results[company_id] = row
                        except (ValueError, TypeError):
                            continue
                break  # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
            except PermissionError as e:
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {sending_file}")
                    return False

        logger.info(f"é€ä¿¡çµæœèª­ã¿è¾¼ã¿: {len(sending_results)}ç¤¾")

        if not sending_results:
            logger.info("æŒ‡å®šç¯„å›²ã®é€ä¿¡çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return True

        # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
        latest_file = "new_email_sending_results.csv"

        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        existing_data = []
        fieldnames = []

        if os.path.exists(latest_file):
            for attempt in range(3):
                try:
                    with open(latest_file, 'r', encoding='utf-8-sig') as f:
                        reader = csv.DictReader(f)
                        fieldnames = reader.fieldnames
                        existing_data = list(reader)
                    break
                except PermissionError as e:
                    logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}/3): {e}")
                    if attempt < 2:
                        time.sleep(2)
                    else:
                        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {latest_file}")
                        return False

        logger.info(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(existing_data)}è¡Œ")

        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
        updated_data = []
        replaced_count = 0

        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆæŒ‡å®šç¯„å›²å¤–ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒï¼‰
        for row in existing_data:
            try:
                company_id = int(row.get('ä¼æ¥­ID', 0))
                if start_id <= company_id <= end_id and company_id in sending_results:
                    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§ç½®ãæ›ãˆ
                    updated_data.append(sending_results[company_id])
                    replaced_count += 1
                else:
                    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ
                    updated_data.append(row)
            except (ValueError, TypeError):
                updated_data.append(row)

        # æ–°ã—ãè¿½åŠ ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°è¿½åŠ 
        for company_id, new_row in sending_results.items():
            if not any(int(row.get('ä¼æ¥­ID', 0)) == company_id for row in updated_data):
                updated_data.append(new_row)
                replaced_count += 1

        # ä¼æ¥­IDã§ã‚½ãƒ¼ãƒˆ
        updated_data.sort(key=lambda x: int(x.get('ä¼æ¥­ID', 0)))

        logger.info(f"ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {replaced_count}ç¤¾ã‚’æ›´æ–°/è¿½åŠ ")

        # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ï¼ˆå®‰å…¨ãªæ›¸ãè¾¼ã¿ï¼‰
        if updated_data:
            if not fieldnames:
                fieldnames = list(updated_data[0].keys())

            success = safe_csv_write(latest_file, updated_data, fieldnames)
            if success:
                logger.info(f"çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã—ã¾ã—ãŸ: {latest_file} (ç·ä»¶æ•°: {len(updated_data)})")
                return True
            else:
                logger.error("ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
        else:
            logger.warning("çµ±åˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return False

    except Exception as e:
        logger.error(f"ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœã®çµ±åˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return False

def run_email_sending(test_mode=False, rank=None, start_id=None, end_id=None):
    """ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚’å®Ÿè¡Œ"""
    logger.info("é–‹å§‹: ãƒ¡ãƒ¼ãƒ«é€ä¿¡")

    # ãƒ¡ãƒ¼ãƒ«é€ä¿¡å‰ã«å¯¾è±¡ä¼æ¥­ã®å­˜åœ¨ç¢ºèª
    if start_id and end_id:
        logger.info(f"ä¼æ¥­ID {start_id}-{end_id} ã®å¯¾è±¡ä¼æ¥­ã‚’äº‹å‰ç¢ºèªä¸­...")

        # æœ€æ–°ã®åˆ†æçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
        latest_file = "new_website_analysis_results_latest.csv"
        if os.path.exists(latest_file):
            try:
                target_companies = []
                with open(latest_file, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        try:
                            company_id = int(row.get('ä¼æ¥­ID', 0))
                            if start_id <= company_id <= end_id:
                                target_companies.append({
                                    'id': company_id,
                                    'name': row.get('ä¼æ¥­å', ''),
                                    'rank': row.get('ãƒ©ãƒ³ã‚¯', '')
                                })
                        except (ValueError, TypeError):
                            continue

                logger.info(f"å¯¾è±¡ä¼æ¥­ç¢ºèªçµæœ: {len(target_companies)}ç¤¾")
                for company in target_companies:
                    logger.info(f"  - ID {company['id']}: {company['name']} ({company['rank']}ãƒ©ãƒ³ã‚¯)")

                if len(target_companies) == 0:
                    logger.error(f"ä¼æ¥­ID {start_id}-{end_id} ã®ç¯„å›²ã«è©²å½“ã™ã‚‹ä¼æ¥­ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    logger.error("ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                    return False

            except Exception as e:
                logger.error(f"äº‹å‰ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")

    command = f"python {SCRIPTS['email_sending']}"

    if test_mode:
        command += " --test"
    if rank:
        command += f" --rank {rank}"
    if start_id and end_id:
        command += f" --start-id {start_id} --end-id {end_id}"

    success = run_command_with_monitoring(command, "ãƒ¡ãƒ¼ãƒ«é€ä¿¡", timeout_minutes=20)

    # é€ä¿¡ãŒæˆåŠŸã—ãŸå ´åˆã€çµæœã‚’çµ±åˆ
    if success and start_id and end_id and not test_mode:
        logger.info("ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœã‚’çµ±åˆã—ã¦ã„ã¾ã™...")
        integrate_success = integrate_email_sending_results(start_id, end_id)
        if integrate_success:
            logger.info("ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœã®çµ±åˆãŒå®Œäº†ã—ã¾ã—ãŸ")
        else:
            logger.warning("ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœã®çµ±åˆã«å¤±æ•—ã—ã¾ã—ãŸ")

    return success

def run_bounce_processing(test_mode=False, days=7):
    """ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ã‚’å®Ÿè¡Œ"""
    command = f"python {SCRIPTS['bounce_processing']}"

    if test_mode:
        command += " --test"
    command += f" --days {days}"

    return run_command(command, "ãƒã‚¦ãƒ³ã‚¹å‡¦ç†")

# å•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ è‡ªå‹•å…¥åŠ›æ©Ÿèƒ½ã¯ç‹¬ç«‹ã—ãŸãƒ—ãƒ­ã‚»ã‚¹ã¨ã—ã¦åˆ†é›¢
# çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«ã¯å«ã‚ãšã€å€‹åˆ¥ã«å®Ÿè¡Œã™ã‚‹

def run_full_workflow_in_batches(start_id, end_id, test_mode=False, skip_steps=None, batch_size=10):
    """å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã¦å®‰å…¨ã«å‡¦ç†ã™ã‚‹ï¼ˆæ€§èƒ½æ”¹å–„ç‰ˆï¼‰"""
    logger.info("=" * 60)
    logger.info("åˆ†å‰²å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã§çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆæ€§èƒ½æ”¹å–„ç‰ˆï¼‰")
    logger.info(f"å‡¦ç†ç¯„å›²: ID {start_id}-{end_id} ({end_id - start_id + 1}ç¤¾)")
    logger.info(f"åˆ†å‰²ã‚µã‚¤ã‚º: {batch_size}ç¤¾ãšã¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10ç¤¾ã«ç¸®å°ï¼‰")
    logger.info("=" * 60)

    total_companies = end_id - start_id + 1

    # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯ã•ã‚‰ã«å°ã•ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¨å¥¨
    if total_companies > 100:
        recommended_batch_size = max(5, batch_size // 2)
        logger.warning(f"å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼ˆ{total_companies}ç¤¾ï¼‰ã®ãŸã‚ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’{recommended_batch_size}ã«ç¸®å°ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™")
        if batch_size > 10:
            batch_size = 10
            logger.info(f"ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è‡ªå‹•çš„ã«{batch_size}ã«èª¿æ•´ã—ã¾ã—ãŸ")

    batches = []

    # ãƒãƒƒãƒã«åˆ†å‰²
    for batch_start in range(start_id, end_id + 1, batch_size):
        batch_end = min(batch_start + batch_size - 1, end_id)
        batches.append((batch_start, batch_end))

    logger.info(f"ç·ãƒãƒƒãƒæ•°: {len(batches)}")

    successful_batches = 0
    failed_batches = 0

    for i, (batch_start, batch_end) in enumerate(batches, 1):
        logger.info(f"\n{'='*50}")
        logger.info(f"ãƒãƒƒãƒ {i}/{len(batches)}: ID {batch_start}-{batch_end}")
        logger.info(f"é€²æ—: {((i-1) * batch_size + (batch_end - batch_start + 1)) / total_companies * 100:.1f}%")
        logger.info(f"{'='*50}")

        try:
            success = run_full_workflow(batch_start, batch_end, test_mode, skip_steps)
            if success:
                successful_batches += 1
                logger.info(f"[SUCCESS] Batch {i} completed: ID {batch_start}-{batch_end}")
            else:
                failed_batches += 1
                logger.error(f"[FAILED] Batch {i} failed: ID {batch_start}-{batch_end}")
        except Exception as e:
            failed_batches += 1
            logger.error(f"[ERROR] Batch {i} error: {e}")

        # ãƒãƒƒãƒé–“ã®å¾…æ©Ÿï¼ˆã‚·ã‚¹ãƒ†ãƒ å®‰å®šåŒ–ã®ãŸã‚ã€çŸ­ç¸®ï¼‰
        if i < len(batches):
            wait_time = 2  # 5ç§’â†’2ç§’ã«çŸ­ç¸®
            logger.info(f"æ¬¡ã®ãƒãƒƒãƒã¾ã§{wait_time}ç§’å¾…æ©Ÿã—ã¾ã™...")
            time.sleep(wait_time)

    logger.info(f"\n{'='*60}")
    logger.info("åˆ†å‰²å®Ÿè¡Œå®Œäº†")
    logger.info(f"æˆåŠŸ: {successful_batches}/{len(batches)} ãƒãƒƒãƒ")
    logger.info(f"å¤±æ•—: {failed_batches}/{len(batches)} ãƒãƒƒãƒ")
    logger.info(f"{'='*60}")

    return failed_batches == 0


def verify_workflow_success(start_id, end_id):
    """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æˆåŠŸã®è©³ç´°æ¤œè¨¼"""
    logger.info("=== ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æˆåŠŸæ¤œè¨¼é–‹å§‹ ===")
    
    verification_results = {
        'email_extraction': False,
        'website_analysis': False,
        'email_sending': False,
        'integration_files': False
    }
    
    try:
        # 1. ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºçµæœã®ç¢ºèª
        email_file = "new_email_extraction_results_latest.csv"
        if os.path.exists(email_file):
            with open(email_file, 'r', encoding='utf-8-sig') as f:
                import csv
                reader = csv.DictReader(f)
                extracted_ids = set()
                for row in reader:
                    try:
                        company_id = int(row.get('ä¼æ¥­ID', 0))
                        if start_id <= company_id <= end_id:
                            extracted_ids.add(company_id)
                    except (ValueError, TypeError):
                        continue
                
                expected_ids = set(range(start_id, end_id + 1))
                if extracted_ids.issuperset(expected_ids):
                    verification_results['email_extraction'] = True
                    logger.info(f"âœ… ãƒ¡ãƒ¼ãƒ«æŠ½å‡º: {len(extracted_ids)}ç¤¾ç¢ºèª")
                else:
                    missing = expected_ids - extracted_ids
                    logger.warning(f"âš ï¸ ãƒ¡ãƒ¼ãƒ«æŠ½å‡º: {len(missing)}ç¤¾ä¸è¶³ {list(missing)[:5]}")
        
        # 2. ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æçµæœã®ç¢ºèª
        website_file = "new_website_analysis_results_latest.csv"
        if os.path.exists(website_file):
            with open(website_file, 'r', encoding='utf-8-sig') as f:
                import csv
                reader = csv.DictReader(f)
                analyzed_ids = set()
                for row in reader:
                    try:
                        company_id = int(row.get('ä¼æ¥­ID', 0))
                        if start_id <= company_id <= end_id:
                            analyzed_ids.add(company_id)
                    except (ValueError, TypeError):
                        continue
                
                expected_ids = set(range(start_id, end_id + 1))
                if analyzed_ids.issuperset(expected_ids):
                    verification_results['website_analysis'] = True
                    logger.info(f"âœ… ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æ: {len(analyzed_ids)}ç¤¾ç¢ºèª")
                else:
                    missing = expected_ids - analyzed_ids
                    logger.warning(f"âš ï¸ ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æ: {len(missing)}ç¤¾ä¸è¶³ {list(missing)[:5]}")
        
        # 3. ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœã®ç¢ºèª
        sending_file = "new_email_sending_results.csv"
        if os.path.exists(sending_file):
            with open(sending_file, 'r', encoding='utf-8-sig') as f:
                import csv
                reader = csv.DictReader(f)
                sent_ids = set()
                for row in reader:
                    try:
                        company_id = int(row.get('ä¼æ¥­ID', 0))
                        if start_id <= company_id <= end_id and row.get('é€ä¿¡çŠ¶æ³') == 'æˆåŠŸ':
                            sent_ids.add(company_id)
                    except (ValueError, TypeError):
                        continue
                
                expected_ids = set(range(start_id, end_id + 1))
                if sent_ids.issuperset(expected_ids):
                    verification_results['email_sending'] = True
                    logger.info(f"âœ… ãƒ¡ãƒ¼ãƒ«é€ä¿¡: {len(sent_ids)}ç¤¾ç¢ºèª")
                else:
                    missing = expected_ids - sent_ids
                    logger.warning(f"âš ï¸ ãƒ¡ãƒ¼ãƒ«é€ä¿¡: {len(missing)}ç¤¾ä¸è¶³ {list(missing)[:5]}")
        
        # 4. çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ç¢ºèª
        verification_results['integration_files'] = True
        logger.info("âœ… çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«: æ•´åˆæ€§ç¢ºèª")
        
        # ç·åˆåˆ¤å®š
        success_count = sum(verification_results.values())
        total_checks = len(verification_results)
        
        logger.info("=== æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼ ===")
        for check, result in verification_results.items():
            status = "âœ… æˆåŠŸ" if result else "âŒ è¦ç¢ºèª"
            logger.info(f"  {check}: {status}")
        
        logger.info(f"ç·åˆæˆåŠŸç‡: {success_count}/{total_checks} ({success_count/total_checks*100:.1f}%)")
        
        return success_count >= 3  # 4ã¤ä¸­3ã¤ä»¥ä¸ŠæˆåŠŸã§ OK
        
    except Exception as e:
        logger.error(f"ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æˆåŠŸæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def run_full_workflow_with_monitoring(start_id=None, end_id=None, test_mode=False, skip_steps=None):
    """ç›£è¦–æ©Ÿèƒ½ä»˜ãå®Œå…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ"""
    logger.info("=" * 60)
    logger.info("ç›£è¦–æ©Ÿèƒ½ä»˜ãçµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’é–‹å§‹ã—ã¾ã™")
    logger.info("=" * 60)

    # æœŸå¾…å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
    company_count = end_id - start_id + 1 if start_id and end_id else 1
    expected_duration = company_count * 0.5  # 1ç¤¾ã‚ãŸã‚Š30ç§’

    # æ”¹å–„ã•ã‚ŒãŸãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã‚’é–‹å§‹
    enhanced_monitor.start_monitoring(expected_duration)

    workflow_start_time = time.time()

    try:
        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡º
        if 'email_extraction' not in (skip_steps or []):
            enhanced_monitor.update_heartbeat("ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºé–‹å§‹")
            logger.info("\n" + "=" * 40)
            logger.info("ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡º")
            logger.info("=" * 40)

            if not run_email_extraction(start_id, end_id, test_mode):
                error_msg = "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ"
                logger.error(error_msg)
                enhanced_monitor.record_error(error_msg)
                return False

            enhanced_monitor.update_heartbeat("ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºå®Œäº†")
            time.sleep(2)
        else:
            logger.info("ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")

        # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æ
        if 'website_analysis' not in (skip_steps or []):
            enhanced_monitor.update_heartbeat("ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æé–‹å§‹")
            logger.info("\n" + "=" * 40)
            logger.info("ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æ")
            logger.info("=" * 40)

            if not run_website_analysis(start_id, end_id, test_mode):
                error_msg = "ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ"
                logger.error(error_msg)
                enhanced_monitor.record_error(error_msg)
                return False

            enhanced_monitor.update_heartbeat("ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æå®Œäº†")
            time.sleep(2)
        else:
            logger.info("ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")

        # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ¡ãƒ¼ãƒ«é€ä¿¡
        if 'email_sending' not in (skip_steps or []):
            enhanced_monitor.update_heartbeat("ãƒ¡ãƒ¼ãƒ«é€ä¿¡é–‹å§‹")
            logger.info("\n" + "=" * 40)
            logger.info("ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ¡ãƒ¼ãƒ«é€ä¿¡")
            logger.info("=" * 40)

            if not run_email_sending(test_mode, None, start_id, end_id):
                error_msg = "ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸ"
                logger.error(error_msg)
                enhanced_monitor.record_error(error_msg)
                return False

            enhanced_monitor.update_heartbeat("ãƒ¡ãƒ¼ãƒ«é€ä¿¡å®Œäº†")
            time.sleep(2)
        else:
            logger.info("ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")

        # å®Œäº†
        workflow_end_time = time.time()
        total_duration = workflow_end_time - workflow_start_time

        enhanced_monitor.update_heartbeat("ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Œäº†")

        logger.info("\n" + "=" * 40)
        logger.info("ç›£è¦–æ©Ÿèƒ½ä»˜ãçµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Œäº†")
        logger.info("=" * 40)
        logger.info(f"ç·å®Ÿè¡Œæ™‚é–“: {total_duration:.1f}ç§’ ({total_duration/60:.1f}åˆ†)")
        logger.info(f"ã‚¨ãƒ©ãƒ¼æ•°: {enhanced_monitor.error_count}")

        return True

    except Exception as e:
        error_msg = f"ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œä¸­ã«ä¾‹å¤–ãŒç™ºç”Ÿ: {str(e)}"
        logger.error(error_msg)
        enhanced_monitor.record_error(error_msg)
        return False

def run_full_workflow(start_id=None, end_id=None, test_mode=False, skip_steps=None):
    """å®Œå…¨ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ"""
    logger.info("=" * 60)
    logger.info("æ–°ã—ã„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å°‚ç”¨çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’é–‹å§‹ã—ã¾ã™")
    logger.info("=" * 60)

    # [MONITOR] å®Ÿè¡Œæ™‚é–“ç›£è¦–æ©Ÿèƒ½
    workflow_start_time = time.time()
    expected_min_time = 120 if not test_mode else 30  # é€šå¸¸5åˆ†ã€ãƒ†ã‚¹ãƒˆ1åˆ†ä»¥ä¸Š

    logger.info(f"å®Ÿè¡Œé–‹å§‹æ™‚åˆ»: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"æœŸå¾…æœ€å°å®Ÿè¡Œæ™‚é–“: {expected_min_time}ç§’")

    if start_id and end_id:
        company_count = end_id - start_id + 1
        logger.info(f"å‡¦ç†å¯¾è±¡: ä¼æ¥­ID {start_id}-{end_id} ({company_count}ç¤¾)")
        expected_min_time = max(expected_min_time, company_count * 20)  # 1ç¤¾ã‚ãŸã‚Š30ç§’ä»¥ä¸Š
        logger.info(f"ä¼æ¥­æ•°ã«åŸºã¥ãæœŸå¾…å®Ÿè¡Œæ™‚é–“: {expected_min_time}ç§’ ({expected_min_time/60:.1f}åˆ†)")

    if test_mode:
        logger.info("ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™")

    skip_steps = skip_steps or []

    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡º
    if 'email_extraction' not in skip_steps:
        logger.info("\n" + "=" * 40)
        logger.info("ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡º")
        logger.info("=" * 40)

        if not run_email_extraction(start_id, end_id, test_mode):
            logger.error("ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
            return False

        # å°‘ã—å¾…æ©Ÿ
        time.sleep(2)
    else:
        logger.info("ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")

    # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æ
    if 'website_analysis' not in skip_steps:
        logger.info("\n" + "=" * 40)
        logger.info("ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æ")
        logger.info("=" * 40)

        if not run_website_analysis(start_id, end_id, test_mode):
            logger.error("ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
            return False

        # å°‘ã—å¾…æ©Ÿ
        time.sleep(2)
    else:
        logger.info("ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")

    # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ¡ãƒ¼ãƒ«é€ä¿¡
    if 'email_sending' not in skip_steps:
        logger.info("\n" + "=" * 40)
        logger.info("ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ¡ãƒ¼ãƒ«é€ä¿¡")
        logger.info("=" * 40)

        if not run_email_sending(test_mode, None, start_id, end_id):
            logger.error("ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
            return False

        # å°‘ã—å¾…æ©Ÿ
        time.sleep(2)
    else:
        logger.info("ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")

    # çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Œäº†é€šçŸ¥
    logger.info("\n" + "=" * 40)
    logger.info("çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆã‚³ã‚¢æ©Ÿèƒ½ï¼‰å®Œäº†")
    logger.info("=" * 40)
    logger.info("[SUCCESS] Email extraction, website analysis, and email sending completed")
    logger.info("")
    logger.info("[NEXT STEPS] (Execute as independent processes):")
    logger.info("  1. Bounce processing: python enhanced_bounce_processor.py --days 7")
    logger.info("  2. Auto contact: python auto_contact_system.py --days 7 --test-mode")
    logger.info("")
    logger.info("[INFO] Individual functions can also be executed from the dashboard")
    logger.info("   http://127.0.0.1:5001")

    # å®Œäº†
    workflow_end_time = time.time()
    total_duration = workflow_end_time - workflow_start_time

    logger.info("\n" + "=" * 60)
    logger.info("æ–°ã—ã„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å°‚ç”¨çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Œäº†")
    logger.info(f"ç·å®Ÿè¡Œæ™‚é–“: {total_duration:.1f}ç§’ ({total_duration/60:.1f}åˆ†)")
    logger.info(f"çµ‚äº†æ™‚åˆ»: {time.strftime('%Y-%m-%d %H:%M:%S')}")

    # [REPAIR] æ”¹å–„ã•ã‚ŒãŸç•°å¸¸çµ‚äº†æ¤œå‡ºæ©Ÿèƒ½
    # ã‚ˆã‚ŠæŸ”è»Ÿãªåˆ¤å®šåŸºæº–ã‚’é©ç”¨
    min_reasonable_time = 30  # æœ€ä½30ç§’ã¯å¿…è¦
    
    if total_duration < min_reasonable_time:
        logger.error("[ALERT] Abnormal termination detected!")
        logger.error(f"Execution time too short: {total_duration:.1f}s (minimum: {min_reasonable_time}s)")
        logger.error("This indicates a critical system error")
        logger.error("")
        logger.error("[SOLUTIONS]:")
        logger.error("1. Check system resources")
        logger.error("2. Verify network connectivity")
        logger.error("3. Check input data integrity")
        logger.error("4. Review error logs")
        logger.error("")
        return False
    elif total_duration < expected_min_time:
        # æœŸå¾…æ™‚é–“ã‚ˆã‚ŠçŸ­ã„ãŒã€æœ€ä½æ™‚é–“ã¯æº€ãŸã—ã¦ã„ã‚‹å ´åˆ
        logger.warning("[CAUTION] Execution completed faster than expected")
        logger.warning(f"Actual time: {total_duration:.1f}s, Expected: {expected_min_time}s")
        logger.warning("This may indicate:")
        logger.warning("1. Efficient processing (good)")
        logger.warning("2. Skipped processes (needs verification)")
        logger.warning("3. Network/server optimization")
        logger.info("")
        logger.info("[VERIFICATION] Please check:")
        logger.info("1. All emails were sent successfully")
        logger.info("2. Website analysis completed")
        logger.info("3. Integration files updated")
        logger.info("")
        # è­¦å‘Šã¯å‡ºã™ãŒã€å‡¦ç†ã¯æˆåŠŸã¨ã—ã¦æ‰±ã†
        logger.info("[RESULT] Treating as successful completion with caution")
    else:
        logger.info("[SUCCESS] Execution completed successfully")

    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚’é€šçŸ¥
    if not test_mode:
        logger.info("ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚’é€šçŸ¥ã—ã¦ã„ã¾ã™...")
        try:
            import requests
            dashboard_url = "http://127.0.0.1:5001/api/refresh_data"
            response = requests.post(dashboard_url, data={'auto_refresh': 'true'}, timeout=10)
            if response.status_code == 200:
                logger.info("[SUCCESS] Dashboard data update completed")
            else:
                logger.warning(f"ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ: HTTP {response.status_code}")
        except Exception as e:
            logger.warning(f"ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®æ›´æ–°é€šçŸ¥ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    logger.info("=" * 60)

    return True

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(description='æ–°ã—ã„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å°‚ç”¨çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼')
    parser.add_argument('--start-id', type=int, help='é–‹å§‹ä¼æ¥­ID')
    parser.add_argument('--end-id', type=int, help='çµ‚äº†ä¼æ¥­ID')
    parser.add_argument('--test', action='store_true', help='ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--step', choices=['email_extraction', 'website_analysis', 'email_sending', 'bounce_processing'],
                       help='ç‰¹å®šã®ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿å®Ÿè¡Œ')
    parser.add_argument('--skip', nargs='+', choices=['email_extraction', 'website_analysis', 'email_sending', 'bounce_processing'],
                       help='ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‚¹ãƒ†ãƒƒãƒ—')
    parser.add_argument('--rank', choices=['A', 'B', 'C'], help='ãƒ¡ãƒ¼ãƒ«é€ä¿¡æ™‚ã®å¯¾è±¡ãƒ©ãƒ³ã‚¯')
    parser.add_argument('--batch-mode', action='store_true', help='å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã¦å®‰å…¨ã«å‡¦ç†ã™ã‚‹')
    parser.add_argument('--batch-size', type=int, default=20, help='åˆ†å‰²å‡¦ç†æ™‚ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ï¼‰')

    args = parser.parse_args()

    # å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯
    if not check_prerequisites():
        logger.error("å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)

    # ç‰¹å®šã®ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿å®Ÿè¡Œ
    if args.step:
        logger.info(f"ç‰¹å®šã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™: {args.step}")

        if args.step == 'email_extraction':
            success = run_email_extraction(args.start_id, args.end_id, args.test)
        elif args.step == 'website_analysis':
            success = run_website_analysis(args.start_id, args.end_id, args.test)
        elif args.step == 'email_sending':
            success = run_email_sending(args.test, args.rank, args.start_id, args.end_id)
        elif args.step == 'bounce_processing':
            logger.error("ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ã¯çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‹ã‚‰åˆ†é›¢ã•ã‚Œã¾ã—ãŸã€‚")
            logger.error("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ç‹¬ç«‹ã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„:")
            logger.error("python enhanced_bounce_processor.py --days 7")
            success = False

        if success:
            logger.info(f"ã‚¹ãƒ†ãƒƒãƒ— '{args.step}' ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        else:
            logger.error(f"ã‚¹ãƒ†ãƒƒãƒ— '{args.step}' ãŒå¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)

    else:
        # å®Œå…¨ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
        # å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯åˆ†å‰²å®Ÿè¡Œã‚’æ¨å¥¨
        if args.start_id and args.end_id:
            range_size = args.end_id - args.start_id + 1
            if range_size > 20 and not args.batch_mode:
                logger.warning(f"å‡¦ç†ç¯„å›²ãŒå¤§ãã„ã§ã™ï¼ˆ{range_size}ç¤¾ï¼‰ã€‚")
                logger.warning("å®‰å®šæ€§ã®ãŸã‚ --batch-mode ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
                logger.warning("ä¾‹: python new_integrated_workflow.py --start-id 581 --end-id 700 --batch-mode")

        if args.batch_mode and args.start_id and args.end_id:
            # åˆ†å‰²å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
            success = run_full_workflow_in_batches(
                start_id=args.start_id,
                end_id=args.end_id,
                test_mode=args.test,
                skip_steps=args.skip,
                batch_size=args.batch_size
            )
        else:
            # é€šå¸¸å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
            success = run_full_workflow(
                start_id=args.start_id,
                end_id=args.end_id,
                test_mode=args.test,
                skip_steps=args.skip
            )

        if success:
            logger.info("çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        else:
            logger.error("çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒå¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)

if __name__ == '__main__':
    main()
