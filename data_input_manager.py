#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
HUGAN JOB ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’çµ±åˆãƒ»ç®¡ç†ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«
"""

import os
import csv
import pandas as pd
import shutil
from datetime import datetime
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataInputManager:
    def __init__(self):
        self.supported_formats = ['.csv', '.xlsx', '.xls']
        self.required_columns = ['ä¼æ¥­å', 'URL']  # æœ€ä½é™å¿…è¦ãªã‚«ãƒ©ãƒ 
        self.optional_columns = ['æ¥­ç¨®', 'æ‰€åœ¨åœ°', 'å¾“æ¥­å“¡æ•°', 'è³‡æœ¬é‡‘', 'å£²ä¸Šé«˜', 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹']
        
    def analyze_input_file(self, file_path):
        """å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®æ§‹é€ ã‚’åˆ†æ"""
        print(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ: {file_path}")
        print("=" * 60)
        
        if not os.path.exists(file_path):
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return None
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
        _, ext = os.path.splitext(file_path)
        if ext.lower() not in self.supported_formats:
            print(f"âŒ ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {ext}")
            print(f"ã‚µãƒãƒ¼ãƒˆå½¢å¼: {', '.join(self.supported_formats)}")
            return None
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            if ext.lower() == '.csv':
                df = self._read_csv_with_encoding(file_path)
            else:
                df = pd.read_excel(file_path)
            
            if df is None:
                return None
            
            # åŸºæœ¬æƒ…å ±è¡¨ç¤º
            print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
            print(f"ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df)}ä»¶")
            print(f"ğŸ“‹ ã‚«ãƒ©ãƒ æ•°: {len(df.columns)}å€‹")
            
            # ã‚«ãƒ©ãƒ æƒ…å ±è¡¨ç¤º
            print(f"\nğŸ“‹ æ¤œå‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ :")
            for i, col in enumerate(df.columns, 1):
                sample_data = df[col].dropna().head(1).values
                sample = sample_data[0] if len(sample_data) > 0 else "ï¼ˆãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰"
                print(f"  {i:2d}. {col} - ä¾‹: {sample}")
            
            # å¿…é ˆã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯
            print(f"\nğŸ” å¿…é ˆã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯:")
            missing_required = []
            for req_col in self.required_columns:
                if req_col in df.columns:
                    print(f"  âœ… {req_col}")
                else:
                    print(f"  âŒ {req_col} (è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“)")
                    missing_required.append(req_col)
            
            # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯
            print(f"\nğŸ“‹ ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯:")
            for opt_col in self.optional_columns:
                if opt_col in df.columns:
                    non_null_count = df[opt_col].notna().sum()
                    print(f"  âœ… {opt_col} ({non_null_count}ä»¶ã®ãƒ‡ãƒ¼ã‚¿)")
                else:
                    print(f"  âšª {opt_col} (ãªã—)")
            
            # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
            print(f"\nğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯:")
            if 'ä¼æ¥­å' in df.columns:
                empty_names = df['ä¼æ¥­å'].isna().sum()
                print(f"  ä¼æ¥­å: {len(df) - empty_names}/{len(df)}ä»¶ (ç©ºç™½: {empty_names}ä»¶)")
            
            if 'URL' in df.columns:
                empty_urls = df['URL'].isna().sum()
                valid_urls = df['URL'].str.contains('http', na=False).sum()
                print(f"  URL: {len(df) - empty_urls}/{len(df)}ä»¶ (ç©ºç™½: {empty_urls}ä»¶, httpå«ã‚€: {valid_urls}ä»¶)")
            
            return {
                'dataframe': df,
                'file_path': file_path,
                'columns': list(df.columns),
                'row_count': len(df),
                'missing_required': missing_required,
                'has_email': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹' in df.columns or 'æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹' in df.columns
            }
            
        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _read_csv_with_encoding(self, file_path):
        """è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        encodings = ['utf-8-sig', 'utf-8', 'shift_jis', 'cp932', 'iso-2022-jp']
        
        for encoding in encodings:
            try:
                df = pd.read_csv(file_path, encoding=encoding)
                print(f"  ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {encoding}")
                return df
            except UnicodeDecodeError:
                continue
            except Exception as e:
                logger.error(f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({encoding}): {e}")
                continue
        
        print(f"âŒ ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ")
        return None
    
    def standardize_data(self, analysis_result):
        """ãƒ‡ãƒ¼ã‚¿ã‚’æ¨™æº–å½¢å¼ã«å¤‰æ›"""
        print(f"\nğŸ”„ ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–å‡¦ç†")
        print("=" * 60)
        
        if not analysis_result:
            return None
        
        df = analysis_result['dataframe'].copy()
        
        # ã‚«ãƒ©ãƒ åã®ãƒãƒƒãƒ”ãƒ³ã‚°
        column_mapping = {
            'äº‹å‹™æ‰€å': 'ä¼æ¥­å',
            'ä¼šç¤¾å': 'ä¼æ¥­å',
            'æ³•äººå': 'ä¼æ¥­å',
            'ä¼æ¥­URL': 'URL',
            'ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ': 'URL',
            'ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸': 'URL',
            'æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
            'Email': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
            'email': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
            'E-mail': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'
        }
        
        # ã‚«ãƒ©ãƒ åã‚’æ¨™æº–åŒ–
        df = df.rename(columns=column_mapping)
        
        # å¿…é ˆã‚«ãƒ©ãƒ ã®ç¢ºèªã¨ä½œæˆ
        if 'ä¼æ¥­å' not in df.columns:
            # æœ€åˆã®ã‚«ãƒ©ãƒ ã‚’ä¼æ¥­åã¨ã—ã¦ä½¿ç”¨
            if len(df.columns) > 0:
                first_col = df.columns[0]
                df = df.rename(columns={first_col: 'ä¼æ¥­å'})
                print(f"  ğŸ“ '{first_col}' ã‚’ 'ä¼æ¥­å' ã¨ã—ã¦ä½¿ç”¨")
        
        if 'URL' not in df.columns:
            # URLé–¢é€£ã®ã‚«ãƒ©ãƒ ã‚’æ¢ã™
            url_candidates = [col for col in df.columns if 'url' in col.lower() or 'http' in str(df[col].iloc[0] if len(df) > 0 else '').lower()]
            if url_candidates:
                df = df.rename(columns={url_candidates[0]: 'URL'})
                print(f"  ğŸ“ '{url_candidates[0]}' ã‚’ 'URL' ã¨ã—ã¦ä½¿ç”¨")
            else:
                df['URL'] = ''  # ç©ºã®URLã‚«ãƒ©ãƒ ã‚’ä½œæˆ
                print(f"  ğŸ“ ç©ºã® 'URL' ã‚«ãƒ©ãƒ ã‚’ä½œæˆ")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        print(f"\nğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°:")
        
        # ä¼æ¥­åã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        if 'ä¼æ¥­å' in df.columns:
            original_count = len(df)
            df = df[df['ä¼æ¥­å'].notna() & (df['ä¼æ¥­å'] != '')]
            cleaned_count = len(df)
            if original_count != cleaned_count:
                print(f"  ä¼æ¥­åãŒç©ºã®è¡Œã‚’å‰Šé™¤: {original_count - cleaned_count}ä»¶")
        
        # URLã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        if 'URL' in df.columns:
            # URLã®æ­£è¦åŒ–
            df['URL'] = df['URL'].fillna('')
            df['URL'] = df['URL'].astype(str)
            
            # httpãŒå«ã¾ã‚Œã¦ã„ãªã„URLã«https://ã‚’è¿½åŠ 
            mask = (df['URL'] != '') & (~df['URL'].str.contains('http', na=False))
            df.loc[mask, 'URL'] = 'https://' + df.loc[mask, 'URL']
            
            print(f"  URLæ­£è¦åŒ–å®Œäº†")
        
        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        if 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹' in df.columns:
            # ç„¡åŠ¹ãªãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’é™¤å»
            email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
            valid_emails = df['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].str.match(email_pattern, na=False)
            invalid_count = (~valid_emails & df['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].notna()).sum()
            if invalid_count > 0:
                df.loc[~valid_emails, 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] = ''
                print(f"  ç„¡åŠ¹ãªãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’ã‚¯ãƒªã‚¢: {invalid_count}ä»¶")
        
        # IDã‚«ãƒ©ãƒ ã®è¿½åŠ 
        df.insert(0, 'ID', range(1, len(df) + 1))
        
        print(f"âœ… æ¨™æº–åŒ–å®Œäº†: {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
        
        return df
    
    def merge_with_existing_data(self, new_df, existing_file='test_input.csv'):
        """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸"""
        print(f"\nğŸ”— ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸å‡¦ç†")
        print("=" * 60)
        
        if not os.path.exists(existing_file):
            print(f"  æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {existing_file}")
            print(f"  æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ä½¿ç”¨ã—ã¾ã™")
            return new_df
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        existing_analysis = self.analyze_input_file(existing_file)
        if not existing_analysis:
            print(f"  æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return new_df
        
        existing_df = self.standardize_data(existing_analysis)
        if existing_df is None:
            print(f"  æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return new_df
        
        print(f"  æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(existing_df)}ä»¶")
        print(f"  æ–°è¦ãƒ‡ãƒ¼ã‚¿: {len(new_df)}ä»¶")
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆä¼æ¥­åãƒ™ãƒ¼ã‚¹ï¼‰
        if 'ä¼æ¥­å' in existing_df.columns and 'ä¼æ¥­å' in new_df.columns:
            duplicates = new_df['ä¼æ¥­å'].isin(existing_df['ä¼æ¥­å'])
            duplicate_count = duplicates.sum()
            
            if duplicate_count > 0:
                print(f"  é‡è¤‡ä¼æ¥­ã‚’æ¤œå‡º: {duplicate_count}ä»¶")
                choice = input("  é‡è¤‡ä¼æ¥­ã®å‡¦ç†æ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„ (1: ã‚¹ã‚­ãƒƒãƒ—, 2: ä¸Šæ›¸ã, 3: ä¸¡æ–¹ä¿æŒ): ")
                
                if choice == '1':
                    new_df = new_df[~duplicates]
                    print(f"  é‡è¤‡ä¼æ¥­ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
                elif choice == '2':
                    existing_df = existing_df[~existing_df['ä¼æ¥­å'].isin(new_df['ä¼æ¥­å'])]
                    print(f"  é‡è¤‡ä¼æ¥­ã‚’ä¸Šæ›¸ãã—ã¾ã™")
                # choice == '3' ã®å ´åˆã¯ä½•ã‚‚ã—ãªã„ï¼ˆä¸¡æ–¹ä¿æŒï¼‰
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
        # IDã‚’å†æ¡ç•ª
        existing_df['ID'] = range(1, len(existing_df) + 1)
        new_df['ID'] = range(len(existing_df) + 1, len(existing_df) + len(new_df) + 1)
        
        # ã‚«ãƒ©ãƒ ã‚’çµ±ä¸€
        all_columns = list(set(existing_df.columns) | set(new_df.columns))
        for col in all_columns:
            if col not in existing_df.columns:
                existing_df[col] = ''
            if col not in new_df.columns:
                new_df[col] = ''
        
        # ã‚«ãƒ©ãƒ é †åºã‚’çµ±ä¸€
        existing_df = existing_df[all_columns]
        new_df = new_df[all_columns]
        
        # ãƒãƒ¼ã‚¸
        merged_df = pd.concat([existing_df, new_df], ignore_index=True)
        
        print(f"âœ… ãƒãƒ¼ã‚¸å®Œäº†: {len(merged_df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
        
        return merged_df
    
    def save_data(self, df, output_file=None):
        """ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        if output_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f'merged_input_{timestamp}.csv'
        
        print(f"\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ä¿å­˜")
        print("=" * 60)
        
        try:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            if os.path.exists('test_input.csv'):
                backup_file = f'test_input_backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
                shutil.copy2('test_input.csv', backup_file)
                print(f"  ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_file}")
            
            # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {output_file}")
            
            # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
            print(f"\nğŸ“Š ä¿å­˜ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
            print(f"  ç·ä»¶æ•°: {len(df)}ä»¶")
            
            if 'ä¼æ¥­å' in df.columns:
                print(f"  ä¼æ¥­åã‚ã‚Š: {df['ä¼æ¥­å'].notna().sum()}ä»¶")
            
            if 'URL' in df.columns:
                valid_urls = df['URL'].str.contains('http', na=False).sum()
                print(f"  æœ‰åŠ¹URL: {valid_urls}ä»¶")
            
            if 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹' in df.columns:
                valid_emails = df['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].notna().sum()
                print(f"  ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹: {valid_emails}ä»¶")
            
            return output_file
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def interactive_data_addition(self):
        """å¯¾è©±å¼ãƒ‡ãƒ¼ã‚¿è¿½åŠ """
        print("=" * 80)
        print("ğŸ“Š HUGAN JOB ãƒ‡ãƒ¼ã‚¿è¿½åŠ ã‚·ã‚¹ãƒ†ãƒ ")
        print("=" * 80)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
        print("\nğŸ“ è¿½åŠ ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")
        file_path = input("ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: ").strip().strip('"')
        
        if not file_path:
            print("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
        analysis = self.analyze_input_file(file_path)
        if not analysis:
            return False
        
        # å¿…é ˆã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯
        if analysis['missing_required']:
            print(f"\nâš ï¸ å¿…é ˆã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {', '.join(analysis['missing_required'])}")
            print("ã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è¡Œã„ã¾ã™ã‹ï¼Ÿ (y/n): ", end="")
            if input().lower() != 'y':
                return False
        
        # ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
        standardized_df = self.standardize_data(analysis)
        if standardized_df is None:
            return False
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨ã®ãƒãƒ¼ã‚¸
        print(f"\nğŸ”— æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒ¼ã‚¸ã—ã¾ã™ã‹ï¼Ÿ (y/n): ", end="")
        if input().lower() == 'y':
            merged_df = self.merge_with_existing_data(standardized_df)
        else:
            merged_df = standardized_df
        
        # ä¿å­˜
        print(f"\nğŸ’¾ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŒ‡å®šã—ã¦ãã ã•ã„ (ç©ºç™½ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ): ", end="")
        output_file = input().strip()
        if not output_file:
            output_file = 'test_input.csv'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å
        
        saved_file = self.save_data(merged_df, output_file)
        
        if saved_file:
            print(f"\nğŸ‰ ãƒ‡ãƒ¼ã‚¿è¿½åŠ ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            print(f"ğŸ“ ä¿å­˜å…ˆ: {saved_file}")
            
            # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ææ¡ˆ
            print(f"\nğŸ“‹ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
            print(f"1. ãƒ¡ãƒ¼ãƒ«æŠ½å‡º: python core_scripts/derivative_email_extractor.py")
            print(f"2. ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æ: python core_scripts/derivative_website_analyzer.py")
            print(f"3. ãƒ¡ãƒ¼ãƒ«é€ä¿¡: python core_scripts/derivative_ad_email_sender.py")
            
            return True
        
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    manager = DataInputManager()
    
    try:
        success = manager.interactive_data_addition()
        if success:
            print(f"\nâœ… å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        else:
            print(f"\nâŒ å‡¦ç†ãŒå¤±æ•—ã—ã¾ã—ãŸ")
    except KeyboardInterrupt:
        print(f"\n\nâŒ å‡¦ç†ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
