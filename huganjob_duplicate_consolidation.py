#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HUGANJOBé‡è¤‡ä¼æ¥­ãƒ‡ãƒ¼ã‚¿çµ±åˆå‡¦ç†
ä¼æ¥­ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸URLãŒåŒä¸€ã®é‡è¤‡ä¼æ¥­ã‚’1ã¤ã®IDã«çµ±åˆã™ã‚‹
"""

import pandas as pd
import datetime
import os

def analyze_duplicates():
    """é‡è¤‡ä¼æ¥­ã®è©³ç´°åˆ†æ"""
    
    try:
        df = pd.read_csv('data/new_input_test.csv')
        print('=== HUGANJOBé‡è¤‡ä¼æ¥­ãƒ‡ãƒ¼ã‚¿åˆ†æ ===')
        print(f'ç·ä¼æ¥­æ•°: {len(df)}ç¤¾')
        print()
        
        # ä¼æ¥­ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸URLã«ã‚ˆã‚‹é‡è¤‡ãƒã‚§ãƒƒã‚¯
        valid_homepage_mask = df['ä¼æ¥­ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸'].notna() & (df['ä¼æ¥­ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸'] != 'â€')
        homepage_counts = df[valid_homepage_mask]['ä¼æ¥­ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸'].value_counts()
        duplicates = homepage_counts[homepage_counts > 1]
        
        print(f'é‡è¤‡ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸URLæ•°: {len(duplicates)}ä»¶')
        print(f'é‡è¤‡ä¼æ¥­ç·æ•°: {duplicates.sum()}ç¤¾')
        print()
        
        duplicate_groups = []
        for url, count in duplicates.items():
            duplicate_companies = df[df['ä¼æ¥­ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸'] == url].copy()
            duplicate_groups.append({
                'url': url,
                'count': count,
                'companies': duplicate_companies
            })
        
        return duplicate_groups
        
    except Exception as e:
        print(f'âŒ ã‚¨ãƒ©ãƒ¼: é‡è¤‡åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ: {e}')
        return []

def consolidate_duplicate_group(group):
    """é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ã‚’çµ±åˆã™ã‚‹"""
    
    companies = group['companies']
    url = group['url']
    
    # æœ€å°IDã‚’åŸºæº–ä¼æ¥­ã¨ã—ã¦é¸æŠ
    base_company = companies.loc[companies['ID'].idxmin()]
    other_companies = companies[companies['ID'] != base_company['ID']]
    
    print(f'ğŸ“Œ çµ±åˆã‚°ãƒ«ãƒ¼ãƒ—: {url}')
    print(f'  åŸºæº–ä¼æ¥­: ID {base_company["ID"]} - {base_company["ä¼æ¥­å"]}')
    print(f'  çµ±åˆå¯¾è±¡: {len(other_companies)}ç¤¾')
    
    # å‹Ÿé›†è·ç¨®ã‚’çµ±åˆ
    all_positions = [base_company['å‹Ÿé›†è·ç¨®']]
    for _, company in other_companies.iterrows():
        if pd.notna(company['å‹Ÿé›†è·ç¨®']) and company['å‹Ÿé›†è·ç¨®'] not in all_positions:
            all_positions.append(company['å‹Ÿé›†è·ç¨®'])
    
    consolidated_position = 'ãƒ»'.join(all_positions)
    
    # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®çµ±åˆï¼ˆæœ‰åŠ¹ãªã‚‚ã®ã‚’å„ªå…ˆï¼‰
    consolidated_email = base_company['æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹']
    if pd.isna(consolidated_email) or consolidated_email == 'â€':
        for _, company in other_companies.iterrows():
            if pd.notna(company['æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹']) and company['æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] != 'â€':
                consolidated_email = company['æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹']
                break
    
    print(f'  çµ±åˆå¾Œå‹Ÿé›†è·ç¨®: {consolidated_position}')
    print(f'  çµ±åˆå¾Œãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹: {consolidated_email}')
    print()
    
    return {
        'base_id': base_company['ID'],
        'remove_ids': other_companies['ID'].tolist(),
        'consolidated_position': consolidated_position,
        'consolidated_email': consolidated_email
    }

def execute_consolidation():
    """é‡è¤‡ä¼æ¥­ã®çµ±åˆå‡¦ç†ã‚’å®Ÿè¡Œ"""
    
    try:
        # å…ƒãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        df = pd.read_csv('data/new_input_test.csv')
        original_count = len(df)
        
        print('=== HUGANJOBé‡è¤‡ä¼æ¥­çµ±åˆå‡¦ç†é–‹å§‹ ===')
        print(f'çµ±åˆå‰ä¼æ¥­æ•°: {original_count}ç¤¾')
        print()
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_filename = f'data/new_input_test_backup_consolidation_{timestamp}.csv'
        df.to_csv(backup_filename, index=False, encoding='utf-8-sig')
        print(f'ğŸ“ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {backup_filename}')
        print()
        
        # é‡è¤‡ä¼æ¥­ã‚’åˆ†æ
        duplicate_groups = analyze_duplicates()
        
        if not duplicate_groups:
            print('âœ… çµ±åˆå¯¾è±¡ã®é‡è¤‡ä¼æ¥­ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ')
            return df
        
        # çµ±åˆå‡¦ç†ã‚’å®Ÿè¡Œ
        df_updated = df.copy()
        total_removed = 0
        
        for group in duplicate_groups:
            consolidation_result = consolidate_duplicate_group(group)
            
            # åŸºæº–ä¼æ¥­ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
            base_index = df_updated[df_updated['ID'] == consolidation_result['base_id']].index[0]
            df_updated.loc[base_index, 'å‹Ÿé›†è·ç¨®'] = consolidation_result['consolidated_position']
            
            if (pd.notna(consolidation_result['consolidated_email']) and 
                consolidation_result['consolidated_email'] != 'â€'):
                df_updated.loc[base_index, 'æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] = consolidation_result['consolidated_email']
            
            # é‡è¤‡ä¼æ¥­ã‚’å‰Šé™¤
            df_updated = df_updated[~df_updated['ID'].isin(consolidation_result['remove_ids'])]
            total_removed += len(consolidation_result['remove_ids'])
        
        final_count = len(df_updated)
        
        print('=== çµ±åˆå‡¦ç†å®Œäº† ===')
        print(f'çµ±åˆå‰ä¼æ¥­æ•°: {original_count}ç¤¾')
        print(f'çµ±åˆå¾Œä¼æ¥­æ•°: {final_count}ç¤¾')
        print(f'å‰Šé™¤ä¼æ¥­æ•°: {total_removed}ç¤¾')
        print(f'çµ±åˆã‚°ãƒ«ãƒ¼ãƒ—æ•°: {len(duplicate_groups)}ä»¶')
        print()
        
        # çµ±åˆå¾Œãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        df_updated.to_csv('data/new_input_test.csv', index=False, encoding='utf-8-sig')
        print('ğŸ’¾ çµ±åˆå¾Œãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: data/new_input_test.csv')
        
        return df_updated
        
    except Exception as e:
        print(f'âŒ ã‚¨ãƒ©ãƒ¼: çµ±åˆå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}')
        return None

def verify_consolidation():
    """çµ±åˆçµæœã‚’æ¤œè¨¼"""
    
    try:
        df = pd.read_csv('data/new_input_test.csv')
        
        print('=== çµ±åˆçµæœæ¤œè¨¼ ===')
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        valid_homepage_mask = df['ä¼æ¥­ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸'].notna() & (df['ä¼æ¥­ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸'] != 'â€')
        homepage_counts = df[valid_homepage_mask]['ä¼æ¥­ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸'].value_counts()
        remaining_duplicates = homepage_counts[homepage_counts > 1]
        
        if len(remaining_duplicates) == 0:
            print('âœ… é‡è¤‡ä¼æ¥­ã®çµ±åˆãŒå®Œäº†ã—ã¾ã—ãŸ')
        else:
            print(f'âš ï¸ æ®‹å­˜é‡è¤‡: {len(remaining_duplicates)}ä»¶')
            for url, count in remaining_duplicates.items():
                print(f'  {url}: {count}ç¤¾')
        
        # çµ±åˆã•ã‚ŒãŸè·ç¨®ã®ç¢ºèª
        multi_position_companies = df[df['å‹Ÿé›†è·ç¨®'].str.contains('ãƒ»', na=False)]
        print(f'ğŸ“Š è¤‡æ•°è·ç¨®çµ±åˆä¼æ¥­: {len(multi_position_companies)}ç¤¾')
        
        if len(multi_position_companies) > 0:
            print('çµ±åˆã•ã‚ŒãŸè·ç¨®ã®ä¾‹:')
            for _, company in multi_position_companies.head(5).iterrows():
                print(f'  ID {company["ID"]}: {company["ä¼æ¥­å"]} - {company["å‹Ÿé›†è·ç¨®"]}')
        
        print()
        return True
        
    except Exception as e:
        print(f'âŒ ã‚¨ãƒ©ãƒ¼: æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}')
        return False

if __name__ == "__main__":
    # çµ±åˆå‡¦ç†ã‚’å®Ÿè¡Œ
    df_updated = execute_consolidation()
    
    if df_updated is not None:
        # çµæœã‚’æ¤œè¨¼
        if verify_consolidation():
            print('ğŸ¯ é‡è¤‡ä¼æ¥­çµ±åˆå‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ')
            print('ğŸ“ˆ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„: http://127.0.0.1:5002/companies')
        else:
            print('âŒ çµ±åˆçµæœã®æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ')
    else:
        print('âŒ çµ±åˆå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ')
