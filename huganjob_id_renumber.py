#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HUGANJOBä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã®IDé€£ç•ªä¿®æ­£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡çµ±åˆå‡¦ç†ã§ç”Ÿã˜ãŸIDæ¬ ç•ªã‚’ä¿®æ­£ã—ã€1ã‹ã‚‰é€£ç•ªã«ãªã‚‹ã‚ˆã†ã«å†æ¡ç•ª

ä½œæˆæ—¥æ™‚: 2025å¹´06æœˆ24æ—¥
ç›®çš„: IDæ¬ ç•ªå•é¡Œã®è§£æ±ºã¨ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®ç¢ºä¿
"""

import pandas as pd
import csv
import os
import json
import glob
import shutil
from datetime import datetime

def backup_files():
    """é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_files = []
    
    files_to_backup = [
        'data/new_input_test.csv',
        'new_email_sending_results.csv',
        'huganjob_email_resolution_results.csv'
    ]
    
    for file_path in files_to_backup:
        if os.path.exists(file_path):
            backup_path = f"{file_path}_backup_{timestamp}"
            shutil.copy2(file_path, backup_path)
            backup_files.append(backup_path)
            print(f"ğŸ“¦ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
    
    return backup_files

def analyze_current_ids():
    """ç¾åœ¨ã®IDçŠ¶æ³ã‚’åˆ†æ"""
    print("=== ç¾åœ¨ã®IDçŠ¶æ³åˆ†æ ===")
    
    # ä¼æ¥­ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = pd.read_csv('data/new_input_test.csv', encoding='utf-8-sig')
    actual_ids = sorted(df['ID'].tolist())
    
    print(f"ç·ä¼æ¥­æ•°: {len(actual_ids)}")
    print(f"IDç¯„å›²: {min(actual_ids)} - {max(actual_ids)}")
    
    # 1-1000ç¯„å›²ã®æ¬ ç•ªç¢ºèª
    expected_ids_1000 = set(range(1, 1001))
    actual_ids_1000 = set([id for id in actual_ids if id <= 1000])
    missing_ids = sorted(list(expected_ids_1000 - actual_ids_1000))
    
    print(f"1-1000ç¯„å›²ã®ä¼æ¥­æ•°: {len(actual_ids_1000)}")
    print(f"æ¬ ç•ªæ•°: {len(missing_ids)}")
    print(f"æ¬ ç•ªID: {missing_ids}")
    
    # 1000è¶…éIDç¢ºèª
    over_1000_ids = [id for id in actual_ids if id > 1000]
    print(f"1000è¶…éä¼æ¥­æ•°: {len(over_1000_ids)}")
    
    return df, missing_ids, over_1000_ids

def create_id_mapping(df):
    """æ–°ã—ã„IDé€£ç•ªãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ"""
    print("\n=== IDé€£ç•ªãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆ ===")
    
    # ç¾åœ¨ã®IDã‚’å–å¾—ã—ã¦ã‚½ãƒ¼ãƒˆ
    current_ids = sorted(df['ID'].tolist())
    
    # æ–°ã—ã„IDãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆ1ã‹ã‚‰é€£ç•ªï¼‰
    id_mapping = {}
    for new_id, old_id in enumerate(current_ids, 1):
        id_mapping[old_id] = new_id
    
    print(f"ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆå®Œäº†: {len(id_mapping)}ä»¶")
    print(f"æ–°IDç¯„å›²: 1 - {len(current_ids)}")
    
    # ãƒãƒƒãƒ”ãƒ³ã‚°ä¾‹ã‚’è¡¨ç¤º
    print("\nãƒãƒƒãƒ”ãƒ³ã‚°ä¾‹ï¼ˆæœ€åˆã®10ä»¶ï¼‰:")
    for i, (old_id, new_id) in enumerate(list(id_mapping.items())[:10]):
        print(f"  {old_id} â†’ {new_id}")
    
    return id_mapping

def update_company_data(df, id_mapping):
    """ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã®IDã‚’æ›´æ–°"""
    print("\n=== ä¼æ¥­ãƒ‡ãƒ¼ã‚¿IDæ›´æ–° ===")
    
    # IDã‚’æ–°ã—ã„é€£ç•ªã«æ›´æ–°
    df['ID'] = df['ID'].map(id_mapping)
    
    # IDã§ã‚½ãƒ¼ãƒˆ
    df = df.sort_values('ID').reset_index(drop=True)
    
    # æ›´æ–°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    df.to_csv('data/new_input_test.csv', index=False, encoding='utf-8-sig')
    print(f"âœ… ä¼æ¥­ãƒ‡ãƒ¼ã‚¿æ›´æ–°å®Œäº†: {len(df)}ç¤¾")
    print(f"æ–°IDç¯„å›²: {df['ID'].min()} - {df['ID'].max()}")
    
    return df

def update_sending_results(id_mapping):
    """é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®IDã‚’æ›´æ–°"""
    print("\n=== é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«IDæ›´æ–° ===")
    
    # ãƒ¡ã‚¤ãƒ³é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«
    if os.path.exists('new_email_sending_results.csv'):
        try:
            df_results = pd.read_csv('new_email_sending_results.csv', encoding='utf-8-sig')
            
            # ä¼æ¥­IDã‚’æ›´æ–°
            df_results['ä¼æ¥­ID'] = df_results['ä¼æ¥­ID'].astype(int).map(id_mapping)
            
            # NaNã‚’é™¤å»ï¼ˆãƒãƒƒãƒ”ãƒ³ã‚°ã«ãªã„IDã¯å‰Šé™¤ï¼‰
            df_results = df_results.dropna(subset=['ä¼æ¥­ID'])
            df_results['ä¼æ¥­ID'] = df_results['ä¼æ¥­ID'].astype(int)
            
            # IDã§ã‚½ãƒ¼ãƒˆ
            df_results = df_results.sort_values('ä¼æ¥­ID').reset_index(drop=True)
            
            # ä¿å­˜
            df_results.to_csv('new_email_sending_results.csv', index=False, encoding='utf-8-sig')
            print(f"âœ… ãƒ¡ã‚¤ãƒ³é€ä¿¡çµæœæ›´æ–°å®Œäº†: {len(df_results)}ä»¶")
            
        except Exception as e:
            print(f"âš ï¸ ãƒ¡ã‚¤ãƒ³é€ä¿¡çµæœæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
    
    # HUGANJOBé€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«
    huganjob_files = [f for f in os.listdir('.') if f.startswith('huganjob_sending_results_') and f.endswith('.csv')]
    for file_name in huganjob_files:
        try:
            df_huganjob = pd.read_csv(file_name, encoding='utf-8-sig')
            
            if 'ä¼æ¥­ID' in df_huganjob.columns:
                df_huganjob['ä¼æ¥­ID'] = df_huganjob['ä¼æ¥­ID'].astype(int).map(id_mapping)
                df_huganjob = df_huganjob.dropna(subset=['ä¼æ¥­ID'])
                df_huganjob['ä¼æ¥­ID'] = df_huganjob['ä¼æ¥­ID'].astype(int)
                df_huganjob = df_huganjob.sort_values('ä¼æ¥­ID').reset_index(drop=True)
                df_huganjob.to_csv(file_name, index=False, encoding='utf-8-sig')
                print(f"âœ… {file_name} æ›´æ–°å®Œäº†: {len(df_huganjob)}ä»¶")
                
        except Exception as e:
            print(f"âš ï¸ {file_name} æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

def update_email_resolution_results(id_mapping):
    """ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºçµæœã®IDã‚’æ›´æ–°"""
    print("\n=== ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºçµæœIDæ›´æ–° ===")
    
    if os.path.exists('huganjob_email_resolution_results.csv'):
        try:
            df_email = pd.read_csv('huganjob_email_resolution_results.csv', encoding='utf-8')
            
            # company_idã‚’æ›´æ–°
            df_email['company_id'] = df_email['company_id'].astype(int).map(id_mapping)
            
            # NaNã‚’é™¤å»
            df_email = df_email.dropna(subset=['company_id'])
            df_email['company_id'] = df_email['company_id'].astype(int)
            
            # IDã§ã‚½ãƒ¼ãƒˆ
            df_email = df_email.sort_values('company_id').reset_index(drop=True)
            
            # ä¿å­˜
            df_email.to_csv('huganjob_email_resolution_results.csv', index=False, encoding='utf-8')
            print(f"âœ… ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºçµæœæ›´æ–°å®Œäº†: {len(df_email)}ä»¶")
            
        except Exception as e:
            print(f"âš ï¸ ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºçµæœæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

def verify_renumbering():
    """IDé€£ç•ªä¿®æ­£ã®æ¤œè¨¼"""
    print("\n=== IDé€£ç•ªä¿®æ­£æ¤œè¨¼ ===")
    
    # ä¼æ¥­ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
    df = pd.read_csv('data/new_input_test.csv', encoding='utf-8-sig')
    ids = sorted(df['ID'].tolist())
    
    print(f"ç·ä¼æ¥­æ•°: {len(ids)}")
    print(f"IDç¯„å›²: {min(ids)} - {max(ids)}")
    
    # é€£ç•ªæ€§ç¢ºèª
    expected_ids = list(range(1, len(ids) + 1))
    is_sequential = ids == expected_ids
    
    if is_sequential:
        print("âœ… IDé€£ç•ªä¿®æ­£æˆåŠŸ: å®Œå…¨ãªé€£ç•ªã«ãªã‚Šã¾ã—ãŸ")
    else:
        missing = set(expected_ids) - set(ids)
        duplicates = len(ids) - len(set(ids))
        print(f"âŒ IDé€£ç•ªä¿®æ­£ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
        print(f"   æ¬ ç•ª: {sorted(list(missing))}")
        print(f"   é‡è¤‡: {duplicates}ä»¶")
    
    return is_sequential

def save_mapping_log(id_mapping, backup_files):
    """IDãƒãƒƒãƒ”ãƒ³ã‚°ãƒ­ã‚°ã‚’ä¿å­˜"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_data = {
        'timestamp': timestamp,
        'operation': 'ID_RENUMBERING',
        'total_companies': len(id_mapping),
        'backup_files': backup_files,
        'id_mapping': id_mapping,
        'summary': {
            'old_id_range': f"{min(id_mapping.keys())} - {max(id_mapping.keys())}",
            'new_id_range': f"{min(id_mapping.values())} - {max(id_mapping.values())}",
            'mapping_count': len(id_mapping)
        }
    }
    
    log_file = f"huganjob_id_renumbering_log_{timestamp}.json"
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(log_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ IDãƒãƒƒãƒ”ãƒ³ã‚°ãƒ­ã‚°ä¿å­˜: {log_file}")
    return log_file

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("ğŸ”§ HUGANJOBä¼æ¥­ãƒ‡ãƒ¼ã‚¿IDé€£ç•ªä¿®æ­£ãƒ„ãƒ¼ãƒ«")
    print("=" * 60)
    
    try:
        # 1. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
        backup_files_list = backup_files()
        
        # 2. ç¾åœ¨ã®IDçŠ¶æ³åˆ†æ
        df, missing_ids, over_1000_ids = analyze_current_ids()
        
        # 3. IDãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆ
        id_mapping = create_id_mapping(df)
        
        # 4. ä¼æ¥­ãƒ‡ãƒ¼ã‚¿æ›´æ–°
        df_updated = update_company_data(df, id_mapping)
        
        # 5. é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°
        update_sending_results(id_mapping)
        
        # 6. ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºçµæœæ›´æ–°
        update_email_resolution_results(id_mapping)
        
        # 7. æ¤œè¨¼
        is_success = verify_renumbering()
        
        # 8. ãƒ­ã‚°ä¿å­˜
        log_file = save_mapping_log(id_mapping, backup_files_list)

        if is_success:
            print(f"\nğŸ‰ IDé€£ç•ªä¿®æ­£ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            print(f"ğŸ“Š ç·ä¼æ¥­æ•°: {len(df_updated)}")
            print(f"ğŸ”¢ æ–°IDç¯„å›²: 1 - {len(df_updated)}")
            print(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
            print(f"ğŸ“¦ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {len(backup_files_list)}ãƒ•ã‚¡ã‚¤ãƒ«")
        else:
            print(f"\nâŒ IDé€£ç•ªä¿®æ­£ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            print(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å¾©å…ƒã—ã¦ãã ã•ã„")
        
        return is_success
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å¾©å…ƒã—ã¦ãã ã•ã„")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
