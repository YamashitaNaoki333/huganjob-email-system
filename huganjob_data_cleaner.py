#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HUGANJOBä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒŠãƒ¼
ãƒ†ã‚¹ãƒˆé–¢é€£ä¼æ¥­ã‚„å­˜åœ¨ã—ãªã„ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºãƒ»å‰Šé™¤ã™ã‚‹ãƒ„ãƒ¼ãƒ«
"""

import pandas as pd
import re
import os
import json
import csv
from datetime import datetime

class HuganjobDataCleaner:
    def __init__(self, csv_file='data/new_input_test.csv'):
        self.csv_file = csv_file
        self.backup_file = f"{csv_file}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.email_results_file = 'huganjob_email_resolution_results.csv'
        self.sending_results_file = 'new_email_sending_results.csv'
        self.sending_history_file = 'huganjob_sending_history.json'
        
    def create_backup(self):
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        try:
            import shutil
            shutil.copy2(self.csv_file, self.backup_file)
            print(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {self.backup_file}")
            return True
        except Exception as e:
            print(f"âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def load_data(self):
        """CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            df = pd.read_csv(self.csv_file, encoding='utf-8-sig')
            print(f"ðŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ç¤¾")
            return df
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def detect_test_companies(self, df):
        """ãƒ†ã‚¹ãƒˆé–¢é€£ä¼æ¥­ã‚’æ¤œå‡º"""
        test_patterns = [
            # ä¼æ¥­åãƒ‘ã‚¿ãƒ¼ãƒ³
            r'ãƒ†ã‚¹ãƒˆ',
            r'test',
            r'TEST',
            r'ã‚µãƒ³ãƒ—ãƒ«',
            r'sample',
            r'SAMPLE',
            r'ãƒ€ãƒŸãƒ¼',
            r'dummy',
            r'DUMMY',
            r'æ­£å¸¸ä¼æ¥­',
            r'ãƒ†ã‚¹ãƒˆä¼æ¥­',
            r'ã‚µãƒ³ãƒ—ãƒ«ä¼æ¥­',
            r'ãƒ€ãƒŸãƒ¼ä¼æ¥­',
            r'æ ªå¼ä¼šç¤¾ãƒ†ã‚¹ãƒˆ',
            r'æ ªå¼ä¼šç¤¾ã‚µãƒ³ãƒ—ãƒ«',
            r'æ ªå¼ä¼šç¤¾ãƒ€ãƒŸãƒ¼',
            r'Example',
            r'EXAMPLE',
            r'Demo',
            r'DEMO',
            r'ãƒ‡ãƒ¢',
        ]
        
        test_companies = []
        
        for index, row in df.iterrows():
            company_name = str(row.get('ä¼æ¥­å', '')).strip()
            website = str(row.get('ä¼æ¥­ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸', '')).strip()
            email = str(row.get('æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', '')).strip()
            
            # ä¼æ¥­åãƒã‚§ãƒƒã‚¯
            for pattern in test_patterns:
                if re.search(pattern, company_name, re.IGNORECASE):
                    test_companies.append({
                        'ID': row['ID'],
                        'ä¼æ¥­å': company_name,
                        'ç†ç”±': f'ä¼æ¥­åã«ãƒ†ã‚¹ãƒˆé–¢é€£æ–‡å­—åˆ—: {pattern}',
                        'ãƒ‡ãƒ¼ã‚¿': f"ä¼æ¥­å: {company_name}"
                    })
                    break
            
            # ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆãƒã‚§ãƒƒã‚¯
            test_domains = ['test.com', 'example.com', 'dummy.com', 'sample.com', 'demo.com']
            for domain in test_domains:
                if domain in website.lower():
                    test_companies.append({
                        'ID': row['ID'],
                        'ä¼æ¥­å': company_name,
                        'ç†ç”±': f'ãƒ†ã‚¹ãƒˆç”¨ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain}',
                        'ãƒ‡ãƒ¼ã‚¿': f"ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ: {website}"
                    })
                    break
            
            # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒã‚§ãƒƒã‚¯
            test_email_patterns = ['test@', 'sample@', 'dummy@', 'demo@', 'example@']
            for pattern in test_email_patterns:
                if email.lower().startswith(pattern):
                    test_companies.append({
                        'ID': row['ID'],
                        'ä¼æ¥­å': company_name,
                        'ç†ç”±': f'ãƒ†ã‚¹ãƒˆç”¨ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹: {pattern}',
                        'ãƒ‡ãƒ¼ã‚¿': f"ãƒ¡ãƒ¼ãƒ«: {email}"
                    })
                    break
        
        return test_companies
    
    def detect_invalid_companies(self, df):
        """ä¸æ­£ãªä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡º"""
        invalid_companies = []
        
        for index, row in df.iterrows():
            company_name = str(row.get('ä¼æ¥­å', '')).strip()
            website = str(row.get('ä¼æ¥­ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸', '')).strip()
            email = str(row.get('æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', '')).strip()
            
            # ä¼æ¥­åãŒç©ºç™½ã¾ãŸã¯è¨˜å·ã®ã¿
            if not company_name or company_name in ['', 'nan', 'NaN', 'null', 'NULL']:
                invalid_companies.append({
                    'ID': row['ID'],
                    'ä¼æ¥­å': company_name,
                    'ç†ç”±': 'ä¼æ¥­åãŒç©ºç™½',
                    'ãƒ‡ãƒ¼ã‚¿': f"ä¼æ¥­å: '{company_name}'"
                })
                continue
            
            # ä¼æ¥­åãŒè¨˜å·ã®ã¿
            if re.match(r'^[^\w\s]+$', company_name):
                invalid_companies.append({
                    'ID': row['ID'],
                    'ä¼æ¥­å': company_name,
                    'ç†ç”±': 'ä¼æ¥­åãŒè¨˜å·ã®ã¿',
                    'ãƒ‡ãƒ¼ã‚¿': f"ä¼æ¥­å: {company_name}"
                })
                continue
            
            # ä¼æ¥­åãŒçŸ­ã™ãŽã‚‹ï¼ˆ1æ–‡å­—ï¼‰
            if len(company_name) == 1:
                invalid_companies.append({
                    'ID': row['ID'],
                    'ä¼æ¥­å': company_name,
                    'ç†ç”±': 'ä¼æ¥­åãŒ1æ–‡å­—ã®ã¿',
                    'ãƒ‡ãƒ¼ã‚¿': f"ä¼æ¥­å: {company_name}"
                })
                continue
            
            # ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã¨ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®ä¸¡æ–¹ãŒç©ºç™½
            website_empty = website in ['', 'â€', '-', 'nan', 'NaN', 'null', 'NULL']
            email_empty = email in ['', 'â€', '-', 'nan', 'NaN', 'null', 'NULL']
            
            if website_empty and email_empty:
                invalid_companies.append({
                    'ID': row['ID'],
                    'ä¼æ¥­å': company_name,
                    'ç†ç”±': 'ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã¨ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®ä¸¡æ–¹ãŒç©ºç™½',
                    'ãƒ‡ãƒ¼ã‚¿': f"ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ: {website}, ãƒ¡ãƒ¼ãƒ«: {email}"
                })
        
        return invalid_companies
    
    def detect_duplicate_companies(self, df):
        """é‡è¤‡ä¼æ¥­ã‚’æ¤œå‡º"""
        duplicates = []
        
        # ä¼æ¥­åã§ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
        name_duplicates = df[df.duplicated(subset=['ä¼æ¥­å'], keep=False)]
        
        for index, row in name_duplicates.iterrows():
            duplicates.append({
                'ID': row['ID'],
                'ä¼æ¥­å': row['ä¼æ¥­å'],
                'ç†ç”±': 'ä¼æ¥­åé‡è¤‡',
                'ãƒ‡ãƒ¼ã‚¿': f"ä¼æ¥­å: {row['ä¼æ¥­å']}"
            })
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
        df_with_domain = df.copy()
        df_with_domain['ãƒ‰ãƒ¡ã‚¤ãƒ³'] = df_with_domain['ä¼æ¥­ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸'].apply(self.extract_domain)
        
        domain_duplicates = df_with_domain[
            (df_with_domain['ãƒ‰ãƒ¡ã‚¤ãƒ³'] != '') & 
            (df_with_domain.duplicated(subset=['ãƒ‰ãƒ¡ã‚¤ãƒ³'], keep=False))
        ]
        
        for index, row in domain_duplicates.iterrows():
            duplicates.append({
                'ID': row['ID'],
                'ä¼æ¥­å': row['ä¼æ¥­å'],
                'ç†ç”±': 'ãƒ‰ãƒ¡ã‚¤ãƒ³é‡è¤‡',
                'ãƒ‡ãƒ¼ã‚¿': f"ãƒ‰ãƒ¡ã‚¤ãƒ³: {row['ãƒ‰ãƒ¡ã‚¤ãƒ³']}"
            })
        
        return duplicates
    
    def extract_domain(self, url):
        """URLã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æŠ½å‡º"""
        if not url or url in ['', 'â€', '-']:
            return ''
        
        try:
            # http://ã‚„https://ã‚’é™¤åŽ»
            domain = url.replace('https://', '').replace('http://', '')
            # www.ã‚’é™¤åŽ»
            domain = domain.replace('www.', '')
            # ãƒ‘ã‚¹ã‚’é™¤åŽ»
            domain = domain.split('/')[0]
            # ãƒãƒ¼ãƒˆç•ªå·ã‚’é™¤åŽ»
            domain = domain.split(':')[0]
            return domain.lower()
        except:
            return ''
    
    def generate_report(self, test_companies, invalid_companies, duplicate_companies):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("\n" + "="*60)
        print("ðŸ“‹ HUGANJOBä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*60)
        
        total_issues = len(test_companies) + len(invalid_companies) + len(duplicate_companies)
        
        print(f"ðŸ” æ¤œå‡ºã•ã‚ŒãŸå•é¡Œä¼æ¥­: {total_issues}ç¤¾")
        print()
        
        # ãƒ†ã‚¹ãƒˆé–¢é€£ä¼æ¥­
        if test_companies:
            print(f"ðŸ§ª ãƒ†ã‚¹ãƒˆé–¢é€£ä¼æ¥­: {len(test_companies)}ç¤¾")
            for company in test_companies:
                print(f"  ID {company['ID']}: {company['ä¼æ¥­å']}")
                print(f"    ç†ç”±: {company['ç†ç”±']}")
                print(f"    ãƒ‡ãƒ¼ã‚¿: {company['ãƒ‡ãƒ¼ã‚¿']}")
                print()
        
        # ä¸æ­£ãƒ‡ãƒ¼ã‚¿ä¼æ¥­
        if invalid_companies:
            print(f"âŒ ä¸æ­£ãƒ‡ãƒ¼ã‚¿ä¼æ¥­: {len(invalid_companies)}ç¤¾")
            for company in invalid_companies:
                print(f"  ID {company['ID']}: {company['ä¼æ¥­å']}")
                print(f"    ç†ç”±: {company['ç†ç”±']}")
                print(f"    ãƒ‡ãƒ¼ã‚¿: {company['ãƒ‡ãƒ¼ã‚¿']}")
                print()
        
        # é‡è¤‡ä¼æ¥­
        if duplicate_companies:
            print(f"ðŸ”„ é‡è¤‡ä¼æ¥­: {len(duplicate_companies)}ç¤¾")
            for company in duplicate_companies:
                print(f"  ID {company['ID']}: {company['ä¼æ¥­å']}")
                print(f"    ç†ç”±: {company['ç†ç”±']}")
                print(f"    ãƒ‡ãƒ¼ã‚¿: {company['ãƒ‡ãƒ¼ã‚¿']}")
                print()
        
        return total_issues
    
    def remove_companies(self, df, companies_to_remove):
        """æŒ‡å®šã•ã‚ŒãŸä¼æ¥­ã‚’å‰Šé™¤"""
        if not companies_to_remove:
            print("âœ… å‰Šé™¤å¯¾è±¡ã®ä¼æ¥­ã¯ã‚ã‚Šã¾ã›ã‚“")
            return df
        
        ids_to_remove = [company['ID'] for company in companies_to_remove]
        
        print(f"ðŸ—‘ï¸ {len(ids_to_remove)}ç¤¾ã‚’å‰Šé™¤ã—ã¾ã™...")
        
        # å‰Šé™¤å‰ã®ãƒ‡ãƒ¼ã‚¿æ•°
        before_count = len(df)
        
        # ä¼æ¥­ã‚’å‰Šé™¤
        df_cleaned = df[~df['ID'].isin(ids_to_remove)].copy()
        
        # å‰Šé™¤å¾Œã®ãƒ‡ãƒ¼ã‚¿æ•°
        after_count = len(df_cleaned)
        
        print(f"âœ… å‰Šé™¤å®Œäº†: {before_count}ç¤¾ â†’ {after_count}ç¤¾ ({before_count - after_count}ç¤¾å‰Šé™¤)")
        
        return df_cleaned
    
    def resequence_ids(self, df):
        """IDã‚’é€£ç•ªã«æŒ¯ã‚Šç›´ã—"""
        print("ðŸ”¢ IDã‚’é€£ç•ªã«æŒ¯ã‚Šç›´ã—ã¦ã„ã¾ã™...")
        
        df_resequenced = df.copy()
        df_resequenced['ID'] = range(1, len(df) + 1)
        
        print(f"âœ… IDæŒ¯ã‚Šç›´ã—å®Œäº†: 1 ã€œ {len(df)}")
        
        return df_resequenced
    
    def save_cleaned_data(self, df):
        """ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        try:
            df.to_csv(self.csv_file, index=False, encoding='utf-8-sig')
            print(f"âœ… ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {self.csv_file}")
            return True
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def remove_specific_companies(self, df, company_ids):
        """æŒ‡å®šã•ã‚ŒãŸIDã®ä¼æ¥­ã‚’å‰Šé™¤"""
        print(f"ðŸ—‘ï¸ æŒ‡å®šã•ã‚ŒãŸ{len(company_ids)}ç¤¾ã‚’å‰Šé™¤ã—ã¾ã™...")
        print(f"å‰Šé™¤å¯¾è±¡ID: {company_ids}")

        # å‰Šé™¤å‰ã®ãƒ‡ãƒ¼ã‚¿æ•°
        before_count = len(df)

        # å‰Šé™¤å¯¾è±¡ä¼æ¥­ã®è©³ç´°ã‚’è¡¨ç¤º
        for company_id in company_ids:
            company_row = df[df['ID'] == company_id]
            if not company_row.empty:
                company_name = company_row.iloc[0]['ä¼æ¥­å']
                email = company_row.iloc[0].get('æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'æœªç™»éŒ²')
                print(f"  ID {company_id}: {company_name} ({email})")

        # ä¼æ¥­ã‚’å‰Šé™¤
        df_cleaned = df[~df['ID'].isin(company_ids)].copy()

        # å‰Šé™¤å¾Œã®ãƒ‡ãƒ¼ã‚¿æ•°
        after_count = len(df_cleaned)

        print(f"âœ… å‰Šé™¤å®Œäº†: {before_count}ç¤¾ â†’ {after_count}ç¤¾ ({before_count - after_count}ç¤¾å‰Šé™¤)")

        return df_cleaned

    def update_related_files(self, old_to_new_id_mapping):
        """é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®IDã‚’æ›´æ–°"""
        print("\nðŸ”„ é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ä¸­...")

        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºçµæžœãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°
        self.update_email_results_file(old_to_new_id_mapping)

        # é€ä¿¡çµæžœãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°
        self.update_sending_results_file(old_to_new_id_mapping)

        # é€ä¿¡å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°
        self.update_sending_history_file(old_to_new_id_mapping)

    def update_email_results_file(self, old_to_new_id_mapping):
        """ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºçµæžœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°"""
        if not os.path.exists(self.email_results_file):
            print(f"âš ï¸ {self.email_results_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return

        try:
            df = pd.read_csv(self.email_results_file, encoding='utf-8-sig')
            original_count = len(df)

            # å‰Šé™¤ã•ã‚ŒãŸIDã‚’é™¤å¤–
            df = df[df['ä¼æ¥­ID'].isin(old_to_new_id_mapping.keys())]

            # IDã‚’æ–°ã—ã„å€¤ã«æ›´æ–°
            df['ä¼æ¥­ID'] = df['ä¼æ¥­ID'].map(old_to_new_id_mapping)

            # ä¿å­˜
            df.to_csv(self.email_results_file, index=False, encoding='utf-8-sig')
            print(f"âœ… {self.email_results_file} æ›´æ–°å®Œäº†: {original_count} â†’ {len(df)}è¡Œ")

        except Exception as e:
            print(f"âŒ {self.email_results_file} æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    def update_sending_results_file(self, old_to_new_id_mapping):
        """é€ä¿¡çµæžœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°"""
        if not os.path.exists(self.sending_results_file):
            print(f"âš ï¸ {self.sending_results_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return

        try:
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ‰‹å‹•ã§èª­ã¿è¾¼ã¿ï¼ˆåˆ—æ•°ãŒä¸å®šã®ãŸã‚ï¼‰
            updated_rows = []
            with open(self.sending_results_file, 'r', encoding='utf-8-sig') as f:
                reader = csv.reader(f)
                header = next(reader, None)
                if header:
                    updated_rows.append(header)

                for row in reader:
                    if len(row) > 0:
                        try:
                            old_id = int(row[0])
                            if old_id in old_to_new_id_mapping:
                                row[0] = str(old_to_new_id_mapping[old_id])
                                updated_rows.append(row)
                        except (ValueError, IndexError):
                            continue

            # æ›´æ–°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            with open(self.sending_results_file, 'w', encoding='utf-8-sig', newline='') as f:
                writer = csv.writer(f)
                writer.writerows(updated_rows)

            print(f"âœ… {self.sending_results_file} æ›´æ–°å®Œäº†: {len(updated_rows)-1}è¡Œ")

        except Exception as e:
            print(f"âŒ {self.sending_results_file} æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    def update_sending_history_file(self, old_to_new_id_mapping):
        """é€ä¿¡å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°"""
        if not os.path.exists(self.sending_history_file):
            print(f"âš ï¸ {self.sending_history_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return

        try:
            with open(self.sending_history_file, 'r', encoding='utf-8') as f:
                history_data = json.load(f)

            updated_history = {}
            for timestamp, entries in history_data.items():
                updated_entries = []
                for entry in entries:
                    if 'company_id' in entry:
                        old_id = entry['company_id']
                        if old_id in old_to_new_id_mapping:
                            entry['company_id'] = old_to_new_id_mapping[old_id]
                            updated_entries.append(entry)

                if updated_entries:
                    updated_history[timestamp] = updated_entries

            # æ›´æ–°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            with open(self.sending_history_file, 'w', encoding='utf-8') as f:
                json.dump(updated_history, f, ensure_ascii=False, indent=2)

            print(f"âœ… {self.sending_history_file} æ›´æ–°å®Œäº†")

        except Exception as e:
            print(f"âŒ {self.sending_history_file} æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    def create_id_mapping(self, df_before, df_after):
        """å‰Šé™¤å‰å¾Œã®IDãƒžãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ"""
        mapping = {}
        for new_id, (_, row) in enumerate(df_after.iterrows(), 1):
            old_id = row['ID']
            mapping[old_id] = new_id
        return mapping

def main():
    print("ðŸ§¹ HUGANJOBä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒŠãƒ¼")
    print("="*50)

    cleaner = HuganjobDataCleaner()

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
    if not cleaner.create_backup():
        return

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = cleaner.load_data()
    if df is None:
        return

    print(f"ðŸ“Š èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ç¤¾")

    # å•é¡Œä¼æ¥­ã‚’æ¤œå‡º
    print("\nðŸ” å•é¡Œä¼æ¥­ã‚’æ¤œå‡ºä¸­...")
    test_companies = cleaner.detect_test_companies(df)
    invalid_companies = cleaner.detect_invalid_companies(df)
    duplicate_companies = cleaner.detect_duplicate_companies(df)

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    total_issues = cleaner.generate_report(test_companies, invalid_companies, duplicate_companies)

    if total_issues == 0:
        print("âœ… å•é¡Œä¼æ¥­ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return

    # å‰Šé™¤ç¢ºèª
    print(f"\nâ“ {total_issues}ç¤¾ã®å•é¡Œä¼æ¥­ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ")
    choice = input("å‰Šé™¤ã™ã‚‹å ´åˆã¯ 'yes' ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip().lower()

    if choice != 'yes':
        print("âŒ å‰Šé™¤ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
        return

    # å‰Šé™¤å¯¾è±¡ã‚’ã¾ã¨ã‚ã‚‹
    all_companies_to_remove = test_companies + invalid_companies + duplicate_companies

    # ä¼æ¥­å‰Šé™¤
    df_cleaned = cleaner.remove_companies(df, all_companies_to_remove)

    # IDæŒ¯ã‚Šç›´ã—ç¢ºèª
    print(f"\nâ“ IDã‚’é€£ç•ªã«æŒ¯ã‚Šç›´ã—ã¾ã™ã‹ï¼Ÿ")
    choice = input("æŒ¯ã‚Šç›´ã™å ´åˆã¯ 'yes' ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip().lower()

    if choice == 'yes':
        df_cleaned = cleaner.resequence_ids(df_cleaned)

    # ä¿å­˜
    if cleaner.save_cleaned_data(df_cleaned):
        print(f"\nðŸŽ‰ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼")
        print(f"ðŸ“ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {cleaner.backup_file}")
        print(f"ðŸ“ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿: {cleaner.csv_file}")
    else:
        print(f"\nâŒ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¤±æ•—")

def main_specific_cleanup():
    """æŒ‡å®šã•ã‚ŒãŸä¼æ¥­IDã‚’å‰Šé™¤ã™ã‚‹å°‚ç”¨é–¢æ•°"""
    print("ðŸ§¹ HUGANJOBæŒ‡å®šä¼æ¥­å‰Šé™¤ãƒ„ãƒ¼ãƒ«")
    print("="*50)

    # å‰Šé™¤å¯¾è±¡ä¼æ¥­ID
    target_ids = [2995, 2996, 2997, 4837, 4838, 4839, 4840, 4832, 4833, 4834]

    cleaner = HuganjobDataCleaner()

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
    print("ðŸ“ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆä¸­...")
    if not cleaner.create_backup():
        return

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = cleaner.load_data()
    if df is None:
        return

    print(f"ðŸ“Š èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ç¤¾")

    # å‰Šé™¤å¯¾è±¡ä¼æ¥­ã®ç¢ºèª
    print(f"\nðŸŽ¯ å‰Šé™¤å¯¾è±¡ä¼æ¥­: {len(target_ids)}ç¤¾")
    for target_id in target_ids:
        company_row = df[df['ID'] == target_id]
        if not company_row.empty:
            company_name = company_row.iloc[0]['ä¼æ¥­å']
            email = company_row.iloc[0].get('æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'æœªç™»éŒ²')
            print(f"  ID {target_id}: {company_name} ({email})")
        else:
            print(f"  ID {target_id}: ä¼æ¥­ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    # å‰Šé™¤ç¢ºèª
    print(f"\nâ“ ä¸Šè¨˜{len(target_ids)}ç¤¾ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ")
    choice = input("å‰Šé™¤ã™ã‚‹å ´åˆã¯ 'yes' ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip().lower()

    if choice != 'yes':
        print("âŒ å‰Šé™¤ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
        return

    # å‰Šé™¤å‰ã®IDãƒžãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆï¼ˆIDæŒ¯ã‚Šç›´ã—ç”¨ï¼‰
    df_before_deletion = df.copy()

    # æŒ‡å®šä¼æ¥­ã‚’å‰Šé™¤
    df_cleaned = cleaner.remove_specific_companies(df, target_ids)

    # IDæŒ¯ã‚Šç›´ã—
    print(f"\nðŸ”¢ IDã‚’é€£ç•ªã«æŒ¯ã‚Šç›´ã—ã¦ã„ã¾ã™...")
    df_resequenced = cleaner.resequence_ids(df_cleaned)

    # IDãƒžãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
    old_to_new_mapping = cleaner.create_id_mapping(df_before_deletion, df_resequenced)

    # ãƒ¡ã‚¤ãƒ³CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    if cleaner.save_cleaned_data(df_resequenced):
        print(f"âœ… ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")

        # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
        cleaner.update_related_files(old_to_new_mapping)

        print(f"\nðŸŽ‰ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼")
        print(f"ðŸ“ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {cleaner.backup_file}")
        print(f"ðŸ“ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿: {cleaner.csv_file}")
        print(f"ðŸ“Š æœ€çµ‚ä¼æ¥­æ•°: {len(df_resequenced)}ç¤¾")
        print(f"ðŸ—‘ï¸ å‰Šé™¤ä¼æ¥­æ•°: {len(target_ids)}ç¤¾")

        # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç¢ºèªã®æ¡ˆå†…
        print(f"\nðŸ’¡ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ç¢ºèªã—ã¦ãã ã•ã„:")
        print(f"   http://127.0.0.1:5002/companies")

    else:
        print(f"\nâŒ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¤±æ•—")

if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--specific":
        main_specific_cleanup()
    else:
        main()
