#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
HUGANJOBå–¶æ¥­ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚·ã‚¹ãƒ†ãƒ å°‚ç”¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
- HUGANJOBæ¡ç”¨å–¶æ¥­ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã«ç‰¹åŒ–
- ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã¨ä¸€æ‹¬ãƒ¡ãƒ¼ãƒ«é€ä¿¡åˆ¶å¾¡
- é€ä¿¡çŠ¶æ³è¿½è·¡ã¨çµæœåˆ†æ
"""

import os
import json
import logging
import datetime
import time  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨
import sys
import csv
import uuid
import base64
import subprocess
import threading
import gc  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ç”¨
import json  # HUGANJOBé€ä¿¡å±¥æ­´èª­ã¿è¾¼ã¿ç”¨
import pandas as pd
import csv  # CSVæ“ä½œç”¨
import re   # æ­£è¦è¡¨ç¾ç”¨
import tempfile  # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ç”¨
import shutil    # ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œç”¨
from werkzeug.utils import secure_filename  # ãƒ•ã‚¡ã‚¤ãƒ«åã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç”¨
from flask import Flask, render_template, request, jsonify, abort, Response, redirect

# HUGANJOBå°‚ç”¨ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
os.makedirs("logs/huganjob_dashboard", exist_ok=True)
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("logs/huganjob_dashboard/huganjob_dashboard.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Flask ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆHUGANJOBå°‚ç”¨ï¼‰
# templatesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨staticãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
app = Flask(__name__, template_folder='../templates', static_folder='../static')
app.config['SECRET_KEY'] = 'huganjob_dashboard_secret_key_2025'
app.config['JSON_AS_ASCII'] = False
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size
app.config['UPLOAD_FOLDER'] = 'temp_uploads'

# HUGANJOBå°‚ç”¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®š
INPUT_FILE = 'data/new_input_test.csv'
PROGRESS_FILE = 'data/huganjob_workflow_progress.json'
SENT_EMAILS_FILE = 'data/huganjob_sent_emails_record.csv'
PROCESS_HISTORY_FILE = 'data/huganjob_consolidated/process_history.json'
CONSOLIDATED_DIR = 'data/huganjob_consolidated'
DASHBOARD_CONFIG_FILE = 'config/huganjob_dashboard_config.json'

# æ´¾ç”Ÿç‰ˆå°‚ç”¨ãƒ•ã‚¡ã‚¤ãƒ«å‘½åè¦å‰‡
DERIVATIVE_EMAIL_SENDING_RESULTS = 'data/derivative_email_sending_results.csv'
DERIVATIVE_BOUNCE_TRACKING = 'data/derivative_bounce_tracking_results.csv'
DERIVATIVE_UNSUBSCRIBE_TRACKING = 'data/derivative_unsubscribe_tracking.csv'
DERIVATIVE_EMAIL_OPEN_TRACKING = 'data/derivative_email_open_tracking.csv'

# HUGANJOBçµ±åˆã‚·ã‚¹ãƒ†ãƒ ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
HUGANJOB_EMAIL_SENDING_RESULTS = 'new_email_sending_results.csv'

# äº’æ›æ€§ã®ãŸã‚ã®åˆ¥åå®šç¾©ï¼ˆHUGANJOBãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆï¼‰
NEW_EMAIL_SENDING_RESULTS = HUGANJOB_EMAIL_SENDING_RESULTS
NEW_BOUNCE_TRACKING = DERIVATIVE_BOUNCE_TRACKING
NEW_EMAIL_OPEN_TRACKING = DERIVATIVE_EMAIL_OPEN_TRACKING
DERIVATIVE_WEBSITE_ANALYSIS_PREFIX = 'derivative_website_analysis_results'
DERIVATIVE_EMAIL_EXTRACTION_PREFIX = 'derivative_email_extraction_results'
DERIVATIVE_WORK_RECORD_PREFIX = 'derivative_work_record'
DERIVATIVE_EMAIL_CONTENT_PREFIX = 'derivative_email_content'
DERIVATIVE_ANALYTICS_PREFIX = 'derivative_analytics_report'

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
LOG_FILES = {
    'integrated': 'new_integrated_workflow_*.log',
    'extraction': 'new_prioritized_extraction.log',
    'bounce': 'new_bounce_processing.log',
    'email_sending': 'new_email_sending.log',
    'dashboard': 'new_dashboard.log'
}

# å®Ÿè¡Œä¸­ã®ãƒ—ãƒ­ã‚»ã‚¹
running_processes = {}

# éå»ã®ãƒ—ãƒ­ã‚»ã‚¹å±¥æ­´
process_history = []
process_history_max_size = 100

# ğŸ†• ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–å¼·åŒ–ç”¨ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
process_sync_enabled = True
last_process_sync_time = 0
process_sync_interval = 30  # 30ç§’é–“éš”ã§ãƒ—ãƒ­ã‚»ã‚¹çŠ¶æ…‹åŒæœŸ

def load_process_history():
    """ãƒ—ãƒ­ã‚»ã‚¹å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€"""
    global process_history

    try:
        if os.path.exists(PROCESS_HISTORY_FILE):
            with open(PROCESS_HISTORY_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    process_history = data
                    logger.info(f"ãƒ—ãƒ­ã‚»ã‚¹å±¥æ­´ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(process_history)}ä»¶")
                    return process_history
                else:
                    logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ãŒä¸æ­£ã§ã™")
    except Exception as e:
        logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹å±¥æ­´ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        logger.error(traceback.format_exc())

    # å±¥æ­´ãŒç©ºã®å ´åˆã‚„èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸå ´åˆã¯ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
    process_history = []
    return process_history

def sync_process_states():
    """ğŸ†• å®Ÿéš›ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ã‚»ã‚¹çŠ¶æ…‹ã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰çŠ¶æ…‹ã‚’åŒæœŸ"""
    global running_processes, last_process_sync_time, process_sync_enabled

    if not process_sync_enabled:
        return

    current_time = time.time()
    if current_time - last_process_sync_time < process_sync_interval:
        return

    try:
        import psutil

        # å®Ÿè¡Œä¸­ãƒ—ãƒ­ã‚»ã‚¹ã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
        processes_to_remove = []

        for process_id, process_info in running_processes.items():
            try:
                process = process_info.get('process')
                if process:
                    # ãƒ—ãƒ­ã‚»ã‚¹ãŒå®Ÿéš›ã«çµ‚äº†ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    poll_result = process.poll()
                    if poll_result is not None:
                        logger.info(f"åŒæœŸãƒã‚§ãƒƒã‚¯: ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ãŒçµ‚äº†æ¸ˆã¿ï¼ˆçµ‚äº†ã‚³ãƒ¼ãƒ‰: {poll_result}ï¼‰")

                        # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±ã‚’æ›´æ–°
                        process_info['status'] = 'completed' if poll_result == 0 else 'failed'
                        process_info['end_time'] = datetime.datetime.now()
                        process_info['return_code'] = poll_result

                        # å±¥æ­´ã«è¿½åŠ 
                        add_process_to_history(process_info.copy())

                        # å‰Šé™¤å¯¾è±¡ã«è¿½åŠ 
                        processes_to_remove.append(process_id)

            except Exception as e:
                logger.warning(f"ãƒ—ãƒ­ã‚»ã‚¹ {process_id} åŒæœŸãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

        # çµ‚äº†æ¸ˆã¿ãƒ—ãƒ­ã‚»ã‚¹ã‚’å‰Šé™¤
        for process_id in processes_to_remove:
            if process_id in running_processes:
                logger.info(f"åŒæœŸå‡¦ç†: çµ‚äº†æ¸ˆã¿ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ã‚’å‰Šé™¤")
                del running_processes[process_id]

        last_process_sync_time = current_time

        if processes_to_remove:
            logger.info(f"ãƒ—ãƒ­ã‚»ã‚¹çŠ¶æ…‹åŒæœŸå®Œäº†: {len(processes_to_remove)}ä»¶ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ›´æ–°")

    except ImportError:
        logger.warning("psutilãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ—ãƒ­ã‚»ã‚¹çŠ¶æ…‹åŒæœŸã‚’ç„¡åŠ¹åŒ–ã—ã¾ã™")
        process_sync_enabled = False
    except Exception as e:
        logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹çŠ¶æ…‹åŒæœŸã‚¨ãƒ©ãƒ¼: {e}")

def fix_unmonitored_processes():
    """ğŸ†• ç›£è¦–ã•ã‚Œã¦ã„ãªã„ãƒ—ãƒ­ã‚»ã‚¹ã«ç›£è¦–ã‚’è¿½åŠ """
    global running_processes

    try:
        processes_to_monitor = []

        for process_id, process_info in running_processes.items():
            process = process_info.get('process')
            if process:
                # ãƒ—ãƒ­ã‚»ã‚¹ãŒå®Ÿéš›ã«çµ‚äº†ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                poll_result = process.poll()
                if poll_result is not None:
                    # ãƒ—ãƒ­ã‚»ã‚¹ãŒçµ‚äº†ã—ã¦ã„ã‚‹å ´åˆã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
                    logger.info(f"æœªç›£è¦–ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ãŒçµ‚äº†æ¸ˆã¿ï¼ˆçµ‚äº†ã‚³ãƒ¼ãƒ‰: {poll_result}ï¼‰")

                    process_info['status'] = 'completed' if poll_result == 0 else 'failed'
                    process_info['end_time'] = datetime.datetime.now()
                    process_info['return_code'] = poll_result

                    # å±¥æ­´ã«è¿½åŠ 
                    add_process_to_history(process_info.copy())

                    # å®Œäº†ãƒ­ã‚°ã«è¨˜éŒ²
                    try:
                        completion_log = {
                            'process_id': process_id,
                            'command': process_info.get('command', 'unknown'),
                            'status': process_info['status'],
                            'return_code': poll_result,
                            'end_time': process_info['end_time'].isoformat(),
                            'duration': str(datetime.datetime.now() - process_info.get('start_time', datetime.datetime.now()))
                        }

                        completion_log_file = 'logs/process_completion.log'
                        os.makedirs('logs', exist_ok=True)
                        with open(completion_log_file, 'a', encoding='utf-8') as f:
                            f.write(f"{datetime.datetime.now().isoformat()}: {json.dumps(completion_log, ensure_ascii=False)}\n")
                    except Exception as log_error:
                        logger.warning(f"æœªç›£è¦–ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ãƒ­ã‚°è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {log_error}")

                    # å‰Šé™¤å¯¾è±¡ã«è¿½åŠ 
                    processes_to_monitor.append(process_id)

        # çµ‚äº†æ¸ˆã¿ãƒ—ãƒ­ã‚»ã‚¹ã‚’å‰Šé™¤
        for process_id in processes_to_monitor:
            if process_id in running_processes:
                logger.info(f"æœªç›£è¦–ãƒ—ãƒ­ã‚»ã‚¹ä¿®æ­£: çµ‚äº†æ¸ˆã¿ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ã‚’å‰Šé™¤")
                del running_processes[process_id]

        if processes_to_monitor:
            logger.info(f"æœªç›£è¦–ãƒ—ãƒ­ã‚»ã‚¹ä¿®æ­£å®Œäº†: {len(processes_to_monitor)}ä»¶ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ›´æ–°")

    except Exception as e:
        logger.error(f"æœªç›£è¦–ãƒ—ãƒ­ã‚»ã‚¹ä¿®æ­£ã‚¨ãƒ©ãƒ¼: {e}")

def save_process_history():
    """ãƒ—ãƒ­ã‚»ã‚¹å±¥æ­´ã‚’ä¿å­˜ã™ã‚‹"""
    global process_history

    try:
        # æœ€å¤§ä¿å­˜æ•°ã‚’è¶…ãˆãŸå ´åˆã¯å¤ã„ã‚‚ã®ã‹ã‚‰å‰Šé™¤
        if len(process_history) > process_history_max_size:
            process_history = process_history[-process_history_max_size:]

        with open(PROCESS_HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(process_history, f, ensure_ascii=False, indent=2)

        logger.info(f"ãƒ—ãƒ­ã‚»ã‚¹å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {len(process_history)}ä»¶")
        return True
    except Exception as e:
        logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹å±¥æ­´ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def add_process_to_history(process_info):
    """ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±ã‚’å±¥æ­´ã«è¿½åŠ ã™ã‚‹"""
    global process_history

    try:
        # ãƒ—ãƒ­ã‚»ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ä¿å­˜ã§ããªã„ã®ã§é™¤å¤–
        history_entry = {k: v for k, v in process_info.items() if k != 'process'}

        # æ—¥æ™‚ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        if 'start_time' in history_entry and isinstance(history_entry['start_time'], datetime.datetime):
            history_entry['start_time'] = history_entry['start_time'].strftime('%Y-%m-%d %H:%M:%S')
        if 'end_time' in history_entry and isinstance(history_entry['end_time'], datetime.datetime):
            history_entry['end_time'] = history_entry['end_time'].strftime('%Y-%m-%d %H:%M:%S')

        # å±¥æ­´ã«è¿½åŠ 
        process_history.append(history_entry)

        # å±¥æ­´ã‚’ä¿å­˜
        save_process_history()

        return True
    except Exception as e:
        logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹å±¥æ­´ã¸ã®è¿½åŠ ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

# ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
company_data_cache = None
company_data_last_updated = None
company_stats_cache = None
company_stats_last_updated = None

# çµ±è¨ˆæƒ…å ±ã‚­ãƒ£ãƒƒã‚·ãƒ¥
stats_cache = None
stats_last_updated = None

# æ—¥åˆ¥çµ±è¨ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥
daily_stats_cache = None
daily_stats_last_updated = None

# é–‹å°ç‡åˆ†æã‚­ãƒ£ãƒƒã‚·ãƒ¥
open_rate_cache = None
open_rate_last_updated = None

# ä½œæ¥­è¨˜éŒ²ã‚­ãƒ£ãƒƒã‚·ãƒ¥
work_logs_cache = None
work_logs_last_updated = None

# ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
filtered_companies_cache = {}
filtered_companies_last_updated = {}

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ç”¨ã®è¨­å®šï¼ˆ2025-06-26 æœ€é©åŒ–ãƒ»å³æ™‚åæ˜ å¯¾å¿œï¼‰
CACHE_TIMEOUT_SECONDS = 60  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ™‚é–“ã‚’1åˆ†ã«çŸ­ç¸®ï¼ˆå³æ™‚åæ˜ å„ªå…ˆï¼‰
STATS_CACHE_TIMEOUT_SECONDS = 30  # çµ±è¨ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ™‚é–“ã‚’30ç§’ã«çŸ­ç¸®
DAILY_STATS_CACHE_TIMEOUT = 300  # æ—¥åˆ¥çµ±è¨ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ™‚é–“ï¼ˆ5åˆ†ï¼‰
OPEN_RATE_CACHE_TIMEOUT = 300  # é–‹å°ç‡åˆ†æã‚­ãƒ£ãƒƒã‚·ãƒ¥æ™‚é–“ï¼ˆ5åˆ†ï¼‰
ENABLE_PERFORMANCE_LOGGING = False  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é‡è¦–ã§ãƒ­ã‚°å‰Šæ¸›
ENABLE_DEBUG_LOGGING = False  # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’ç„¡åŠ¹åŒ–
LAZY_LOADING_ENABLED = True
STARTUP_LAZY_LOADING = True  # èµ·å‹•æ™‚ã®é…å»¶èª­ã¿è¾¼ã¿æœ‰åŠ¹åŒ–
MAX_COMPANIES_PER_PAGE = 50  # ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºç¸®å°ï¼ˆæ¨å¥¨è¨­å®šï¼‰
PERFORMANCE_MODE = True  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é‡è¦–ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹åŒ–

# æ—¥æ™‚è§£æã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šï¼‰
_datetime_cache = {}

def parse_datetime_optimized(date_str):
    """æœ€é©åŒ–ã•ã‚ŒãŸæ—¥æ™‚è§£æé–¢æ•°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
    if not date_str or not date_str.strip():
        return None

    date_str = date_str.strip()

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    if date_str in _datetime_cache:
        return _datetime_cache[date_str]

    # è¤‡æ•°ã®æ—¥æ™‚å½¢å¼ã«å¯¾å¿œï¼ˆãƒã‚¤ã‚¯ãƒ­ç§’ä»˜ãã‚‚å«ã‚€ï¼‰
    date_formats = [
        '%Y-%m-%d %H:%M:%S.%f',  # ãƒã‚¤ã‚¯ãƒ­ç§’ä»˜ã
        '%Y-%m-%d %H:%M:%S',
        '%Y-%m-%d %H:%M',
        '%Y-%m-%d'
    ]

    for date_format in date_formats:
        try:
            result = datetime.datetime.strptime(date_str, date_format)
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆæœ€å¤§1000ä»¶ã¾ã§ï¼‰
            if len(_datetime_cache) < 1000:
                _datetime_cache[date_str] = result
            return result
        except ValueError:
            continue

    return None

def ensure_directories():
    """å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
    directories = [CONSOLIDATED_DIR, 'new_logs', 'new_archives']
    for directory in directories:
        if not os.path.exists(directory):
            os.makedirs(directory)
            logger.info(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ: {directory}")

def initialize_config_files():
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–"""
    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
    if not os.path.exists(DASHBOARD_CONFIG_FILE):
        config = {
            "last_updated": datetime.datetime.now().isoformat(),
            "primary_files": {
                "email_extraction": f"{CONSOLIDATED_DIR}/new_email_extraction_results_consolidated.csv",
                "prioritized_extraction": f"{CONSOLIDATED_DIR}/new_email_extraction_results_consolidated.csv",
                "website_analysis": f"{CONSOLIDATED_DIR}/new_website_analysis_results_consolidated.csv",
                "email_sending": f"{CONSOLIDATED_DIR}/new_sent_emails_record_consolidated.csv"
            },
            "file_locations": {
                "consolidated_dir": CONSOLIDATED_DIR,
                "archive_dir": "new_archives"
            }
        }
        with open(DASHBOARD_CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        logger.info(f"ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ: {DASHBOARD_CONFIG_FILE}")

    # é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«
    if not os.path.exists(PROGRESS_FILE):
        progress = {
            "extract_emails": {"status": "not_started", "progress": 0, "last_updated": None},
            "analyze_websites": {"status": "not_started", "progress": 0, "last_updated": None},
            "prepare_email_content": {"status": "not_started", "progress": 0, "last_updated": None},
            "send_emails": {"status": "not_started", "progress": 0, "last_updated": None},
            "process_bounces": {"status": "not_started", "progress": 0, "last_updated": None},
            "analyze_results": {"status": "not_started", "progress": 0, "last_updated": None}
        }
        with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)
        logger.info(f"é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ: {PROGRESS_FILE}")

def check_input_file():
    """å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ã¨å†…å®¹ã‚’ç¢ºèª"""
    if not os.path.exists(INPUT_FILE):
        logger.error(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {INPUT_FILE}")
        return False

    # è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã™
    encodings = ['utf-8-sig', 'utf-8', 'shift_jis', 'cp932', 'iso-2022-jp']

    for encoding in encodings:
        try:
            with open(INPUT_FILE, 'r', encoding=encoding) as f:
                reader = csv.DictReader(f)
                rows = list(reader)
                logger.info(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ« {INPUT_FILE} ã‚’ç¢ºèªã—ã¾ã—ãŸ: {len(rows)}ç¤¾ã®ãƒ‡ãƒ¼ã‚¿ (ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {encoding})")
                return True
        except UnicodeDecodeError:
            continue
        except Exception as e:
            logger.error(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({encoding}): {e}")
            continue

    logger.error("å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ä¿å­˜ã—ã¦ãã ã•ã„ã€‚")
    return False

def load_progress():
    """é€²æ—æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        if os.path.exists(PROGRESS_FILE):
            with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é€²æ—æƒ…å ±ã‚’è¿”ã™
            return {
                "extract_emails": {"status": "not_started", "progress": 0, "last_updated": None},
                "analyze_websites": {"status": "not_started", "progress": 0, "last_updated": None},
                "prepare_email_content": {"status": "not_started", "progress": 0, "last_updated": None},
                "send_emails": {"status": "not_started", "progress": 0, "last_updated": None},
                "process_bounces": {"status": "not_started", "progress": 0, "last_updated": None},
                "analyze_results": {"status": "not_started", "progress": 0, "last_updated": None}
            }
    except Exception as e:
        logger.error(f"é€²æ—æƒ…å ±ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return {}

def get_step_display_name(step):
    """HUGANJOBå–¶æ¥­ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚¹ãƒ†ãƒƒãƒ—ã®è¡¨ç¤ºåã‚’å–å¾—"""
    step_names = {
        'huganjob_email_resolution': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æ±ºå®š',
        'huganjob_email_preparation': 'ãƒ¡ãƒ¼ãƒ«å†…å®¹æº–å‚™',
        'huganjob_bulk_sending': 'ä¸€æ‹¬ãƒ¡ãƒ¼ãƒ«é€ä¿¡',
        'huganjob_delivery_tracking': 'é…ä¿¡çŠ¶æ³è¿½è·¡',
        'huganjob_results_analysis': 'é€ä¿¡çµæœåˆ†æ'
    }
    return step_names.get(step, step)

def get_step_info(step, progress_data):
    """ã‚¹ãƒ†ãƒƒãƒ—ã®è©³ç´°æƒ…å ±ã‚’å–å¾—"""
    step_data = progress_data.get(step, {})
    return {
        'name': step,
        'display_name': get_step_display_name(step),
        'status': step_data.get('status', 'not_started'),
        'progress': step_data.get('progress', 0),
        'last_updated': step_data.get('last_updated'),
        'description': get_step_description(step)
    }

def get_step_description(step):
    """HUGANJOBå–¶æ¥­ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚¹ãƒ†ãƒƒãƒ—ã®èª¬æ˜ã‚’å–å¾—"""
    descriptions = {
        'huganjob_email_resolution': 'CSVãƒ‡ãƒ¼ã‚¿ã¨ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æ±ºå®šã—ã¾ã™',
        'huganjob_email_preparation': 'ä¼æ¥­åãƒ»å‹Ÿé›†è·ç¨®ã‚’å‹•çš„æŒ¿å…¥ã—ãŸHTMLãƒ¡ãƒ¼ãƒ«ã‚’æº–å‚™ã—ã¾ã™',
        'huganjob_bulk_sending': 'HUGANJOBæ¡ç”¨å–¶æ¥­ãƒ¡ãƒ¼ãƒ«ã‚’ä¸€æ‹¬é€ä¿¡ã—ã¾ã™',
        'huganjob_delivery_tracking': 'ãƒ¡ãƒ¼ãƒ«é…ä¿¡çŠ¶æ³ã¨ãƒã‚¦ãƒ³ã‚¹ã‚’è¿½è·¡ã—ã¾ã™',
        'huganjob_results_analysis': 'é€ä¿¡çµæœã‚’åˆ†æã—ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™'
    }
    return descriptions.get(step, '')

@app.route('/')
def index():
    """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ - åˆ¶å¾¡ãƒ‘ãƒãƒ«æ©Ÿèƒ½çµ±åˆç‰ˆ"""
    try:
        progress_data = load_progress()

        # HUGANJOBå–¶æ¥­ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚¹ãƒ†ãƒƒãƒ—
        steps = [
            'huganjob_email_resolution',
            'huganjob_email_preparation',
            'huganjob_bulk_sending',
            'huganjob_delivery_tracking',
            'huganjob_results_analysis'
        ]

        steps_info = [get_step_info(step, progress_data) for step in steps]

        # åŸºæœ¬çµ±è¨ˆæƒ…å ±ï¼ˆè»½é‡ç‰ˆï¼‰
        stats = get_basic_stats_lightweight()

        # å®Ÿè¡Œä¸­ã®ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±ã‚’å–å¾—ï¼ˆæœ€å¤§5ä»¶ã¾ã§ï¼‰
        active_processes = []
        process_count = 0
        for pid, info in running_processes.items():
            if info['status'] == 'running' and process_count < 5:
                active_processes.append({
                    'id': pid,
                    'command': info['command'][:50] + '...' if len(info['command']) > 50 else info['command'],
                    'start_time': info['start_time'].strftime('%H:%M:%S'),
                    'status': info['status'],
                    'duration': str(datetime.datetime.now() - info['start_time']).split('.')[0],
                    'description': info.get('description', info['command'])
                })
                process_count += 1

        # ç›´è¿‘ã®ãƒ—ãƒ­ã‚»ã‚¹å±¥æ­´ã‚’å–å¾—ï¼ˆ3ä»¶ï¼‰
        recent_process_history = load_process_history_lightweight(3)

        return render_template(
            'index.html',
            progress=progress_data,
            steps=steps_info,
            email_stats=stats,
            processes=active_processes,
            recent_history=recent_process_history,
            last_updated=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            get_step_display_name=get_step_display_name,
            lazy_loading=STARTUP_LAZY_LOADING
        )
    except Exception as e:
        logger.error(f"ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯æœ€å°é™ã®æƒ…å ±ã§è¡¨ç¤º
        return render_template(
            'index.html',
            progress={'current_step': 'huganjob_email_resolution', 'completed_steps': []},
            steps=[],
            email_stats={'total_companies': 0, 'sent_emails': 0},
            processes=[],
            recent_history=[],
            last_updated=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            get_step_display_name=get_step_display_name,
            lazy_loading=False
        )

def get_basic_stats_lightweight():
    """è»½é‡ç‰ˆåŸºæœ¬çµ±è¨ˆæƒ…å ±ã‚’å–å¾—ï¼ˆèµ·å‹•æ™‚é–“çŸ­ç¸®ç”¨ï¼‰"""
    global stats_cache, stats_last_updated

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    if (stats_cache is not None and
        stats_last_updated is not None and
        (datetime.datetime.now() - stats_last_updated).seconds < STATS_CACHE_TIMEOUT_SECONDS):
        return stats_cache

    try:
        # è»½é‡ç‰ˆçµ±è¨ˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã®ã¿ï¼‰
        stats = {
            'total_companies': 0,
            'extracted_emails': 0,
            'analyzed_websites': 0,
            'sent_emails': 0,
            'bounced_emails': 0,
            'files_status': {}
        }

        # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼ˆèª­ã¿è¾¼ã¿ãªã—ï¼‰
        files_to_check = {
            'input': INPUT_FILE,
            'email_extraction': f"{CONSOLIDATED_DIR}/new_email_extraction_results_consolidated.csv",
            'website_analysis': f"{CONSOLIDATED_DIR}/new_website_analysis_results_consolidated.csv",
            'email_sending': f"{CONSOLIDATED_DIR}/new_sent_emails_record_consolidated.csv",
            'bounce_tracking': 'comprehensive_bounce_tracking_results.csv'
        }

        for key, file_path in files_to_check.items():
            stats['files_status'][key] = os.path.exists(file_path)
            if os.path.exists(file_path):
                try:
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ã¿å–å¾—ï¼ˆé«˜é€Ÿï¼‰
                    file_size = os.path.getsize(file_path)
                    stats[f'{key}_size'] = file_size
                except:
                    stats[f'{key}_size'] = 0

        # åŸºæœ¬çš„ãªè¡Œæ•°ã®ã¿å–å¾—ï¼ˆæœ€å°é™ï¼‰
        if os.path.exists(INPUT_FILE):
            try:
                with open(INPUT_FILE, 'r', encoding='utf-8-sig') as f:
                    stats['total_companies'] = sum(1 for line in f) - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼é™¤ã
            except:
                stats['total_companies'] = 4006  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
        stats_cache = stats
        stats_last_updated = datetime.datetime.now()

        return stats

    except Exception as e:
        logger.warning(f"è»½é‡ç‰ˆçµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'total_companies': 4006,
            'extracted_emails': 0,
            'analyzed_websites': 0,
            'sent_emails': 0,
            'bounced_emails': 0,
            'files_status': {}
        }

# get_bounce_detailed_stats é–¢æ•°ã¯å‰Šé™¤ã•ã‚Œã¾ã—ãŸï¼ˆbounce-analysis ãƒšãƒ¼ã‚¸ãŒä¸è¦ãªãŸã‚ï¼‰

def get_basic_stats():
    """åŸºæœ¬çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
    try:
        companies = load_company_data()

        total_companies = len(companies)
        email_extracted = sum(1 for c in companies if c.get('email_extracted', False))
        analyzed = sum(1 for c in companies if c.get('rank'))
        email_sent = sum(1 for c in companies if c.get('email_sent', False))
        bounced = sum(1 for c in companies if c.get('bounced', False))

        # é–‹å°çµ±è¨ˆã‚’å–å¾—ï¼ˆæ–°ã—ã„é–‹å°ç‡ç®¡ç†æ©Ÿèƒ½ã€ãƒã‚¦ãƒ³ã‚¹ä¼æ¥­ã‚’é™¤å¤–ï¼‰
        try:
            open_rate_stats = get_comprehensive_open_rate_stats()
            opened = open_rate_stats['unique_opens']
            open_rate = open_rate_stats['open_rate']
            valid_sent_count = open_rate_stats.get('valid_sent_count', email_sent)
            bounced_count = open_rate_stats.get('bounced_count', bounced)
            bounce_rate_corrected = open_rate_stats.get('bounce_rate', 0.0)
        except Exception as e:
            logger.error(f"é–‹å°ç‡çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            opened = 0
            open_rate = 0.0
            valid_sent_count = email_sent
            bounced_count = bounced
            bounce_rate_corrected = round((bounced / email_sent * 100) if email_sent > 0 else 0, 2)

        # ãƒã‚¦ãƒ³ã‚¹è©³ç´°çµ±è¨ˆã‚’å–å¾—ï¼ˆç°¡ç•¥åŒ–ï¼‰
        bounce_details = {'total_candidates': 0, 'valid_bounces': 0, 'csv_records': 0}

        # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã‚µãƒãƒªãƒ¼ã‚’å–å¾—
        try:
            integrity_summary = get_bounce_open_inconsistency_summary()
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã‚µãƒãƒªãƒ¼å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            integrity_summary = {'has_issues': False, 'total_issues': 0, 'integrity_rate': 100.0}

        return {
            'total': total_companies,
            'email_extracted': email_extracted,
            'analyzed': analyzed,
            'email_sent': email_sent,
            'bounced': bounced_count,  # ä¿®æ­£ã•ã‚ŒãŸãƒã‚¦ãƒ³ã‚¹æ•°
            'success': valid_sent_count,  # ãƒã‚¦ãƒ³ã‚¹é™¤å¤–å¾Œã®æœ‰åŠ¹é€ä¿¡æ•°
            'pending': email_extracted - email_sent,
            'opened': opened,
            'open_rate': open_rate,  # ãƒã‚¦ãƒ³ã‚¹é™¤å¤–å¾Œã®é–‹å°ç‡
            'bounce_rate': bounce_rate_corrected,  # ä¿®æ­£ã•ã‚ŒãŸãƒã‚¦ãƒ³ã‚¹ç‡
            'success_rate': round(((valid_sent_count) / email_sent * 100) if email_sent > 0 else 0, 2),
            'bounce_details': bounce_details,
            'integrity_summary': integrity_summary  # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æƒ…å ±ã‚’è¿½åŠ 
        }
    except Exception as e:
        logger.error(f"çµ±è¨ˆæƒ…å ±ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return {
            'total': 0,
            'email_extracted': 0,
            'analyzed': 0,
            'email_sent': 0,
            'bounced': 0,
            'opened': 0,
            'open_rate': 0.0,
            'bounce_details': {}
        }

@app.route('/companies')
def companies():
    """ä¼æ¥­ä¸€è¦§ãƒšãƒ¼ã‚¸ - ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œ"""
    try:
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
        page = request.args.get('page', 1, type=int)
        filter_type = request.args.get('filter', 'all')
        search_query = request.args.get('search', '')
        per_page = min(request.args.get('per_page', MAX_COMPANIES_PER_PAGE, type=int), MAX_COMPANIES_PER_PAGE)

        # é…å»¶èª­ã¿è¾¼ã¿å¯¾å¿œï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚’ç¶­æŒã—ã¤ã¤å…¨ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼‰
        if STARTUP_LAZY_LOADING:
            all_companies = load_company_data_lazy()
        else:
            all_companies = load_company_data()

        total_count = len(all_companies)

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆåŠ¹ç‡åŒ–ï¼‰
        filtered_companies = all_companies
        if filter_type != 'all':
            if filter_type == 'email-success':
                filtered_companies = [c for c in all_companies if c.get('email')]
            elif filter_type == 'email-failure':
                filtered_companies = [c for c in all_companies if not c.get('email')]
            elif filter_type == 'sent':
                filtered_companies = [c for c in all_companies if c.get('email_sent')]
            elif filter_type == 'bounced':
                # ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ã®åˆ¤å®šã‚’å¼·åŒ–
                filtered_companies = []
                for c in all_companies:
                    # è¤‡æ•°ã®æ–¹æ³•ã§ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
                    is_bounced = (
                        c.get('bounced') == True or
                        c.get('bounce_status') == 'permanent' or
                        str(c.get('bounce_status', '')).strip().lower() == 'permanent' or
                        str(c.get('csv_bounce_status', '')).strip().lower() == 'permanent'
                    )
                    if is_bounced:
                        filtered_companies.append(c)

                # ãƒ‡ãƒãƒƒã‚°: ãƒã‚¦ãƒ³ã‚¹ä¼æ¥­ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
                if not PERFORMANCE_MODE:
                    logger.info(f"ãƒã‚¦ãƒ³ã‚¹ä¼æ¥­ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ: {len(filtered_companies)}ç¤¾ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
                    if len(filtered_companies) > 0:
                        # æœ€åˆã®5ç¤¾ã®è©³ç´°ã‚’ãƒ­ã‚°å‡ºåŠ›
                        for i, c in enumerate(filtered_companies[:5]):
                            logger.info(f"ãƒã‚¦ãƒ³ã‚¹ä¼æ¥­ {i+1}: ID={c.get('id')}, ä¼æ¥­å={c.get('name')}, "
                                      f"bounced={c.get('bounced')}, bounce_status={c.get('bounce_status')}, "
                                      f"csv_bounce_status={c.get('csv_bounce_status')}")

        # æ¤œç´¢ï¼ˆåŠ¹ç‡åŒ–ï¼‰
        if search_query:
            search_lower = search_query.lower()
            filtered_companies = [c for c in filtered_companies
                                if search_lower in c.get('name', '').lower() or
                                   search_lower in c.get('website', '').lower()]

        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
        total_companies = len(filtered_companies)
        start = (page - 1) * per_page
        end = start + per_page
        paginated_companies = filtered_companies[start:end]

        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±
        total_pages = (total_companies + per_page - 1) // per_page
        has_prev = page > 1
        has_next = page < total_pages

        total_pages = (len(filtered_companies) + per_page - 1) // per_page

        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±
        has_prev = page > 1
        has_next = page < total_pages
        prev_num = page - 1 if has_prev else None
        next_num = page + 1 if has_next else None

        # çµ±è¨ˆæƒ…å ±ï¼ˆç°¡æ˜“ç‰ˆï¼‰- ãƒã‚¦ãƒ³ã‚¹åˆ¤å®šã‚’å¼·åŒ–
        def is_company_bounced(c):
            """ä¼æ¥­ã®ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ã‚’æ­£ç¢ºã«åˆ¤å®š"""
            is_bounced = (
                c.get('bounced') == True or
                c.get('bounce_status') == 'permanent' or
                str(c.get('bounce_status', '')).strip().lower() == 'permanent' or
                str(c.get('csv_bounce_status', '')).strip().lower() == 'permanent'
            )
            return is_bounced

        # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—ï¼ˆçµ±ä¸€é€ä¿¡æ•°ã‚’ä½¿ç”¨ï¼‰
        bounced_companies = [c for c in all_companies if is_company_bounced(c)]

        # çµ±ä¸€ã•ã‚ŒãŸé€ä¿¡æ•°ã‚’å–å¾—
        unified_sent_count = get_unified_sent_email_count()

        stats = {
            'email_extracted': len([c for c in all_companies if c.get('email')]),
            'email_not_extracted': len([c for c in all_companies if not c.get('email')]),
            'rank_a': len([c for c in all_companies if c.get('rank') == 'A']),
            'rank_b': len([c for c in all_companies if c.get('rank') == 'B']),
            'rank_c': len([c for c in all_companies if c.get('rank') == 'C']),
            'not_analyzed': len([c for c in all_companies if not c.get('rank')]),
            'email_sent': unified_sent_count,  # çµ±ä¸€ã•ã‚ŒãŸé€ä¿¡æ•°ã‚’ä½¿ç”¨
            'email_not_sent': len(all_companies) - unified_sent_count,
            'delivered': unified_sent_count - len(bounced_companies),  # é€ä¿¡æ•°ã‹ã‚‰ãƒã‚¦ãƒ³ã‚¹æ•°ã‚’å¼•ã
            'bounced': len(bounced_companies)
        }

        # ãƒ‡ãƒãƒƒã‚°: çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        logger.info(f"çµ±è¨ˆæƒ…å ±è¨ˆç®—: ç·ä¼æ¥­æ•°={len(all_companies)}, ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºæ¸ˆã¿={stats['email_extracted']}, "
                  f"ãƒ¡ãƒ¼ãƒ«é€ä¿¡æ¸ˆã¿={stats['email_sent']}ï¼ˆçµ±ä¸€é€ä¿¡æ•°ï¼‰, ãƒã‚¦ãƒ³ã‚¹ä¼æ¥­={stats['bounced']}")

        # ãƒã‚¦ãƒ³ã‚¹ä¼æ¥­ã®è©³ç´°ç¢ºèªï¼ˆæœ€åˆã®5ç¤¾ï¼‰
        if len(bounced_companies) > 0:
            logger.info(f"ãƒã‚¦ãƒ³ã‚¹ä¼æ¥­è©³ç´°ï¼ˆæœ€åˆã®5ç¤¾ï¼‰:")
            for i, c in enumerate(bounced_companies[:5]):
                logger.info(f"  {i+1}. ID={c.get('id')}, ä¼æ¥­å={c.get('name')}, "
                          f"bounced={c.get('bounced')}, bounce_status={c.get('bounce_status')}, "
                          f"csv_bounce_status={c.get('csv_bounce_status')}")

        return render_template(
            'companies.html',
            companies=paginated_companies,
            current_page=page,
            page=page,
            total_pages=total_pages,
            total_companies=len(all_companies),
            total_filtered_companies=len(filtered_companies),
            per_page=per_page,
            has_prev=has_prev,
            has_next=has_next,
            prev_num=prev_num,
            next_num=next_num,
            current_filter=filter_type,
            current_search=search_query,
            stats=stats,
            last_updated=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        )
    except Exception as e:
        logger.error(f"ä¼æ¥­ä¸€è¦§ã®è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        logger.error(f"è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±: {traceback.format_exc()}")

        # ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã‚’è¿”ã™
        error_html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>ã‚¨ãƒ©ãƒ¼ - æ–°ã—ã„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰</title>
            <meta charset="utf-8">
        </head>
        <body>
            <h1>ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ</h1>
            <p>ä¼æ¥­ä¸€è¦§ã®è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚</p>
            <p>ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}</p>
            <p><a href="/">ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã«æˆ»ã‚‹</a></p>
        </body>
        </html>
        """
        return error_html, 500

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
company_data_cache = None
company_data_last_updated = None

def clear_cache():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
    global company_data_cache, company_data_last_updated
    company_data_cache = None
    company_data_last_updated = None
    logger.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
stats_cache = None
stats_last_updated = None

def clear_all_caches():
    """å…¨ã¦ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
    global company_data_cache, company_data_last_updated, stats_cache, stats_last_updated
    global company_stats_cache, company_stats_last_updated, work_logs_cache, work_logs_last_updated
    global filtered_companies_cache, filtered_companies_last_updated

    company_data_cache = None
    company_data_last_updated = None
    stats_cache = None
    stats_last_updated = None
    company_stats_cache = None
    company_stats_last_updated = None
    work_logs_cache = None
    work_logs_last_updated = None
    filtered_companies_cache.clear()
    filtered_companies_last_updated.clear()

    # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
    gc.collect()

    logger.info("å…¨ã¦ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã€ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã¾ã—ãŸ")

def optimize_memory():
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æœ€é©åŒ–"""
    try:
        # å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
        current_time = datetime.datetime.now()

        # æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å‰Šé™¤
        expired_filters = []
        for key, last_updated in filtered_companies_last_updated.items():
            if (current_time - last_updated).seconds > CACHE_TIMEOUT_SECONDS * 2:
                expired_filters.append(key)

        for key in expired_filters:
            if key in filtered_companies_cache:
                del filtered_companies_cache[key]
            if key in filtered_companies_last_updated:
                del filtered_companies_last_updated[key]

        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        collected = gc.collect()

        if collected > 0:
            logger.info(f"ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–å®Œäº†: {collected}å€‹ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å›å")

        return collected

    except Exception as e:
        logger.warning(f"ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return 0

def load_csv_optimized(file_path, max_rows=None, columns=None):
    """æœ€é©åŒ–ã•ã‚ŒãŸCSVèª­ã¿è¾¼ã¿"""
    try:
        start_time = datetime.datetime.now()

        # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        if not os.path.exists(file_path):
            logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return None

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        file_size = os.path.getsize(file_path) / 1024 / 1024  # MB

        # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿
        if file_size > 10:  # 10MBä»¥ä¸Š
            logger.info(f"å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º ({file_size:.1f}MB): ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿ã‚’ä½¿ç”¨")

            chunks = []
            chunk_size = 1000  # 1000è¡Œãšã¤èª­ã¿è¾¼ã¿

            for chunk in pd.read_csv(file_path, encoding='utf-8-sig', chunksize=chunk_size):
                if columns:
                    # å¿…è¦ãªåˆ—ã®ã¿é¸æŠ
                    available_columns = [col for col in columns if col in chunk.columns]
                    if available_columns:
                        chunk = chunk[available_columns]

                chunks.append(chunk)

                # æœ€å¤§è¡Œæ•°åˆ¶é™
                if max_rows and len(chunks) * chunk_size >= max_rows:
                    break

            if chunks:
                df = pd.concat(chunks, ignore_index=True)
                if max_rows:
                    df = df.head(max_rows)
            else:
                df = pd.DataFrame()
        else:
            # é€šå¸¸ã®èª­ã¿è¾¼ã¿
            df = pd.read_csv(file_path, encoding='utf-8-sig')

            if columns:
                available_columns = [col for col in columns if col in df.columns]
                if available_columns:
                    df = df[available_columns]

            if max_rows:
                df = df.head(max_rows)

        load_time = (datetime.datetime.now() - start_time).total_seconds()
        logger.info(f"CSVèª­ã¿è¾¼ã¿å®Œäº†: {file_path} ({len(df)}è¡Œ, {load_time:.2f}ç§’)")

        return df

    except Exception as e:
        logger.error(f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
        return None

def get_csv_info_fast(file_path):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’é«˜é€Ÿå–å¾—ï¼ˆè¡Œæ•°ã¨ã‚µã‚¤ã‚ºã®ã¿ï¼‰"""
    try:
        if not os.path.exists(file_path):
            return None

        file_size = os.path.getsize(file_path) / 1024 / 1024  # MB

        # è¡Œæ•°ã‚’é«˜é€Ÿã‚«ã‚¦ãƒ³ãƒˆ
        with open(file_path, 'r', encoding='utf-8-sig') as f:
            row_count = sum(1 for line in f) - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼é™¤ã

        return {
            'file_path': file_path,
            'size_mb': file_size,
            'rows': row_count,
            'exists': True
        }

    except Exception as e:
        logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
        return {
            'file_path': file_path,
            'size_mb': 0,
            'rows': 0,
            'exists': False,
            'error': str(e)
        }

def load_company_data_lazy():
    """ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’é…å»¶èª­ã¿è¾¼ã¿ï¼ˆå¿…è¦æ™‚ã®ã¿ï¼‰"""
    global company_data_cache, company_data_last_updated

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ãªå ´åˆã¯è¿”ã™
    if (company_data_cache is not None and
        company_data_last_updated is not None and
        (datetime.datetime.now() - company_data_last_updated).seconds < CACHE_TIMEOUT_SECONDS):
        return company_data_cache

    # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¯å¿…è¦æ™‚ã®ã¿
    return load_company_data()

def load_company_data():
    """ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆæ¨™æº–ãƒ‡ãƒ¼ã‚¿ã¨åºƒå‘Šå–¶æ¥­ãƒ‡ãƒ¼ã‚¿ã®ä¸¡æ–¹ï¼‰"""
    global company_data_cache, company_data_last_updated

    logger.info("ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚’é–‹å§‹ã—ã¾ã™")

    # æ—¢å­˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ï¼ˆé€ä¿¡å±¥æ­´ã‚„ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ä¿æŒã™ã‚‹ãŸã‚ï¼‰
    existing_cache = {}
    if company_data_cache is not None:
        # ä¼æ¥­IDã‚’ã‚­ãƒ¼ã¨ã—ã¦æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        for company in company_data_cache:
            if company.get('id'):
                company_id = int(company['id'])
                # é‡è¦ãªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æƒ…å ±ã‚’ä¿å­˜
                existing_cache[company_id] = {
                    'email_sent': company.get('email_sent', False),
                    'sent_date': company.get('sent_date'),
                    'email_subject': company.get('email_subject'),
                    'email_content': company.get('email_content'),
                    'bounced': company.get('bounced', False),
                    'bounce_reason': company.get('bounce_reason'),
                    'history': company.get('history', []),
                    'email_extracted': company.get('email_extracted', False),
                    'email_confidence': company.get('email_confidence'),
                    'extraction_method': company.get('extraction_method'),
                    'rank': company.get('rank'),
                    'analysis_completed': company.get('analysis_completed', False)
                }
        logger.info(f"{len(existing_cache)}ç¤¾ã®æ—¢å­˜ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æƒ…å ±ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ä¿å­˜ã—ã¾ã—ãŸ")

    # ãƒ‡ãƒãƒƒã‚°: å¸¸ã«æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    logger.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–ã—ã¦æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã™")

    try:
        companies = []

        # ğŸ†• HUGANJOB ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºçµæœã‚’èª­ã¿è¾¼ã¿ï¼ˆä¿®å¾©ç‰ˆï¼‰
        huganjob_results = {}
        huganjob_results_file = 'huganjob_email_resolution_results.csv'

        # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèªã‚’å¼·åŒ–
        if os.path.exists(huganjob_results_file):
            logger.info(f"HUGANJOB ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºçµæœã‚’èª­ã¿è¾¼ã¿ä¸­: {huganjob_results_file}")
            try:
                # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
                results_df = pd.read_csv(huganjob_results_file, encoding='utf-8')
                if results_df is not None and not results_df.empty:
                    logger.info(f"ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºçµæœã‹ã‚‰{len(results_df)}è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

                    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’å¼·åŒ–
                    for idx, row in results_df.iterrows():
                        try:
                            company_id = row.get('company_id')
                            if pd.notna(company_id) and str(company_id).strip():
                                company_id_int = int(float(company_id))  # floatçµŒç”±ã§intã«å¤‰æ›
                                huganjob_results[company_id_int] = {
                                    'final_email': str(row.get('final_email', '')).strip() if pd.notna(row.get('final_email')) else '',
                                    'email_source': str(row.get('extraction_method', '')).strip() if pd.notna(row.get('extraction_method')) else '',
                                    'status': str(row.get('status', '')).strip() if pd.notna(row.get('status')) else ''
                                }
                        except (ValueError, TypeError) as e:
                            logger.debug(f"è¡Œã‚¹ã‚­ãƒƒãƒ—ï¼ˆIDå¤‰æ›ã‚¨ãƒ©ãƒ¼ï¼‰: {e}")
                            continue

                    logger.info(f"HUGANJOB ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºçµæœ: {len(huganjob_results)}ç¤¾")
                else:
                    logger.warning("HUGANJOB ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™")
            except Exception as e:
                logger.error(f"HUGANJOB ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºçµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®è¾æ›¸ã‚’ä½¿ç”¨ã—ã¦å‡¦ç†ã‚’ç¶™ç¶š
                huganjob_results = {}
        else:
            logger.warning(f"HUGANJOB ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {huganjob_results_file}")
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã‚‚ç©ºã®è¾æ›¸ã§å‡¦ç†ã‚’ç¶™ç¶š
            huganjob_results = {}

        # æ–°ã—ã„æ¡ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆçš„ã«èª­ã¿è¾¼ã¿
        new_input_file = 'data/new_input_test.csv'
        if os.path.exists(new_input_file):
            logger.info(f"æ–°ã—ã„æ¡ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {new_input_file}")
            try:
                # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ‰‹å‹•ã§èª­ã¿è¾¼ã¿ã€å•é¡Œã‚’å›é¿
                import csv

                companies_data = []
                with open(new_input_file, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        companies_data.append(row)

                # DataFrameã«å¤‰æ›
                df = pd.DataFrame(companies_data)

                if df is not None and not df.empty:
                    logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰{len(df)}è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                    logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ—å: {list(df.columns)}")
                    # æœ€åˆã®æ•°è¡Œã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
                    if len(df) > 0:
                        logger.info(f"æœ€åˆã®è¡Œã®ãƒ‡ãƒ¼ã‚¿: {dict(df.iloc[0])}")

                    # å¿…è¦ãªåˆ—ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    required_columns = ['ID', 'ä¼æ¥­å', 'ä¼æ¥­ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸', 'æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'å‹Ÿé›†è·ç¨®']
                    missing_columns = [col for col in required_columns if col not in df.columns]
                    if missing_columns:
                        logger.error(f"å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_columns}")
                        logger.error(f"åˆ©ç”¨å¯èƒ½ãªåˆ—: {list(df.columns)}")
                        return companies
                    companies_added = 0
                    for idx, row in df.iterrows():
                        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®å‡¦ç†ï¼ˆ"â€"ã¯ç©ºã¨ã—ã¦æ‰±ã†ï¼‰
                        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å®Ÿéš›ã®åˆ—åã¯ã€Œæ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã€
                        email_address = row.get('æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', '').strip() if pd.notna(row.get('æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹')) else ''
                        if email_address == 'â€':
                            email_address = ''

                        # IDã®å®‰å…¨ãªå–å¾—
                        try:
                            company_id_raw = row.get('ID', len(companies) + 1)
                            company_id = int(company_id_raw)
                        except (ValueError, TypeError) as e:
                            logger.error(f"IDå¤‰æ›ã‚¨ãƒ©ãƒ¼: '{company_id_raw}' -> {e}")
                            continue
                        company_name = row.get('ä¼æ¥­å', '').strip() if pd.notna(row.get('ä¼æ¥­å')) else ''
                        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é‡è¦–ãƒ¢ãƒ¼ãƒ‰ã§ã¯ãƒ­ã‚°å‡ºåŠ›ã‚’æœ€å°åŒ–
                        if not PERFORMANCE_MODE and (idx < 5 or idx >= len(df) - 5):
                            logger.info(f"è¡Œ{idx+1}ã‚’å‡¦ç†ä¸­: ID={company_id}, ä¼æ¥­å={company_name}")

                        # HUGANJOBæŠ½å‡ºçµæœãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                        huganjob_result = huganjob_results.get(company_id, {})
                        final_email = huganjob_result.get('final_email', '')
                        email_source = huganjob_result.get('email_source', '')
                        extraction_status = huganjob_result.get('status', '')

                        # æœ€çµ‚çš„ãªãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æ±ºå®šï¼ˆHUGANJOBçµæœã‚’å„ªå…ˆï¼‰
                        if final_email:
                            effective_email = final_email
                            extraction_method = email_source
                            email_extracted = True
                            extraction_source = 'huganjob_extraction'
                            confidence = 0.9 if email_source == 'web_extraction' else 1.0
                        elif email_address:
                            effective_email = email_address
                            extraction_method = 'csv_import'
                            email_extracted = True
                            extraction_source = 'recruitment_csv'
                            confidence = 1.0
                        else:
                            effective_email = ''
                            extraction_method = None
                            email_extracted = False
                            extraction_source = None
                            confidence = None

                        # ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ã‚’CSVã‹ã‚‰èª­ã¿å–ã‚Šï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã®ã¿ï¼‰
                        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ—åã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                        csv_bounce_status = row.get('ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹', '').strip() if pd.notna(row.get('ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹')) else ''
                        csv_bounce_date = row.get('ãƒã‚¦ãƒ³ã‚¹æ—¥æ™‚', '').strip() if pd.notna(row.get('ãƒã‚¦ãƒ³ã‚¹æ—¥æ™‚')) else ''
                        csv_bounce_reason = row.get('ãƒã‚¦ãƒ³ã‚¹ç†ç”±', '').strip() if pd.notna(row.get('ãƒã‚¦ãƒ³ã‚¹ç†ç”±')) else ''

                        # æ–‡å­—åˆ—ã¨ã—ã¦å‡¦ç†
                        csv_bounce_status = str(csv_bounce_status).strip() if csv_bounce_status else ''
                        csv_bounce_date = str(csv_bounce_date).strip() if csv_bounce_date else ''
                        csv_bounce_reason = str(csv_bounce_reason).strip() if csv_bounce_reason else ''

                        # æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ã‚’ç¢ºèª
                        existing_bounce_status = None
                        existing_bounce_date = None
                        existing_bounce_reason = None
                        existing_is_bounced = False

                        if existing_cache and int(company_id) in existing_cache:
                            cached_data = existing_cache[int(company_id)]
                            existing_bounce_status = cached_data.get('bounce_status', '')
                            existing_bounce_date = cached_data.get('bounce_date', '')
                            existing_bounce_reason = cached_data.get('bounce_reason', '')
                            existing_is_bounced = cached_data.get('bounced', False)

                        # ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ã®å„ªå…ˆé †ä½: æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ > CSV
                        if existing_bounce_status:
                            # æ—¢å­˜ã®ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ã‚’å„ªå…ˆ
                            bounce_status = existing_bounce_status
                            bounce_date = existing_bounce_date
                            bounce_reason = existing_bounce_reason
                            is_bounced = existing_is_bounced
                            logger.info(f"ID {company_id}: æ—¢å­˜ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ã‚’ä¿æŒ - {bounce_status}")
                        else:
                            # CSVã®ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ã‚’ä½¿ç”¨
                            bounce_status = csv_bounce_status
                            bounce_date = csv_bounce_date
                            bounce_reason = csv_bounce_reason
                            is_bounced = bounce_status == 'permanent'
                            if is_bounced:
                                logger.info(f"ID {company_id}: CSVã‹ã‚‰ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ã‚’è¨­å®š - {bounce_status}")

                        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é‡è¦–ãƒ¢ãƒ¼ãƒ‰ã§ã¯ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ãƒ­ã‚°ã‚’æœ€å°åŒ–
                        if not PERFORMANCE_MODE and (idx < 3 or idx >= len(df) - 3 or is_bounced):
                            logger.info(f"ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯: ID={company_id}, ä¼æ¥­å={company_name}, "
                                      f"ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹='{bounce_status}', is_bounced={is_bounced}")
                            if is_bounced:
                                logger.info(f"ãƒã‚¦ãƒ³ã‚¹ä¼æ¥­æ¤œå‡º: ID={company_id}, ç†ç”±={bounce_reason}, æ—¥æ™‚={bounce_date}")

                        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ—åã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                        website = row.get('ä¼æ¥­ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸', '').strip() if pd.notna(row.get('ä¼æ¥­ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸')) else ''
                        job_position = row.get('å‹Ÿé›†è·ç¨®', '').strip() if pd.notna(row.get('å‹Ÿé›†è·ç¨®')) else ''

                        # æ–‡å­—åˆ—ã¨ã—ã¦å‡¦ç†
                        website = str(website).strip() if website else ''
                        job_position = str(job_position).strip() if job_position else ''

                        company = {
                            'id': str(company_id),  # IDã¯æ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
                            'name': company_name,
                            'website': website,
                            'recruitment_email': email_address,  # å…ƒã®CSVã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹
                            'job_position': job_position,
                            'industry': '',  # æ–°ã—ã„CSVã«ã¯å«ã¾ã‚Œã¦ã„ãªã„
                            'location': '',  # æ–°ã—ã„CSVã«ã¯å«ã¾ã‚Œã¦ã„ãªã„
                            # HUGANJOBå°‚ç”¨ã‚·ã‚¹ãƒ†ãƒ  - campaign_typeå‰Šé™¤æ¸ˆã¿
                            'email_extracted': email_extracted,
                            'email': effective_email,  # æœ€çµ‚çš„ãªãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆHUGANJOBçµæœã‚’å„ªå…ˆï¼‰
                            'extraction_method': extraction_method,
                            'confidence': confidence,
                            'email_confidence': confidence,
                            'smtp_verified': False,
                            'extraction_source': extraction_source,
                            'huganjob_status': extraction_status,  # HUGANJOBæŠ½å‡ºã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
                            'huganjob_email_source': email_source,  # HUGANJOBæŠ½å‡ºã‚½ãƒ¼ã‚¹
                            'email_sent': False,
                            'sent_date': None,
                            'email_subject': None,    # ãƒ¡ãƒ¼ãƒ«ä»¶å
                            'email_content': None,    # ãƒ¡ãƒ¼ãƒ«å†…å®¹
                            'bounced': is_bounced,    # CSVã®ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ã‚’æœ€å„ªå…ˆã§åæ˜ 
                            'bounce_reason': bounce_reason if bounce_reason else None,
                            'bounce_date': bounce_date if bounce_date else None,
                            'bounce_status': bounce_status if bounce_status else None,
                            'csv_bounce_status': bounce_status,  # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šå…ƒã®CSVå€¤ã‚’ä¿æŒ
                            'is_bounced': is_bounced,  # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”¨ï¼šç¢ºå®Ÿãªãƒã‚¦ãƒ³ã‚¹åˆ¤å®šãƒ•ãƒ©ã‚°
                            'unsubscribed': False,
                            'history': []             # å‡¦ç†å±¥æ­´
                        }
                        companies.append(company)
                        companies_added += 1
                        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é‡è¦–ãƒ¢ãƒ¼ãƒ‰ã§ã¯è©³ç´°ãƒ­ã‚°ã‚’å‰Šæ¸›
                        if not PERFORMANCE_MODE and (idx < 3 or idx >= len(df) - 3):
                            logger.info(f"ä¼æ¥­ã‚’è¿½åŠ ã—ã¾ã—ãŸ: ID={company_id}, ä¼æ¥­å={company_name}, ç¾åœ¨ã®ä¼æ¥­æ•°={len(companies)}")
                    logger.info(f"æ–°ã—ã„æ¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰{companies_added}ç¤¾ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆåˆè¨ˆ: {len(companies)}ç¤¾ï¼‰")
            except Exception as e:
                logger.error(f"æ–°ã—ã„æ¡ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                logger.error(f"è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±: {traceback.format_exc()}")

        # æ¨™æº–ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆæ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        elif os.path.exists(INPUT_FILE):
            # æœ€é©åŒ–ã•ã‚ŒãŸCSVèª­ã¿è¾¼ã¿ã‚’ä½¿ç”¨ï¼ˆè‹±èªã‚«ãƒ©ãƒ åã«å¯¾å¿œï¼‰
            df = load_csv_optimized(INPUT_FILE, columns=['company_name', 'website_url', 'industry', 'location'])

            if df is not None and not df.empty:
                for i, row in df.iterrows():
                    company = {
                        'id': str(i + 1),
                        'name': row.get('company_name', '').strip() if pd.notna(row.get('company_name')) else '',
                        'website': row.get('website_url', '').strip() if pd.notna(row.get('website_url')) else '',
                        'industry': row.get('industry', '').strip() if pd.notna(row.get('industry')) else '',
                        'location': row.get('location', '').strip() if pd.notna(row.get('location')) else '',
                        'recruitment_email': '',  # æ–°ã—ã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆç©ºï¼‰
                        'job_position': '',       # æ–°ã—ã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆç©ºï¼‰
                        # HUGANJOBå°‚ç”¨ã‚·ã‚¹ãƒ†ãƒ  - campaign_typeå‰Šé™¤æ¸ˆã¿
                        'email_extracted': False,
                        'email': None,
                        'extraction_method': None,
                        'confidence': None,
                        'email_confidence': None,  # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”¨
                        'smtp_verified': False,    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”¨
                        'extraction_source': None, # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”¨
                        'rank': None,
                        'score': None,
                        'ux_score': None,          # ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æç”¨
                        'design_score': None,      # ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æç”¨
                        'tech_score': None,        # ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æç”¨
                        'accessibility_score': None,    # ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢
                        'content_score': None,          # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªã‚¹ã‚³ã‚¢
                        'visual_hierarchy_score': None, # è¦–è¦šçš„éšå±¤ã‚¹ã‚³ã‚¢
                        'brand_consistency_score': None, # ãƒ–ãƒ©ãƒ³ãƒ‰ä¸€è²«æ€§ã‚¹ã‚³ã‚¢
                        'performance_score': None,      # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢
                        'security_score': None,         # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢
                        'email_sent': False,
                        'sent_date': None,
                        'email_subject': None,    # ãƒ¡ãƒ¼ãƒ«ä»¶å
                        'email_content': None,    # ãƒ¡ãƒ¼ãƒ«å†…å®¹
                        'bounced': False,
                        'unsubscribed': False,
                        'history': []             # å‡¦ç†å±¥æ­´
                    }
                    companies.append(company)
            else:
                logger.warning("æ¨™æº–ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")

        # åºƒå‘Šå–¶æ¥­ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚èª­ã¿è¾¼ã¿
        ad_input_file = 'data/derivative_ad_input.csv'
        if os.path.exists(ad_input_file):
            logger.info(f"åºƒå‘Šå–¶æ¥­ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {ad_input_file}")
            try:
                ad_df = pd.read_csv(ad_input_file, encoding='utf-8-sig')
                if ad_df is not None and not ad_df.empty:
                    for _, row in ad_df.iterrows():
                        company = {
                            'id': str(row.get('id', len(companies) + 1)),
                            'name': row.get('company_name', '').strip() if pd.notna(row.get('company_name')) else '',
                            'website': row.get('website_url', '').strip() if pd.notna(row.get('website_url')) else '',
                            'industry': row.get('industry', '').strip() if pd.notna(row.get('industry')) else '',
                            'location': row.get('location', '').strip() if pd.notna(row.get('location')) else '',
                            # HUGANJOBå°‚ç”¨ã‚·ã‚¹ãƒ†ãƒ  - campaign_typeå‰Šé™¤æ¸ˆã¿
                            'email_extracted': False,
                            'email': None,
                            'extraction_method': None,
                            'confidence': None,
                            'email_confidence': None,
                            'smtp_verified': False,
                            'extraction_source': None,
                            'rank': None,
                            'score': None,
                            'ux_score': None,
                            'design_score': None,
                            'tech_score': None,
                            'accessibility_score': None,
                            'content_score': None,
                            'visual_hierarchy_score': None,
                            'brand_consistency_score': None,
                            'performance_score': None,
                            'security_score': None,
                            'email_sent': False,
                            'sent_date': None,
                            'email_subject': None,
                            'email_content': None,
                            'bounced': False,
                            'unsubscribed': False,
                            'history': []
                        }
                        companies.append(company)
                    logger.info(f"åºƒå‘Šå–¶æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(ad_df)}ç¤¾")
                else:
                    logger.warning("åºƒå‘Šå–¶æ¥­ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            except Exception as e:
                logger.error(f"åºƒå‘Šå–¶æ¥­ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            logger.info("åºƒå‘Šå–¶æ¥­ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        # ãƒ‡ãƒ¼ã‚¿çµ±åˆå‡¦ç†ï¼ˆé€ä¿¡å±¥æ­´ã‚’ç¢ºå®Ÿã«è¡¨ç¤ºã™ã‚‹ãŸã‚å®Œå…¨ç‰ˆã‚’ä½¿ç”¨ï¼‰
        companies = integrate_email_extraction_results(companies)
        companies = integrate_email_sending_results(companies)
        companies = integrate_bounce_tracking_results(companies)

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é‡è¦–ãƒ¢ãƒ¼ãƒ‰ã§ã¯ä¸€éƒ¨ã®é‡ã„å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
        if not PERFORMANCE_MODE:
            companies = generate_email_content(companies)
            validate_data_integrity(companies)

        # æ—¢å­˜ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å¾©å…ƒï¼ˆé€ä¿¡ãƒ»ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ã®æœ€çµ‚ç¢ºèªï¼‰
        if existing_cache:
            restored_count = 0
            for company in companies:
                company_id = int(company['id'])
                if company_id in existing_cache:
                    cached_data = existing_cache[company_id]
                    # æ—¢å­˜ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æƒ…å ±ã‚’å¾©å…ƒï¼ˆæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ä¸Šæ›¸ãã—ãªã„ï¼‰
                    for key, value in cached_data.items():
                        # é€ä¿¡çŠ¶æ…‹ã¯æ—¢å­˜ã®å€¤ã‚’å„ªå…ˆï¼ˆé€ä¿¡æ¸ˆã¿ã®å ´åˆã¯ä¿æŒï¼‰
                        if key in ['email_sent', 'sent_date', 'email_subject', 'email_content']:
                            if value and not company.get(key):  # æ—¢å­˜ã®å€¤ãŒã‚ã‚Šã€æ–°ã—ã„å€¤ãŒãªã„å ´åˆã®ã¿å¾©å…ƒ
                                company[key] = value
                        # ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ã¯æœ€æ–°ã®ãƒã‚¦ãƒ³ã‚¹å‡¦ç†çµæœã‚’å„ªå…ˆï¼ˆdelivery_statusãŒã‚ã‚‹å ´åˆï¼‰
                        elif key in ['bounced', 'bounce_reason', 'bounce_date', 'bounce_status']:
                            if not company.get('delivery_status') and value:  # æœ€æ–°ã®ãƒã‚¦ãƒ³ã‚¹å‡¦ç†çµæœãŒãªã„å ´åˆã®ã¿å¾©å…ƒ
                                company[key] = value
                        # å±¥æ­´ã¯å¸¸ã«å¾©å…ƒ
                        elif key == 'history':
                            if value:
                                company[key] = value
                        elif key not in company or not company[key]:
                            # ãã®ä»–ã®æƒ…å ±ã¯æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã®ã¿å¾©å…ƒ
                            company[key] = value
                    restored_count += 1
            logger.info(f"{restored_count}ç¤¾ã®æ—¢å­˜ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æƒ…å ±ã‚’å¾©å…ƒã—ã¾ã—ãŸ")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°
        company_data_cache = companies
        company_data_last_updated = datetime.datetime.now()

        logger.info(f"{len(companies)}ç¤¾ã®ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        return companies

    except Exception as e:
        logger.error(f"ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []

def validate_data_integrity(companies):
    """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å•é¡ŒãŒã‚ã‚Œã°è­¦å‘Šã‚’å‡ºåŠ›"""
    total_companies = len(companies)
    companies_with_email = sum(1 for c in companies if c.get('email_extracted'))
    companies_with_analysis = sum(1 for c in companies if c.get('rank'))
    companies_with_sending = sum(1 for c in companies if c.get('email_sent'))

    logger.info(f"ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯:")
    logger.info(f"  ç·ä¼æ¥­æ•°: {total_companies}")
    logger.info(f"  ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºæ¸ˆã¿: {companies_with_email}")
    logger.info(f"  ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†ææ¸ˆã¿: {companies_with_analysis}")
    logger.info(f"  ãƒ¡ãƒ¼ãƒ«é€ä¿¡æ¸ˆã¿: {companies_with_sending}")

    # ID 1-5ã®ç‰¹åˆ¥ãƒã‚§ãƒƒã‚¯
    for company_id in ['1', '2', '3', '4', '5']:
        company = next((c for c in companies if c.get('id') == company_id), None)
        if company:
            logger.info(f"ID {company_id} ({company.get('name', 'N/A')}): "
                      f"ãƒ¡ãƒ¼ãƒ«={company.get('email_extracted', False)}, "
                      f"åˆ†æ={bool(company.get('rank'))}, "
                      f"é€ä¿¡={company.get('email_sent', False)}")
        else:
            logger.warning(f"ID {company_id} ã®ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

def get_max_company_id_from_file(file_path):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æœ€å¤§ã®ä¼æ¥­IDã‚’å–å¾—"""
    try:
        import pandas as pd
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        if 'ä¼æ¥­ID' in df.columns and not df.empty:
            return df['ä¼æ¥­ID'].max()
        return 0
    except Exception as e:
        logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ã‹ã‚‰æœ€å¤§IDã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")
        return 0

def get_min_company_id_from_file(file_path):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æœ€å°ã®ä¼æ¥­IDã‚’å–å¾—"""
    try:
        import pandas as pd
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        if 'ä¼æ¥­ID' in df.columns and not df.empty:
            return df['ä¼æ¥­ID'].min()
        return None
    except Exception as e:
        logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ã‹ã‚‰æœ€å°IDã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")
        return None

def integrate_email_extraction_results_light(companies):
    """ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºçµæœã‚’ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã«çµ±åˆï¼ˆè»½é‡ç‰ˆï¼‰"""
    try:
        # HUGANJOBå°‚ç”¨ã®æŠ½å‡ºçµæœã®ã¿å‡¦ç†
        huganjob_results_file = 'huganjob_email_resolution_results.csv'
        if not os.path.exists(huganjob_results_file):
            return companies

        # æœ€å°é™ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿èª­ã¿è¾¼ã¿
        df = pd.read_csv(huganjob_results_file, encoding='utf-8-sig',
                        usecols=['company_id', 'final_email', 'email_source', 'status'])

        if df.empty:
            return companies

        # ä¼æ¥­IDã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
        company_by_id = {c['id']: c for c in companies}

        # æŠ½å‡ºçµæœã‚’çµ±åˆï¼ˆæœ€å°é™ã®å‡¦ç†ï¼‰
        for _, row in df.iterrows():
            company_id = str(row.get('company_id', ''))
            if company_id in company_by_id:
                company = company_by_id[company_id]
                final_email = row.get('final_email', '')
                if final_email:
                    company['email_extracted'] = True
                    company['email'] = final_email
                    company['extraction_method'] = row.get('email_source', '')
                    company['confidence'] = 0.9

        return companies

    except Exception as e:
        logger.error(f"è»½é‡ç‰ˆãƒ¡ãƒ¼ãƒ«æŠ½å‡ºçµæœçµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
        return companies

def integrate_email_sending_results_light(companies):
    """ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœã‚’ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã«çµ±åˆï¼ˆè»½é‡ç‰ˆï¼‰"""
    try:
        # HUGANJOBé€ä¿¡å±¥æ­´ã®ã¿å‡¦ç†
        huganjob_history_file = 'huganjob_sending_history.json'
        if not os.path.exists(huganjob_history_file):
            return companies

        # ä¼æ¥­IDã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
        company_by_id = {c['id']: c for c in companies}

        # JSONå±¥æ­´ã‚’èª­ã¿è¾¼ã¿ï¼ˆæœ€å°é™ã®å‡¦ç†ï¼‰
        with open(huganjob_history_file, 'r', encoding='utf-8') as f:
            history_data = json.load(f)

        # é€ä¿¡çµæœã‚’çµ±åˆ
        for record in history_data:
            company_id = str(record.get('company_id', ''))
            if company_id in company_by_id:
                company = company_by_id[company_id]
                company['email_sent'] = True
                company['sent_date'] = record.get('sent_date', '')

        return companies

    except Exception as e:
        logger.error(f"è»½é‡ç‰ˆãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœçµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
        return companies

def integrate_bounce_tracking_results_light(companies):
    """ãƒã‚¦ãƒ³ã‚¹è¿½è·¡çµæœã‚’ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã«çµ±åˆï¼ˆè»½é‡ç‰ˆï¼‰"""
    try:
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ã®ã¿ä½¿ç”¨ï¼ˆæ—¢ã«èª­ã¿è¾¼ã¿æ¸ˆã¿ï¼‰
        # è¿½åŠ ã®å‡¦ç†ã¯ä¸è¦
        return companies

    except Exception as e:
        logger.error(f"è»½é‡ç‰ˆãƒã‚¦ãƒ³ã‚¹çµæœçµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
        return companies

def integrate_bounce_reports_to_daily_stats_light(daily_stats, start_date, end_date):
    """è»½é‡ç‰ˆæ—¥åˆ¥çµ±è¨ˆçµ±åˆï¼ˆåŸºæœ¬çš„ãªé›†è¨ˆã®ã¿ï¼‰"""
    try:
        # HUGANJOBé€ä¿¡å±¥æ­´ã‹ã‚‰åŸºæœ¬çµ±è¨ˆã‚’å–å¾—
        huganjob_history_file = 'huganjob_sending_history.json'
        if os.path.exists(huganjob_history_file):
            with open(huganjob_history_file, 'r', encoding='utf-8') as f:
                history_data = json.load(f)

            # ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ã„å½¢å¼ã‹ãƒã‚§ãƒƒã‚¯
            if not isinstance(history_data, list):
                logger.warning("HUGANJOBé€ä¿¡å±¥æ­´ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“")
                return daily_stats

            # æ—¥ä»˜åˆ¥ã«é›†è¨ˆ
            for record in history_data:
                if not isinstance(record, dict):
                    continue  # è¾æ›¸ã§ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—

                sent_date = record.get('sent_date', '')
                if sent_date and sent_date in daily_stats:
                    daily_stats[sent_date]['total'] += 1
                    if record.get('sent_result') == 'success':
                        daily_stats[sent_date]['success'] += 1
                    elif 'bounce' in str(record.get('error_message', '')).lower():
                        daily_stats[sent_date]['bounce'] += 1

        return daily_stats

    except Exception as e:
        logger.error(f"è»½é‡ç‰ˆæ—¥åˆ¥çµ±è¨ˆçµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
        return daily_stats

def get_lightweight_stats():
    """è»½é‡ç‰ˆçµ±è¨ˆæƒ…å ±ã‚’å–å¾—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰"""
    try:
        stats = {
            'total': 0,
            'email_extracted': 0,
            'analyzed': 0,
            'email_sent': 0,
            'bounced': 0,
            'success': 0,
            'pending': 0,
            'opened': 0,
            'open_rate': 0.0,
            'bounce_rate': 0.0,
            'success_rate': 0.0,
            'bounce_details': {},
            'integrity_summary': {}
        }

        # ä¼æ¥­ç·æ•°ã‚’é«˜é€Ÿå–å¾—
        if os.path.exists(INPUT_FILE):
            with open(INPUT_FILE, 'r', encoding='utf-8-sig') as f:
                stats['total'] = sum(1 for line in f) - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼é™¤ã

        # HUGANJOBæŠ½å‡ºçµæœã‹ã‚‰çµ±è¨ˆå–å¾—
        huganjob_results_file = 'huganjob_email_resolution_results.csv'
        if os.path.exists(huganjob_results_file):
            with open(huganjob_results_file, 'r', encoding='utf-8-sig') as f:
                stats['email_extracted'] = sum(1 for line in f) - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼é™¤ã

        # HUGANJOBé€ä¿¡å±¥æ­´ã‹ã‚‰çµ±è¨ˆå–å¾—
        huganjob_history_file = 'huganjob_sending_history.json'
        if os.path.exists(huganjob_history_file):
            try:
                with open(huganjob_history_file, 'r', encoding='utf-8') as f:
                    history_data = json.load(f)
                    stats['email_sent'] = len(history_data)
            except:
                stats['email_sent'] = 0

        # åŸºæœ¬çš„ãªè¨ˆç®—
        stats['pending'] = max(0, stats['email_extracted'] - stats['email_sent'])
        stats['success'] = stats['email_sent'] - stats['bounced']

        if stats['email_sent'] > 0:
            stats['bounce_rate'] = round((stats['bounced'] / stats['email_sent']) * 100, 2)
            stats['success_rate'] = round((stats['success'] / stats['email_sent']) * 100, 2)

        return stats

    except Exception as e:
        logger.error(f"è»½é‡ç‰ˆçµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'total': 0,
            'email_extracted': 0,
            'analyzed': 0,
            'email_sent': 0,
            'bounced': 0,
            'opened': 0,
            'open_rate': 0.0,
            'bounce_details': {}
        }

def get_total_companies_count():
    """ä¼æ¥­ç·æ•°ã‚’é«˜é€Ÿå–å¾—"""
    try:
        if os.path.exists(INPUT_FILE):
            with open(INPUT_FILE, 'r', encoding='utf-8-sig') as f:
                return sum(1 for line in f) - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼é™¤ã
        return 0
    except Exception as e:
        logger.error(f"ä¼æ¥­ç·æ•°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return 0

def load_company_data_paginated(page, per_page, filter_type='all', search_query=''):
    """ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œã®ä¼æ¥­ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆè»½é‡ç‰ˆï¼‰"""
    try:
        companies = []

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å¿…è¦ãªç¯„å›²ã®ã¿èª­ã¿è¾¼ã¿
        start_index = (page - 1) * per_page
        end_index = start_index + per_page

        if not os.path.exists(INPUT_FILE):
            return companies

        # HUGANJOBæŠ½å‡ºçµæœã‚’äº‹å‰èª­ã¿è¾¼ã¿
        huganjob_results = {}
        huganjob_results_file = 'huganjob_email_resolution_results.csv'
        if os.path.exists(huganjob_results_file):
            try:
                results_df = pd.read_csv(huganjob_results_file, encoding='utf-8-sig')
                for _, row in results_df.iterrows():
                    company_id = row.get('company_id')
                    if pd.notna(company_id):
                        huganjob_results[int(company_id)] = {
                            'final_email': row.get('final_email', '').strip() if pd.notna(row.get('final_email')) else '',
                            'email_source': row.get('email_source', '').strip() if pd.notna(row.get('email_source')) else ''
                        }
            except Exception as e:
                logger.warning(f"HUGANJOBæŠ½å‡ºçµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŒ‡å®šç¯„å›²ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        with open(INPUT_FILE, 'r', encoding='utf-8-sig') as f:
            reader = csv.reader(f)
            header = next(reader, None)  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—

            current_index = 0
            for row in reader:
                if current_index >= start_index and current_index < end_index:
                    if len(row) >= 5:  # æœ€å°é™ã®åˆ—æ•°ãƒã‚§ãƒƒã‚¯
                        company_id = int(row[0]) if row[0].isdigit() else current_index + 1
                        company_name = row[1].strip() if len(row) > 1 else ''
                        website = row[2].strip() if len(row) > 2 else ''
                        email_address = row[3].strip() if len(row) > 3 and row[3] != 'â€' else ''
                        job_position = row[4].strip() if len(row) > 4 else ''

                        # HUGANJOBæŠ½å‡ºçµæœãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                        huganjob_result = huganjob_results.get(company_id, {})
                        final_email = huganjob_result.get('final_email', '')

                        # æœ€çµ‚çš„ãªãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æ±ºå®š
                        effective_email = final_email if final_email else email_address

                        # é€ä¿¡çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯
                        email_sent = False
                        sent_date = None
                        if len(row) > 12 and row[12] == 'é€ä¿¡æ¸ˆã¿':
                            email_sent = True
                            if len(row) > 13:
                                sent_date = row[13]

                        # ãƒã‚¦ãƒ³ã‚¹çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯
                        is_bounced = False
                        if len(row) > 15 and row[15] and row[15].lower() in ['permanent', 'temporary', 'unknown']:
                            is_bounced = True
                            bounce_status = row[15]

                        company = {
                            'id': str(company_id),
                            'name': company_name,
                            'website': website,
                            'recruitment_email': email_address,
                            'job_position': job_position,
                            'email_extracted': bool(effective_email),
                            'email': effective_email,
                            'extraction_method': huganjob_result.get('email_source', 'csv_import' if email_address else ''),
                            'confidence': 0.9 if final_email else (1.0 if email_address else None),
                            'email_sent': email_sent,  # CSVã‹ã‚‰é€ä¿¡çŠ¶æ³ã‚’å–å¾—
                            'sent_date': sent_date,    # CSVã‹ã‚‰é€ä¿¡æ—¥æ™‚ã‚’å–å¾—
                            'bounced': is_bounced,     # CSVã‹ã‚‰ãƒã‚¦ãƒ³ã‚¹çŠ¶æ³ã‚’å–å¾—
                            'bounce_status': bounce_status,
                            'unsubscribed': False
                        }
                        companies.append(company)

                current_index += 1
                if current_index >= end_index:
                    break

        return companies

    except Exception as e:
        logger.error(f"ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ä¼æ¥­ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def integrate_email_extraction_results(companies):
    """ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºçµæœã‚’ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã«çµ±åˆ"""
    try:
        # æ–°ã—ã„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å°‚ç”¨ã®ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        import glob

        # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æœ€å„ªå…ˆã§ç¢ºèª
        latest_file = 'new_email_extraction_results_latest.csv'
        extraction_files = []

        # çµ¶å¯¾ãƒ‘ã‚¹ã§ç¢ºèª
        current_dir = os.getcwd()
        latest_file_path = os.path.join(current_dir, latest_file)

        logger.info(f"æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ä¸­: {latest_file_path}")

        # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆã—ã€å­˜åœ¨ã—ãªã„å ´åˆã¯IDç¯„å›²åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆ
        if os.path.exists(latest_file_path):
            extraction_files.append(latest_file_path)
            logger.info(f"âœ… æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {latest_file}")

            # è¿½åŠ : IDç¯„å›²åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ãƒã‚§ãƒƒã‚¯ã—ã¦ã€æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã«å«ã¾ã‚Œã¦ã„ãªã„ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°çµ±åˆ
            new_files = glob.glob('new_email_extraction_results_id*.csv')
            if new_files:
                logger.info(f"IDç¯„å›²åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç¢ºèª: {len(new_files)}å€‹è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€å¤§IDã‚’ç¢ºèª
                max_id_in_latest = get_max_company_id_from_file(latest_file_path)
                logger.info(f"æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€å¤§ä¼æ¥­ID: {max_id_in_latest}")

                # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚ˆã‚Šæ–°ã—ã„IDã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 
                for file_path in new_files:
                    min_id_in_file = get_min_company_id_from_file(file_path)
                    if min_id_in_file and min_id_in_file > max_id_in_latest:
                        extraction_files.append(file_path)
                        logger.info(f"âœ… è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆ: {os.path.basename(file_path)} (æœ€å°ID: {min_id_in_file})")
        else:
            # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯IDç¯„å›²åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆï¼ˆæ”¹è‰¯ç‰ˆã‚’å„ªå…ˆï¼‰
            improved_files = glob.glob('improved_email_extraction_results_id*.csv')
            new_files = glob.glob('new_email_extraction_results_id*.csv')

            logger.info(f"æ”¹è‰¯ç‰ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢: {len(improved_files)}å€‹è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            logger.info(f"å¾“æ¥ç‰ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢: {len(new_files)}å€‹è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

            if improved_files:
                extraction_files = improved_files
                logger.info(f"âœ… æ”¹è‰¯ç‰ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {[os.path.basename(f) for f in extraction_files]}")
            elif new_files:
                extraction_files = new_files
                logger.info(f"âœ… å¾“æ¥ç‰ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {[os.path.basename(f) for f in extraction_files]}")
            else:
                logger.warning("ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return companies

        if not extraction_files:
            logger.info("æ–°ã—ã„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ã®ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            logger.info("ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚ãƒ•ã‚¡ã‚¤ãƒ«åã¯ 'new_email_extraction_results_latest.csv' ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
            return companies

        # ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        logger.info(f"è¦‹ã¤ã‹ã£ãŸæ–°ã—ã„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºçµæœãƒ•ã‚¡ã‚¤ãƒ«: {len(extraction_files)}å€‹")
        logger.info(f"å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {[os.path.basename(f) for f in extraction_files]}")  # ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤º

        # ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        for file_path in extraction_files:
            file_size = os.path.getsize(file_path)
            file_mtime = os.path.getmtime(file_path)
            import datetime
            mtime_str = datetime.datetime.fromtimestamp(file_mtime).strftime('%Y-%m-%d %H:%M:%S')
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°: {os.path.basename(file_path)} (ã‚µã‚¤ã‚º: {file_size}ãƒã‚¤ãƒˆ, æ›´æ–°æ—¥æ™‚: {mtime_str})")

        # ä¼æ¥­åã¨IDã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
        company_by_name = {c['name']: c for c in companies}
        company_by_id = {c['id']: c for c in companies}

        # ãƒ‡ãƒãƒƒã‚°: ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã®çŠ¶æ³ã‚’ç¢ºèª
        logger.info(f"èª­ã¿è¾¼ã¿å¯¾è±¡ä¼æ¥­æ•°: {len(companies)}")
        logger.info(f"ä¼æ¥­IDã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {list(company_by_id.keys())[:10]}...")  # æœ€åˆã®10å€‹ã®IDã‚’è¡¨ç¤º

        matched_count = 0
        processed_rows = 0

        # ã™ã¹ã¦ã®ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        for file_path in extraction_files:
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­: {os.path.basename(file_path)}")

            try:
                with open(file_path, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    file_processed_rows = 0
                    file_matched_count = 0

                    for row in reader:
                        processed_rows += 1
                        file_processed_rows += 1
                        company_name = row.get('ä¼æ¥­å', '').strip()
                        company_id = row.get('ä¼æ¥­ID', '').strip()
                        email = row.get('ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', '').strip()
                        extraction_method = row.get('æŠ½å‡ºæ–¹æ³•', '').strip()
                        confidence = row.get('ä¿¡é ¼åº¦', '').strip()

                        # ãƒ‡ãƒãƒƒã‚°: ä¼æ¥­ID 1-5ã®å‡¦ç†çŠ¶æ³ã‚’è©³ã—ãè¡¨ç¤º
                        if company_id in ['1', '2', '3', '4', '5']:
                            logger.info(f"ä¼æ¥­ID {company_id} ã‚’å‡¦ç†ä¸­: åå‰={company_name}, ãƒ¡ãƒ¼ãƒ«={email}")

                        # ä¼æ¥­ã‚’ç‰¹å®šï¼ˆIDã¾ãŸã¯åå‰ã§ï¼‰
                        company = None
                        if company_id and company_id in company_by_id:
                            company = company_by_id[company_id]
                            if company_id in ['1', '2', '3', '4', '5']:
                                logger.info(f"ä¼æ¥­ID {company_id} ã§ãƒãƒƒãƒ: {company_name}")
                        elif company_name and company_name in company_by_name:
                            company = company_by_name[company_name]
                            if company_id in ['1', '2', '3', '4', '5']:
                                logger.info(f"ä¼æ¥­å {company_name} ã§ãƒãƒƒãƒ")

                        if company and email:
                            company['email_extracted'] = True
                            company['email'] = email
                            company['extraction_method'] = extraction_method
                            try:
                                # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ä½¿ç”¨ã•ã‚Œã‚‹åå‰ã«åˆã‚ã›ã‚‹
                                company['confidence'] = float(confidence) if confidence else 0.0
                                company['email_confidence'] = float(confidence) if confidence else 0.0
                            except ValueError:
                                company['confidence'] = 0.0
                                company['email_confidence'] = 0.0

                            # æ”¹è‰¯ç‰ˆãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯è¿½åŠ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‡¦ç†
                            extraction_steps = row.get('æŠ½å‡ºã‚¹ãƒ†ãƒƒãƒ—', '').strip()
                            source = row.get('æŠ½å‡ºæ–¹æ³•', '').strip()

                            if extraction_steps:
                                company['extraction_steps'] = extraction_steps
                            if source:
                                company['extraction_source'] = source

                            matched_count += 1
                            file_matched_count += 1
                            if company_id in ['1', '2', '3', '4', '5']:
                                logger.info(f"ä¼æ¥­ID {company_id} ã®ãƒ¡ãƒ¼ãƒ«æƒ…å ±ã‚’çµ±åˆ: {company_name} -> {email}")

                            # ãƒ‡ãƒãƒƒã‚°: ID 1-5ã®è©³ç´°ãƒ­ã‚°
                            if int(company_id) <= 5:
                                logger.info(f"[DEBUG] ID {company_id}: ä¼æ¥­å={company_name}, ãƒ¡ãƒ¼ãƒ«={email}, æŠ½å‡ºæ–¹æ³•={extraction_method}, ä¿¡é ¼åº¦={confidence}")
                                if extraction_steps:
                                    logger.info(f"[DEBUG] ID {company_id}: æŠ½å‡ºã‚¹ãƒ†ãƒƒãƒ—={extraction_steps}")

                    logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(file_path)}: {file_processed_rows}è¡Œå‡¦ç†, {file_matched_count}ç¤¾ãƒãƒƒãƒ")

            except Exception as e:
                logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        extracted_count = sum(1 for c in companies if c.get('email_extracted', False))
        logger.info(f"å‡¦ç†ã—ãŸè¡Œæ•°: {processed_rows}")
        logger.info(f"ãƒãƒƒãƒã—ãŸä¼æ¥­æ•°: {matched_count}")
        logger.info(f"ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºçµæœã‚’çµ±åˆã—ã¾ã—ãŸ: {extracted_count}ç¤¾")

    except Exception as e:
        logger.error(f"ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºçµæœã®çµ±åˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        logger.error(f"è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±: {traceback.format_exc()}")

    return companies

def integrate_website_analysis_results(companies):
    """ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æçµæœã‚’ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã«çµ±åˆ"""
    try:
        # æ–°ã—ã„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å°‚ç”¨ã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        import glob

        # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æœ€å„ªå…ˆã§ç¢ºèª
        latest_file = 'new_website_analysis_results_latest.csv'
        selected_file = None

        if os.path.exists(latest_file):
            selected_file = latest_file
            logger.info(f"æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {latest_file}")
        else:
            # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
            analysis_files = glob.glob('new_website_analysis_results_*.csv')

            if not analysis_files:
                logger.info("æ–°ã—ã„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã¾ã åˆ†æãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼‰")
                return companies

            # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ï¼ˆæ›´æ–°æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼‰
            selected_file = max(analysis_files, key=os.path.getmtime)
            logger.info(f"æ›´æ–°æ—¥æ™‚ãŒæœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {selected_file}")

        logger.info(f"ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {selected_file}")

        # ä¼æ¥­IDã¨ä¼æ¥­åã®ä¸¡æ–¹ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
        company_by_id = {c['id']: c for c in companies}
        company_by_name = {c['name']: c for c in companies}

        matched_count = 0
        processed_rows = 0

        with open(selected_file, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                processed_rows += 1
                company_id = row.get('ä¼æ¥­ID', '').strip()
                company_name = row.get('ä¼æ¥­å', '').strip()
                rank = row.get('ãƒ©ãƒ³ã‚¯', '').strip()
                score = row.get('ç·åˆã‚¹ã‚³ã‚¢', '').strip()

                # æ–°ã—ã„CSVãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ—åã«å¯¾å¿œï¼ˆå®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«åˆã‚ã›ã¦ä¿®æ­£ï¼‰
                ux_score = row.get('UXã‚¹ã‚³ã‚¢', '').strip()
                design_score = row.get('ãƒ‡ã‚¶ã‚¤ãƒ³ã‚¹ã‚³ã‚¢', '').strip()
                technical_score = row.get('æŠ€è¡“ã‚¹ã‚³ã‚¢', '').strip()

                # æ—§å½¢å¼ã¨ã®äº’æ›æ€§ã‚‚ä¿æŒ
                accessibility_score = row.get('ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£', '').strip()
                usability_score = row.get('ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£', '').strip()
                content_score = row.get('ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ª', '').strip()
                visual_hierarchy_score = row.get('è¦–è¦šçš„éšå±¤', '').strip()
                brand_consistency_score = row.get('ãƒ–ãƒ©ãƒ³ãƒ‰ä¸€è²«æ€§', '').strip()
                seo_score = row.get('SEOæœ€é©åŒ–', '').strip()
                performance_score = row.get('ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹', '').strip()
                security_score = row.get('ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£', '').strip()

                # åˆ†æçµæœã®è©³ç´°æƒ…å ±
                strengths = row.get('å¼·ã¿', '').strip()
                weaknesses = row.get('å¼±ã¿', '').strip()
                improvements = row.get('æ”¹å–„ææ¡ˆ', '').strip()

                # ä¼æ¥­IDã¾ãŸã¯ä¼æ¥­åã§ãƒãƒƒãƒãƒ³ã‚°
                company = None
                if company_id and company_id in company_by_id:
                    company = company_by_id[company_id]
                    matched_count += 1
                elif company_name and company_name in company_by_name:
                    company = company_by_name[company_name]
                    matched_count += 1

                if company:
                    company['rank'] = rank
                    try:
                        company['score'] = float(score) if score else 0.0
                    except ValueError:
                        company['score'] = 0.0

                    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆè©³ç´°ã‚¹ã‚³ã‚¢ã‹ã‚‰ï¼‰
                    try:
                        # UXã‚¹ã‚³ã‚¢ï¼ˆ30ç‚¹æº€ç‚¹ï¼‰= (ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ + ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£ + ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ª) / 3 * 0.3
                        if accessibility_score and usability_score and content_score:
                            ux_calc = (float(accessibility_score) + float(usability_score) + float(content_score)) / 3 * 0.3
                            company['ux_score'] = round(ux_calc, 1)
                            logger.debug(f"ä¼æ¥­ID {company_id}: UXã‚¹ã‚³ã‚¢è¨ˆç®— = ({accessibility_score} + {usability_score} + {content_score}) / 3 * 0.3 = {ux_calc:.1f}")
                        elif ux_score:
                            company['ux_score'] = float(ux_score)
                            logger.debug(f"ä¼æ¥­ID {company_id}: UXã‚¹ã‚³ã‚¢ï¼ˆæ—¢å­˜å€¤ä½¿ç”¨ï¼‰ = {ux_score}")
                        else:
                            company['ux_score'] = 0.0
                            logger.debug(f"ä¼æ¥­ID {company_id}: UXã‚¹ã‚³ã‚¢ = 0.0ï¼ˆãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰")
                    except (ValueError, TypeError) as e:
                        company['ux_score'] = 0.0
                        logger.warning(f"ä¼æ¥­ID {company_id}: UXã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼ = {e}")

                    try:
                        # ãƒ‡ã‚¶ã‚¤ãƒ³ã‚¹ã‚³ã‚¢ï¼ˆ40ç‚¹æº€ç‚¹ï¼‰= (ãƒ‡ã‚¶ã‚¤ãƒ³å“è³ª + è¦–è¦šçš„éšå±¤ + ãƒ–ãƒ©ãƒ³ãƒ‰ä¸€è²«æ€§) / 3 * 0.4
                        design_quality = row.get('ãƒ‡ã‚¶ã‚¤ãƒ³å“è³ª', '').strip()
                        if design_quality and visual_hierarchy_score and brand_consistency_score:
                            design_calc = (float(design_quality) + float(visual_hierarchy_score) + float(brand_consistency_score)) / 3 * 0.4
                            company['design_score'] = round(design_calc, 1)
                            logger.debug(f"ä¼æ¥­ID {company_id}: ãƒ‡ã‚¶ã‚¤ãƒ³ã‚¹ã‚³ã‚¢è¨ˆç®— = ({design_quality} + {visual_hierarchy_score} + {brand_consistency_score}) / 3 * 0.4 = {design_calc:.1f}")
                        elif design_score:
                            company['design_score'] = float(design_score)
                            logger.debug(f"ä¼æ¥­ID {company_id}: ãƒ‡ã‚¶ã‚¤ãƒ³ã‚¹ã‚³ã‚¢ï¼ˆæ—¢å­˜å€¤ä½¿ç”¨ï¼‰ = {design_score}")
                        else:
                            company['design_score'] = 0.0
                            logger.debug(f"ä¼æ¥­ID {company_id}: ãƒ‡ã‚¶ã‚¤ãƒ³ã‚¹ã‚³ã‚¢ = 0.0ï¼ˆãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰")
                    except (ValueError, TypeError) as e:
                        company['design_score'] = 0.0
                        logger.warning(f"ä¼æ¥­ID {company_id}: ãƒ‡ã‚¶ã‚¤ãƒ³ã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼ = {e}")

                    try:
                        # æŠ€è¡“ã‚¹ã‚³ã‚¢ï¼ˆ30ç‚¹æº€ç‚¹ï¼‰= (SEOæœ€é©åŒ– + ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ + ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£) / 3 * 0.3
                        if seo_score and performance_score and security_score:
                            tech_calc = (float(seo_score) + float(performance_score) + float(security_score)) / 3 * 0.3
                            company['tech_score'] = round(tech_calc, 1)
                            logger.debug(f"ä¼æ¥­ID {company_id}: æŠ€è¡“ã‚¹ã‚³ã‚¢è¨ˆç®— = ({seo_score} + {performance_score} + {security_score}) / 3 * 0.3 = {tech_calc:.1f}")
                        elif technical_score:
                            company['tech_score'] = float(technical_score)
                            logger.debug(f"ä¼æ¥­ID {company_id}: æŠ€è¡“ã‚¹ã‚³ã‚¢ï¼ˆæ—¢å­˜å€¤ä½¿ç”¨ï¼‰ = {technical_score}")
                        else:
                            company['tech_score'] = 0.0
                            logger.debug(f"ä¼æ¥­ID {company_id}: æŠ€è¡“ã‚¹ã‚³ã‚¢ = 0.0ï¼ˆãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰")
                    except (ValueError, TypeError) as e:
                        company['tech_score'] = 0.0
                        logger.warning(f"ä¼æ¥­ID {company_id}: æŠ€è¡“ã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼ = {e}")

                    # æ—§å½¢å¼ã¨ã®äº’æ›æ€§ã®ãŸã‚è©³ç´°ã‚¹ã‚³ã‚¢ã‚‚çµ±åˆ
                    try:
                        company['accessibility_score'] = float(accessibility_score) if accessibility_score else 0.0
                    except ValueError:
                        company['accessibility_score'] = 0.0

                    try:
                        company['usability_score'] = float(usability_score) if usability_score else 0.0
                    except ValueError:
                        company['usability_score'] = 0.0

                    try:
                        company['content_score'] = float(content_score) if content_score else 0.0
                    except ValueError:
                        company['content_score'] = 0.0

                    try:
                        company['visual_hierarchy_score'] = float(visual_hierarchy_score) if visual_hierarchy_score else 0.0
                    except ValueError:
                        company['visual_hierarchy_score'] = 0.0

                    try:
                        company['brand_consistency_score'] = float(brand_consistency_score) if brand_consistency_score else 0.0
                    except ValueError:
                        company['brand_consistency_score'] = 0.0

                    try:
                        company['seo_score'] = float(seo_score) if seo_score else 0.0
                    except ValueError:
                        company['seo_score'] = 0.0

                    try:
                        company['performance_score'] = float(performance_score) if performance_score else 0.0
                    except ValueError:
                        company['performance_score'] = 0.0

                    try:
                        company['security_score'] = float(security_score) if security_score else 0.0
                    except ValueError:
                        company['security_score'] = 0.0

                    # åˆ†æçµæœã®è©³ç´°æƒ…å ±
                    company['analysis_strengths'] = strengths
                    company['analysis_weaknesses'] = weaknesses
                    company['analysis_improvements'] = improvements

                    logger.debug(f"ä¼æ¥­ID {company_id} ({company_name}) ã®åˆ†æçµæœã‚’çµ±åˆ: ãƒ©ãƒ³ã‚¯ {rank}, ã‚¹ã‚³ã‚¢ {score}")

                    # ãƒ‡ãƒãƒƒã‚°: ID 1-5ã®è©³ç´°ãƒ­ã‚°
                    if int(company_id) <= 5:
                        logger.info(f"[DEBUG] ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æ ID {company_id}: ä¼æ¥­å={company_name}, ãƒ©ãƒ³ã‚¯={rank}, ã‚¹ã‚³ã‚¢={score}, UX={ux_score}, ãƒ‡ã‚¶ã‚¤ãƒ³={design_score}, æŠ€è¡“={technical_score}")
                else:
                    logger.debug(f"ãƒãƒƒãƒã—ãªã„ä¼æ¥­: ID={company_id}, åå‰={company_name}")

                    # ãƒ‡ãƒãƒƒã‚°: ID 1-5ã®ãƒãƒƒãƒã—ãªã„å ´åˆã‚‚ãƒ­ã‚°
                    if int(company_id) <= 5:
                        logger.warning(f"[DEBUG] ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æã§ãƒãƒƒãƒã—ãªã„ ID {company_id}: ä¼æ¥­å={company_name}")

        analyzed_count = sum(1 for c in companies if c['rank'])
        logger.info(f"å‡¦ç†ã—ãŸè¡Œæ•°: {processed_rows}")
        logger.info(f"ãƒãƒƒãƒã—ãŸä¼æ¥­æ•°: {matched_count}")
        logger.info(f"ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æçµæœã‚’çµ±åˆã—ã¾ã—ãŸ: {analyzed_count}ç¤¾")

    except Exception as e:
        logger.error(f"ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æçµæœã®çµ±åˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    return companies

def integrate_email_sending_results(companies):
    """ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœã‚’ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã«çµ±åˆï¼ˆHUGANJOBçµ±åˆã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œï¼‰"""
    try:
        # ä¼æ¥­IDã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
        company_by_id = {c['id']: c for c in companies}

        # 1. HUGANJOBçµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®é€ä¿¡å±¥æ­´JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æœ€å„ªå…ˆã§ç¢ºèª
        huganjob_history_file = 'huganjob_sending_history.json'
        huganjob_processed = 0

        if os.path.exists(huganjob_history_file):
            logger.info(f"HUGANJOBçµ±åˆã‚·ã‚¹ãƒ†ãƒ é€ä¿¡å±¥æ­´ã‚’èª­ã¿è¾¼ã¿ä¸­: {huganjob_history_file}")
            try:
                with open(huganjob_history_file, 'r', encoding='utf-8') as f:
                    history_data = json.load(f)

                sending_records = history_data.get('sending_records', [])
                logger.info(f"HUGANJOBé€ä¿¡å±¥æ­´: {len(sending_records)}ä»¶ã®è¨˜éŒ²ã‚’ç™ºè¦‹")

                for record in sending_records:
                    company_id = str(record.get('company_id', ''))
                    company_name = record.get('company_name', '')
                    email_address = record.get('email_address', '')
                    send_time = record.get('send_time', '')
                    script_name = record.get('script_name', '')

                    if company_id in company_by_id:
                        company = company_by_id[company_id]
                        company['email_sent'] = True
                        company['sent_date'] = send_time
                        company['email_subject'] = f"ã€{company.get('job_position', 'äººæ')}ã®äººææ¡ç”¨ã‚’å¼·åŒ–ã—ã¾ã›ã‚“ã‹ï¼Ÿã€‘æ ªå¼ä¼šç¤¾HUGANã‹ã‚‰ã®ã”ææ¡ˆ"
                        company['final_status'] = 'é…ä¿¡æˆåŠŸ'
                        company['sending_system'] = 'huganjob_unified'
                        company['script_name'] = script_name

                        # é€ä¿¡å±¥æ­´ã«è¨˜éŒ²ã‚’è¿½åŠ 
                        if 'history' not in company:
                            company['history'] = []

                        company['history'].append({
                            'action': 'email_sent',
                            'timestamp': send_time,
                            'details': f'HUGANJOBçµ±åˆã‚·ã‚¹ãƒ†ãƒ ã§é€ä¿¡: {email_address}',
                            'system': 'huganjob_unified'
                        })

                        huganjob_processed += 1
                        logger.info(f"HUGANJOBé€ä¿¡è¨˜éŒ²ã‚’çµ±åˆ: ID {company_id} - {company_name}")

                logger.info(f"HUGANJOBçµ±åˆã‚·ã‚¹ãƒ†ãƒ é€ä¿¡çµæœã‚’çµ±åˆ: {huganjob_processed}ç¤¾")

            except Exception as e:
                logger.error(f"HUGANJOBé€ä¿¡å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        # 2. å¾“æ¥ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç¢ºèªï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        import glob

        # æœ€å„ªå…ˆã§ new_email_sending_results.csv ã‚’ç¢ºèª
        primary_file = 'new_email_sending_results.csv'
        selected_file = None

        if os.path.exists(primary_file):
            selected_file = primary_file
            logger.info(f"ãƒ¡ã‚¤ãƒ³é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {primary_file}")
        else:
            # IDç¯„å›²åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            sending_files = glob.glob('sent_emails_record_id*.csv')
            if sending_files:
                # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
                selected_file = max(sending_files, key=os.path.getmtime)
                logger.info(f"æœ€æ–°ã®ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {selected_file}")
            elif os.path.exists(NEW_EMAIL_SENDING_RESULTS):
                selected_file = NEW_EMAIL_SENDING_RESULTS
                logger.info(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {NEW_EMAIL_SENDING_RESULTS}")

        csv_processed = 0
        if selected_file:
            logger.info(f"å¾“æ¥ã®CSVé€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {selected_file}")
            try:
                with open(selected_file, 'r', encoding='utf-8-sig') as f:
                    reader = csv.reader(f)
                    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’èª­ã¿å–ã‚Šã€æ§‹é€ ã‚’æ¤œè¨¼
                    header = next(reader, None)

                    # CSVãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã®æ¤œè¨¼ã¨ãƒ­ã‚°å‡ºåŠ›
                    if header:
                        logger.info(f"CSVãƒ˜ãƒƒãƒ€ãƒ¼æ§‹é€ : {header}")
                        logger.info(f"CSVãƒ˜ãƒƒãƒ€ãƒ¼åˆ—æ•°: {len(header)}")
                        expected_columns = ['ä¼æ¥­ID', 'ä¼æ¥­å', 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'å‹Ÿé›†è·ç¨®', 'ãƒ¡ãƒ¼ãƒ«ç”¨è·ç¨®', 'é€ä¿¡æ—¥æ™‚', 'é€ä¿¡çµæœ', 'ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ID', 'ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸', 'ä»¶å']
                        if len(header) != len(expected_columns):
                            logger.warning(f"CSVãƒ˜ãƒƒãƒ€ãƒ¼åˆ—æ•°ä¸ä¸€è‡´: æœŸå¾…={len(expected_columns)}, å®Ÿéš›={len(header)}")
                            logger.warning(f"æœŸå¾…ã•ã‚Œã‚‹åˆ—: {expected_columns}")
                            logger.warning(f"å®Ÿéš›ã®åˆ—: {header}")
                            # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«åˆã‚ã›ã¦å‡¦ç†ã‚’èª¿æ•´
                            logger.info("å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«åˆã‚ã›ã¦å‡¦ç†ã‚’ç¶™ç¶šã—ã¾ã™")

                    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å®Ÿéš›ã®æ§‹é€ ã«åŸºã¥ã„ã¦èª­ã¿è¾¼ã¿
                    # å‹•çš„ã«åˆ—æ§‹é€ ã‚’åˆ¤å®šï¼ˆä¸€éƒ¨ã®è¡Œã«ã€Œãƒ¡ãƒ¼ãƒ«ç”¨è·ç¨®ã€åˆ—ãŒå­˜åœ¨ã™ã‚‹ä¸æ•´åˆã«å¯¾å¿œï¼‰
                    for row_data in reader:
                        # è¡Œãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã¨ã—ã¦å–å¾—
                        if len(row_data) < 7:  # æœ€ä½é™å¿…è¦ãªåˆ—æ•°ã‚’ãƒã‚§ãƒƒã‚¯
                            continue

                        # åˆ—æ•°ã«åŸºã¥ã„ã¦å‹•çš„ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ±ºå®š
                        if len(row_data) == 10:  # ãƒ¡ãƒ¼ãƒ«ç”¨è·ç¨®åˆ—ãŒå­˜åœ¨ã™ã‚‹å ´åˆï¼ˆID 1-5ãªã©ï¼‰
                            # åˆ—é †åº: ä¼æ¥­ID,ä¼æ¥­å,ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹,å‹Ÿé›†è·ç¨®,ãƒ¡ãƒ¼ãƒ«ç”¨è·ç¨®,é€ä¿¡æ—¥æ™‚,é€ä¿¡çµæœ,ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ID,ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸,ä»¶å
                            company_id = row_data[0].strip() if len(row_data) > 0 else ''
                            company_name = row_data[1].strip() if len(row_data) > 1 else ''
                            email_address = row_data[2].strip() if len(row_data) > 2 else ''
                            job_position = row_data[3].strip() if len(row_data) > 3 else ''
                            # row_data[4] ã¯ãƒ¡ãƒ¼ãƒ«ç”¨è·ç¨®ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰
                            sent_date = row_data[5].strip() if len(row_data) > 5 else ''
                            sent_result = row_data[6].strip() if len(row_data) > 6 else ''
                            tracking_id = row_data[7].strip() if len(row_data) > 7 else ''
                            error_message = row_data[8].strip() if len(row_data) > 8 else ''
                            subject = row_data[9].strip() if len(row_data) > 9 else ''
                            logger.debug(f"10åˆ—å½¢å¼ã§å‡¦ç†: ID={company_id}")
                        else:  # ãƒ¡ãƒ¼ãƒ«ç”¨è·ç¨®åˆ—ãŒå­˜åœ¨ã—ãªã„å ´åˆï¼ˆID 6ä»¥é™ãªã©ï¼‰
                            # åˆ—é †åº: ä¼æ¥­ID,ä¼æ¥­å,ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹,å‹Ÿé›†è·ç¨®,é€ä¿¡æ—¥æ™‚,é€ä¿¡çµæœ,ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ID,ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸,ä»¶å
                            company_id = row_data[0].strip() if len(row_data) > 0 else ''
                            company_name = row_data[1].strip() if len(row_data) > 1 else ''
                            email_address = row_data[2].strip() if len(row_data) > 2 else ''
                            job_position = row_data[3].strip() if len(row_data) > 3 else ''
                            sent_date = row_data[4].strip() if len(row_data) > 4 else ''
                            sent_result = row_data[5].strip() if len(row_data) > 5 else ''
                            tracking_id = row_data[6].strip() if len(row_data) > 6 else ''
                            error_message = row_data[7].strip() if len(row_data) > 7 else ''
                            subject = row_data[8].strip() if len(row_data) > 8 else ''
                            logger.debug(f"9åˆ—å½¢å¼ã§å‡¦ç†: ID={company_id}")

                        # ãƒã‚¦ãƒ³ã‚¹é–¢é€£ã®æƒ…å ±ã¯ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰åˆ¤å®š
                        bounce_status = ''
                        bounce_reason = ''
                        bounce_date = ''
                        final_status = ''

                        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚‹å ´åˆã¯ãƒã‚¦ãƒ³ã‚¹ã¨ã—ã¦æ‰±ã†
                        if error_message and error_message != '':
                            bounce_status = 'permanent'
                            bounce_reason = error_message
                            bounce_date = sent_date
                            final_status = 'ãƒã‚¦ãƒ³ã‚¹'
                        elif sent_result == 'success':
                            final_status = 'é…ä¿¡æˆåŠŸ'
                        elif sent_result == 'bounced':
                            bounce_status = 'permanent'
                            bounce_reason = 'ãƒã‚¦ãƒ³ã‚¹æ¤œå‡º'
                            bounce_date = sent_date
                            final_status = 'ãƒã‚¦ãƒ³ã‚¹'
                        elif sent_result == 'skipped':
                            final_status = 'ã‚¹ã‚­ãƒƒãƒ—'

                        # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                        if not company_id:
                            logger.warning(f"ä¼æ¥­IDãŒç©ºã®è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—: {row_data}")
                            continue

                        if not sent_result:
                            logger.warning(f"é€ä¿¡çµæœãŒç©ºã®è¡Œ: ID={company_id}")

                        logger.info(f"CSVè¡Œå‡¦ç†: ID={company_id}, é€ä¿¡çµæœ={sent_result}, ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ID={tracking_id}, ã‚¨ãƒ©ãƒ¼={error_message}")

                        if company_id in company_by_id:
                            company = company_by_id[company_id]

                            # ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDã‚’å¸¸ã«æ›´æ–°ï¼ˆé–‹å°çŠ¶æ³è¿½è·¡ã®ãŸã‚ï¼‰
                            if tracking_id:
                                company['tracking_id'] = tracking_id
                                logger.info(f"ä¼æ¥­ID {company_id} ã«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDè¨­å®š: {tracking_id}")

                            # é€ä¿¡çµæœã®çµ±åˆå‡¦ç†ã‚’æ”¹å–„
                            # CSVã«é€ä¿¡è¨˜éŒ²ãŒã‚ã‚‹å ´åˆã¯ã€HUGANJOBã‚·ã‚¹ãƒ†ãƒ ã®è¨˜éŒ²ãŒãªãã¦ã‚‚é€ä¿¡æ¸ˆã¿ã¨ã—ã¦æ‰±ã†
                            if sent_result == 'success':
                                # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒæœªç™»éŒ²ã®å ´åˆã¯é€ä¿¡æ¸ˆã¿ã¨ã—ãªã„ï¼ˆIDæ¬ ç•ªä¿®æ­£æ™‚ã®ä¸æ•´åˆå¯¾ç­–ï¼‰
                                company_email = company.get('email', '').strip()
                                if company_email and company_email != 'â€' and company_email != '-':
                                    # é€ä¿¡æˆåŠŸã®å ´åˆã¯å¿…ãšé€ä¿¡æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯
                                    company['email_sent'] = True
                                    company['sent_date'] = sent_date

                                    # é€ä¿¡ã‚·ã‚¹ãƒ†ãƒ ã®æƒ…å ±ã‚’è¨­å®šï¼ˆHUGANJOBçµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®è¨˜éŒ²ãŒãªã„å ´åˆï¼‰
                                    if not company.get('sending_system'):
                                        company['sending_system'] = 'legacy_csv'
                                else:
                                    # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æœªç™»éŒ²ã®å ´åˆã¯ãƒ­ã‚°å‡ºåŠ›
                                    logger.warning(f"ä¼æ¥­ID {company_id}: ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æœªç™»éŒ²ã®ãŸã‚é€ä¿¡æ¸ˆã¿ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ— ({company_email})")
                                    logger.info(f"ä¼æ¥­ID {company_id}: CSVé€ä¿¡çµæœã‹ã‚‰é€ä¿¡æ¸ˆã¿çŠ¶æ…‹ã‚’è¨­å®š")

                                # ãƒã‚¦ãƒ³ã‚¹æƒ…å ±ã‚’è¿½åŠ ï¼ˆæ—¢å­˜ã®ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ãŒãªã„å ´åˆã®ã¿ï¼‰
                                if not company.get('bounced') and not company.get('bounce_status'):
                                    company['bounced'] = (bounce_status == 'permanent')  # CSVã§ã¯'permanent'ã¨ã—ã¦è¨˜éŒ²
                                    company['bounce_reason'] = bounce_reason if bounce_reason else None
                                    company['bounce_date'] = bounce_date if bounce_date else None
                                    company['bounce_status'] = bounce_status if bounce_status else None
                                    logger.info(f"ä¼æ¥­ID {company_id}: CSVé€ä¿¡çµæœã‹ã‚‰ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ã‚’è¨­å®š - {bounce_status}")
                                else:
                                    logger.info(f"ä¼æ¥­ID {company_id}: æ—¢å­˜ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ã‚’ä¿æŒ")

                                company['final_status'] = final_status if final_status else 'é…ä¿¡æˆåŠŸ'

                            csv_processed += 1

                logger.info(f"å¾“æ¥ã®CSVé€ä¿¡çµæœã‚’çµ±åˆ: {csv_processed}ç¤¾")

            except Exception as e:
                logger.error(f"CSVé€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        total_sent = sum(1 for c in companies if c.get('email_sent', False))
        total_bounced = sum(1 for c in companies if c.get('bounced', False))
        total_success = sum(1 for c in companies if c.get('email_sent', False) and not c.get('bounced', False))

        logger.info(f"ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœçµ±åˆå®Œäº†: ç·é€ä¿¡æ¸ˆã¿ {total_sent}ç¤¾ (HUGANJOB: {huganjob_processed}ç¤¾, CSV: {csv_processed}ç¤¾)")
        logger.info(f"é€ä¿¡çµæœè©³ç´°: é…ä¿¡æˆåŠŸ {total_success}ç¤¾, ãƒã‚¦ãƒ³ã‚¹ {total_bounced}ç¤¾")

        # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯
        inconsistent_count = 0
        for company in companies:
            if company.get('email_sent', False) and not company.get('sent_date'):
                logger.warning(f"ä¼æ¥­ID {company.get('id')}: é€ä¿¡æ¸ˆã¿ã ãŒé€ä¿¡æ—¥æ™‚ãŒæœªè¨­å®š")
                inconsistent_count += 1

        if inconsistent_count > 0:
            logger.warning(f"ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®å•é¡Œ: {inconsistent_count}ç¤¾ã§ä¸æ•´åˆã‚’æ¤œå‡º")

    except Exception as e:
        logger.error(f"ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœã®çµ±åˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    return companies

def integrate_bounce_tracking_results(companies):
    """ãƒã‚¦ãƒ³ã‚¹è¿½è·¡çµæœã‚’ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã«çµ±åˆ"""
    try:
        # åŒ…æ‹¬çš„ãƒã‚¦ãƒ³ã‚¹æ¤œå‡ºçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆçš„ã«ç¢ºèª
        comprehensive_bounce_file = 'comprehensive_bounce_tracking_results.csv'
        selected_file = None

        if os.path.exists(comprehensive_bounce_file):
            selected_file = comprehensive_bounce_file
            logger.info(f"åŒ…æ‹¬çš„ãƒã‚¦ãƒ³ã‚¹æ¤œå‡ºçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {comprehensive_bounce_file}")
        elif os.path.exists(NEW_BOUNCE_TRACKING):
            selected_file = NEW_BOUNCE_TRACKING
            logger.info(f"æ¨™æº–ãƒã‚¦ãƒ³ã‚¹è¿½è·¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {NEW_BOUNCE_TRACKING}")
        else:
            logger.info("ãƒã‚¦ãƒ³ã‚¹è¿½è·¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã¾ã ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼‰")
            return companies

        logger.info(f"ãƒã‚¦ãƒ³ã‚¹è¿½è·¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {selected_file}")

        # ä¼æ¥­IDã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
        company_by_id = {c['id']: c for c in companies}
        bounce_count = 0

        with open(selected_file, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                company_id = row.get('ä¼æ¥­ID', '').strip()
                status = row.get('ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹', '').strip()
                bounce_reason = row.get('ãƒã‚¦ãƒ³ã‚¹ç†ç”±', '').strip()
                bounce_date = row.get('ãƒã‚¦ãƒ³ã‚¹æ—¥æ™‚', '').strip()
                bounce_type = row.get('ãƒã‚¦ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—', '').strip()

                if company_id in company_by_id:
                    company = company_by_id[company_id]
                    # ãƒã‚¦ãƒ³ã‚¹è¿½è·¡çµæœã¯æœ€æ–°ã®æƒ…å ±ã¨ã—ã¦å„ªå…ˆçš„ã«é©ç”¨
                    company['bounced'] = (status == 'bounced')
                    company['bounce_reason'] = bounce_reason
                    company['bounce_date'] = bounce_date
                    company['bounce_type'] = bounce_type
                    company['delivery_status'] = status
                    bounce_count += 1
                    logger.info(f"ä¼æ¥­ID {company_id}: ãƒã‚¦ãƒ³ã‚¹è¿½è·¡çµæœã‚’æ›´æ–° - {status}")

        logger.info(f"ãƒã‚¦ãƒ³ã‚¹è¿½è·¡çµæœçµ±åˆå®Œäº†: {bounce_count}ä»¶ã®ãƒã‚¦ãƒ³ã‚¹æƒ…å ±ã‚’çµ±åˆã—ã¾ã—ãŸ")

        bounced_count = sum(1 for c in companies if c.get('bounced', False))
        delivered_count = sum(1 for c in companies if c.get('delivery_status') == 'delivered')
        logger.info(f"ãƒã‚¦ãƒ³ã‚¹è¿½è·¡çµæœã‚’çµ±åˆã—ã¾ã—ãŸ: é…ä¿¡æˆåŠŸ {delivered_count}ç¤¾, ãƒã‚¦ãƒ³ã‚¹ {bounced_count}ç¤¾")

    except Exception as e:
        logger.error(f"ãƒã‚¦ãƒ³ã‚¹è¿½è·¡çµæœã®çµ±åˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    return companies

def load_email_template(rank):
    """ãƒ©ãƒ³ã‚¯ã«å¿œã˜ãŸãƒ¡ãƒ¼ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    template_file = None
    if rank == 'A':
        template_file = 'email_generation/templates/email-a.txt'
    elif rank == 'B':
        template_file = 'email_generation/templates/email-b.txt'
    else:  # rank == 'C'
        template_file = 'email_generation/templates/email-c.txt'

    if template_file and os.path.exists(template_file):
        try:
            with open(template_file, 'r', encoding='utf-8-sig') as f:
                return f.read()
        except Exception as e:
            logger.error(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ« {template_file} ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    return None

def generate_email_content(companies):
    """ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ãƒ¡ãƒ¼ãƒ«å†…å®¹ã‚’ç”Ÿæˆï¼ˆHUGAN JOBæ¡ç”¨ãƒ¡ãƒ¼ãƒ«ç”¨ï¼‰"""
    try:
        for company in companies:
            # ä¼æ¥­åã¨å‹Ÿé›†è·ç¨®ã‚’å–å¾—
            company_name = company.get('name', 'ä¼æ¥­åä¸æ˜')
            job_position = company.get('job_position', 'äººæ')

            # ãƒ¡ãƒ¼ãƒ«ä»¶åã‚’è¨­å®šï¼ˆå‹Ÿé›†è·ç¨®ã‚’å«ã‚€ï¼‰
            if not company.get('email_subject'):
                company['email_subject'] = f'ã€{job_position}ã®äººææ¡ç”¨ã‚’å¼·åŒ–ã—ã¾ã›ã‚“ã‹ï¼Ÿã€‘æ ªå¼ä¼šç¤¾HUGANã‹ã‚‰ã®ã”ææ¡ˆ'

                # ãƒ¡ãƒ¼ãƒ«å†…å®¹ã‚’è¨­å®šï¼ˆHUGAN JOBæ¡ç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ï¼‰
                if not company.get('email_content'):
                    # corporate-email-newsletter.htmlãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
                    template_file = 'corporate-email-newsletter.html'
                    if os.path.exists(template_file):
                        try:
                            with open(template_file, 'r', encoding='utf-8') as f:
                                template = f.read()

                            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå†…ã®å¤‰æ•°ã‚’ç½®æ›
                            email_content = template.replace('{{company_name}}', company_name)
                            email_content = email_content.replace('{{job_position}}', job_position)

                            # ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ”ã‚¯ã‚»ãƒ«ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’è¿½åŠ ï¼ˆå®Ÿéš›ã®é€ä¿¡æ™‚ã«ç½®æ›ã•ã‚Œã‚‹ï¼‰
                            email_content = email_content.replace('{{tracking_pixel}}', '')

                            company['email_content'] = email_content
                        except Exception as e:
                            logger.error(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ« {template_file} ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                            company['email_content'] = None
                    else:
                        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒèª­ã¿è¾¼ã‚ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        company['email_content'] = f'''
<html>
<head>
    <meta charset="UTF-8">
    <style>
        body {{ font-family: Arial, sans-serif; line-height: 1.6; color: #333; }}
        .header {{ background-color: #3498db; color: white; padding: 20px; text-align: center; }}
        .content {{ padding: 20px; }}
        .footer {{ background-color: #f8f9fa; padding: 20px; text-align: center; font-size: 12px; color: #777; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>HUGAN JOB</h1>
        <p>äººæç´¹ä»‹ã‚µãƒ¼ãƒ“ã‚¹</p>
    </div>
    <div class="content">
        <p>{company_name} æ§˜</p>

        <p>åˆã‚ã¦ã”é€£çµ¡ã„ãŸã—ã¾ã™ã€‚<br>
        æ ªå¼ä¼šç¤¾HUGANã§ã€äººæç´¹ä»‹ã‚µãƒ¼ãƒ“ã‚¹ã€ŒHUGAN JOBã€ã‚’æ‹…å½“ã—ã¦ãŠã‚Šã¾ã™ç«¹ä¸‹ã¨ç”³ã—ã¾ã™ã€‚</p>

        <p>ã“ã®åº¦ã€è²´ç¤¾ãŒå‹Ÿé›†ã•ã‚Œã¦ãŠã‚Šã¾ã™ã€Œ{job_position}ã€ã®æ±‚äººã‚’æ‹è¦‹ã—ã€å¼Šç¤¾ã®ã‚µãƒ¼ãƒ“ã‚¹ãŒè²´ç¤¾ã®æ¡ç”¨æ´»å‹•ã«è²¢çŒ®ã§ãã‚‹ã‚‚ã®ã¨è€ƒãˆã€ã”é€£çµ¡ã„ãŸã—ã¾ã—ãŸã€‚</p>

        <p>åˆæœŸè²»ç”¨0å††ã®å®Œå…¨æˆåŠŸå ±é…¬å‹ã§ã€æ¡ç”¨å·¥æ•°ã®å‰Šæ¸›ã¨ãƒŸã‚¹ãƒãƒƒãƒé˜²æ­¢ã‚’å®Ÿç¾ã„ãŸã—ã¾ã™ã€‚</p>
    </div>
    <div class="footer">
        <p>æ ªå¼ä¼šç¤¾HUGAN<br>
        ç«¹ä¸‹éš¼å¹³<br>
        Email: contact@huganjob.jp</p>
    </div>
</body>
</html>
'''

        logger.info(f"ãƒ¡ãƒ¼ãƒ«å†…å®¹ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {len([c for c in companies if c.get('email_content')])}ç¤¾")

    except Exception as e:
        logger.error(f"ãƒ¡ãƒ¼ãƒ«å†…å®¹ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    return companies

@app.route('/api/companies')
def api_companies():
    """ä¼æ¥­ãƒ‡ãƒ¼ã‚¿API"""
    try:
        companies = load_company_data()
        return jsonify({
            'success': True,
            'companies': companies,
            'total': len(companies),
            'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
    except Exception as e:
        logger.error(f"ä¼æ¥­ãƒ‡ãƒ¼ã‚¿APIã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }), 500

@app.route('/api/stats')
def api_stats():
    """çµ±è¨ˆæƒ…å ±API - è»½é‡ç‰ˆå¯¾å¿œ"""
    try:
        # è»½é‡ç‰ˆçµ±è¨ˆã‚’ä½¿ç”¨ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã‚’çŸ­ç¸®
        lightweight = request.args.get('lightweight', 'false').lower() == 'true'

        if lightweight:
            stats = get_basic_stats_lightweight()
        else:
            stats = get_basic_stats()

        return jsonify({
            'success': True,
            'stats': stats,
            'lightweight': lightweight,
            'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
    except Exception as e:
        logger.error(f"çµ±è¨ˆæƒ…å ±APIã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }), 500

@app.route('/api/memory_optimize', methods=['POST'])
def api_memory_optimize():
    """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–API"""
    try:
        collected = optimize_memory()
        return jsonify({
            'success': True,
            'collected_objects': collected,
            'message': f'{collected}å€‹ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å›åã—ã¾ã—ãŸ',
            'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
    except Exception as e:
        logger.error(f"ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–APIã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }), 500

@app.route('/api/cache_clear', methods=['POST'])
def api_cache_clear():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢API"""
    try:
        clear_all_caches()
        return jsonify({
            'success': True,
            'message': 'ã™ã¹ã¦ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ',
            'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
    except Exception as e:
        logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢APIã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }), 500

@app.route('/api/email_open_stats')
def api_email_open_stats():
    """ãƒ¡ãƒ¼ãƒ«é–‹å°çµ±è¨ˆAPI"""
    try:
        open_stats = get_email_open_stats()
        return jsonify({
            'success': True,
            'open_stats': open_stats,
            'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
    except Exception as e:
        logger.error(f"é–‹å°çµ±è¨ˆAPIã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }), 500

@app.route('/api/refresh', methods=['POST'])
def api_refresh():
    """ãƒ‡ãƒ¼ã‚¿ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥APIï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
    try:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
        global company_data_cache, company_data_last_updated
        global company_stats_cache, company_stats_last_updated
        global stats_cache, stats_last_updated
        global filtered_companies_cache, filtered_companies_last_updated

        company_data_cache = None
        company_data_last_updated = None
        company_stats_cache = None
        company_stats_last_updated = None
        stats_cache = None
        stats_last_updated = None
        filtered_companies_cache = {}
        filtered_companies_last_updated = {}

        logger.info("å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")

        # ãƒ‡ãƒ¼ã‚¿ã‚’å†èª­ã¿è¾¼ã¿
        companies = load_company_data()
        logger.info(f"ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’å†èª­ã¿è¾¼ã¿ã—ã¾ã—ãŸ: {len(companies)}ç¤¾")

        return jsonify({
            'success': True,
            'message': f'ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã¾ã—ãŸï¼ˆ{len(companies)}ç¤¾ï¼‰',
            'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }), 500



def analyze_huganjob_progress(process_info):
    """HUGANJOBçµ±åˆé€ä¿¡ã®é€²è¡ŒçŠ¶æ³ã‚’è§£æ"""
    try:
        output = process_info.get('output', '')
        command = process_info.get('command', '')

        # HUGANJOBçµ±åˆé€ä¿¡ãƒ—ãƒ­ã‚»ã‚¹ã‹ã©ã†ã‹ç¢ºèª
        if 'huganjob_unified_sender' not in command:
            return {'type': 'unknown', 'message': 'HUGANJOBçµ±åˆé€ä¿¡ä»¥å¤–ã®ãƒ—ãƒ­ã‚»ã‚¹'}

        # ã‚³ãƒãƒ³ãƒ‰ã‹ã‚‰IDç¯„å›²ã‚’æŠ½å‡º
        import re
        start_match = re.search(r'--start-id\s+(\d+)', command)
        end_match = re.search(r'--end-id\s+(\d+)', command)

        if not start_match or not end_match:
            return {'type': 'unknown', 'message': 'IDç¯„å›²ã‚’ç‰¹å®šã§ãã¾ã›ã‚“'}

        start_id = int(start_match.group(1))
        end_id = int(end_match.group(1))
        total_companies = end_id - start_id + 1

        # å‡ºåŠ›ã‹ã‚‰é€²è¡ŒçŠ¶æ³ã‚’è§£æ
        progress_info = {
            'type': 'huganjob_unified_sender',
            'start_id': start_id,
            'end_id': end_id,
            'total_companies': total_companies,
            'processed_companies': 0,
            'success_count': 0,
            'failed_count': 0,
            'skipped_count': 0,
            'bounced_count': 0,
            'unsubscribed_count': 0,
            'current_company': None,
            'progress_percent': 0.0,
            'estimated_remaining_time': 'N/A'
        }

        # å‡ºåŠ›ã‹ã‚‰å‡¦ç†æ¸ˆã¿ä¼æ¥­æ•°ã‚’è§£æ
        lines = output.split('\n')
        for line in lines:
            # é€ä¿¡é–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ¤œç´¢
            if 'é€ä¿¡é–‹å§‹' in line and 'ID' in line:
                match = re.search(r'(\d+)/(\d+):\s*ID\s*(\d+)', line)
                if match:
                    current_index = int(match.group(1))
                    total_count = int(match.group(2))
                    current_id = int(match.group(3))

                    progress_info['processed_companies'] = current_index
                    progress_info['current_company'] = current_id
                    progress_info['progress_percent'] = round((current_index / total_count) * 100, 1)

            # é€ä¿¡çµæœã‚’è§£æ
            elif 'é€ä¿¡çµæœ:' in line:
                if 'success' in line:
                    progress_info['success_count'] += 1
                elif 'failed' in line:
                    progress_info['failed_count'] += 1
                elif 'skipped' in line:
                    progress_info['skipped_count'] += 1
                elif 'bounced' in line:
                    progress_info['bounced_count'] += 1
                elif 'unsubscribed' in line:
                    progress_info['unsubscribed_count'] += 1

        # æ®‹ã‚Šæ™‚é–“ã‚’æ¨å®šï¼ˆ5ç§’é–“éš”ã§é€ä¿¡ï¼‰
        remaining_companies = total_companies - progress_info['processed_companies']
        if remaining_companies > 0:
            estimated_seconds = remaining_companies * 5
            if estimated_seconds < 60:
                progress_info['estimated_remaining_time'] = f"{estimated_seconds}ç§’"
            elif estimated_seconds < 3600:
                minutes = estimated_seconds // 60
                progress_info['estimated_remaining_time'] = f"{minutes}åˆ†"
            else:
                hours = estimated_seconds // 3600
                minutes = (estimated_seconds % 3600) // 60
                progress_info['estimated_remaining_time'] = f"{hours}æ™‚é–“{minutes}åˆ†"
        else:
            progress_info['estimated_remaining_time'] = 'å®Œäº†'

        return progress_info

    except Exception as e:
        logger.error(f"é€²è¡ŒçŠ¶æ³è§£æã‚¨ãƒ©ãƒ¼: {e}")
        return {'type': 'error', 'message': f'è§£æã‚¨ãƒ©ãƒ¼: {e}'}

@app.route('/api/process_status/<process_id>')
def get_process_status(process_id):
    """ãƒ—ãƒ­ã‚»ã‚¹ã®çŠ¶æ…‹ã‚’å–å¾—ï¼ˆé€²è¡ŒçŠ¶æ³è¡¨ç¤ºå¼·åŒ–ï¼‰"""
    if process_id not in running_processes:
        # å±¥æ­´ã‹ã‚‰æ¤œç´¢
        for history_process in process_history:
            if history_process.get('id') == process_id:
                return jsonify({
                    'status': history_process.get('status', 'unknown'),
                    'output': history_process.get('output', ''),
                    'error': history_process.get('error', ''),
                    'progress': history_process.get('progress', {}),
                    'duration': history_process.get('duration', 'N/A')
                })

        return jsonify({'success': False, 'message': 'ãƒ—ãƒ­ã‚»ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'})

    process_info = running_processes[process_id]

    # å‡ºåŠ›ã‚’å–å¾—
    output = process_info.get('output', '')

    # ğŸ†• HUGANJOBçµ±åˆé€ä¿¡ã®é€²è¡ŒçŠ¶æ³ã‚’è§£æ
    progress_info = analyze_huganjob_progress(process_info)

    # çŠ¶æ…‹ã‚’è¿”ã™
    return jsonify({
        'status': process_info['status'],
        'output': output,
        'error': process_info.get('error', ''),
        'description': process_info.get('description', process_info['command']),
        'duration': str(datetime.datetime.now() - process_info['start_time']).split('.')[0],
        'progress': progress_info
    })



@app.route('/details/<step>')
def step_details(step):
    """ã‚¹ãƒ†ãƒƒãƒ—ã®è©³ç´°ãƒšãƒ¼ã‚¸"""
    progress_data = load_progress()
    step_info = get_step_info(step, progress_data)

    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’å–å¾—
    log_content = get_log_content_for_step(step)

    return render_template(
        'details.html',
        step=step_info,
        log_content=log_content,
        last_updated=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        get_step_display_name=get_step_display_name
    )



def get_log_content_for_step(step):
    """ã‚¹ãƒ†ãƒƒãƒ—ã«å¯¾å¿œã™ã‚‹ãƒ­ã‚°å†…å®¹ã‚’å–å¾—"""
    try:
        log_file = None
        if step == 'extract_emails':
            log_file = LOG_FILES['extraction']
        elif step == 'process_bounces':
            log_file = LOG_FILES['bounce']
        elif step == 'send_emails':
            log_file = LOG_FILES['email_sending']
        else:
            log_file = LOG_FILES['integrated']

        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯å†…å®¹ã‚’èª­ã¿è¾¼ã¿
        if log_file and os.path.exists(log_file):
            with open(log_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                # æœ€æ–°ã®100è¡Œã‚’è¿”ã™
                return ''.join(lines[-100:])
        else:
            return f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {log_file}"
    except Exception as e:
        return f"ãƒ­ã‚°èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"

@app.route('/company/<int:company_id>')
def company_detail(company_id):
    """ä¼æ¥­è©³ç´°ãƒšãƒ¼ã‚¸"""
    try:
        companies = load_company_data()

        # æŒ‡å®šã•ã‚ŒãŸIDã®ä¼æ¥­ã‚’æ¤œç´¢
        company = None
        for c in companies:
            if int(c['id']) == company_id:
                company = c
                break

        if not company:
            abort(404)

        # ãƒ‡ãƒãƒƒã‚°: ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
        logger.info(f"ä¼æ¥­è©³ç´°ãƒšãƒ¼ã‚¸ - ä¼æ¥­ID: {company_id}")
        logger.info(f"ä¼æ¥­å: {company.get('name', 'N/A')}")
        logger.info(f"ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹: {company.get('email', 'N/A')}")
        logger.info(f"ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºãƒ•ãƒ©ã‚°: {company.get('email_extracted', 'N/A')}")
        logger.info(f"ä¿¡é ¼åº¦: {company.get('email_confidence', 'N/A')}")
        logger.info(f"æŠ½å‡ºæ–¹æ³•: {company.get('extraction_method', 'N/A')}")

        # é–‹å°çŠ¶æ³ã‚’å–å¾—
        open_status = get_company_open_status_detail(company_id)

        return render_template(
            'company_detail.html',
            company=company,
            open_status=open_status,
            last_updated=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        )
    except Exception as e:
        logger.error(f"ä¼æ¥­è©³ç´°ãƒšãƒ¼ã‚¸ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return render_template('error.html', error=str(e)), 500

def run_process(command, args=None):
    """ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œ"""
    try:
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’æ§‹ç¯‰
        cmd = ['python', command]

        # ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œæ™‚ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        env['PYTHONPATH'] = os.getcwd() + os.pathsep + env.get('PYTHONPATH', '')

        # analyze_websites_new_criteria.pyã®å ´åˆã¯ã€é©åˆ‡ãªå…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®š
        if command == 'analyze_websites_new_criteria.py':
            # å¼•æ•°ã‚’è§£æã—ã¦ã€--input-fileãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯è¿½åŠ 
            args_list = []
            if args:
                if isinstance(args, str):
                    args_list = args.split()
                elif isinstance(args, list):
                    args_list = args.copy()

            # --input-fileãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯è¿½åŠ 
            if '--input-file' not in ' '.join(args_list):
                args_list.extend(['--input-file', INPUT_FILE])
                logger.info(f"ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æã®ãŸã‚ã€å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ : {INPUT_FILE}")

            # ç’°å¢ƒå¤‰æ•°ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¨­å®š
            env['PYTHONIOENCODING'] = 'utf-8'

            cmd.extend(args_list)
        # çµ±åˆãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œã®å ´åˆã®ç‰¹åˆ¥å‡¦ç†
        elif command == 'integrated_email_marketing_system.py':
            # å¼•æ•°ã‚’æ–‡å­—åˆ—ã‹ã‚‰ãƒªã‚¹ãƒˆã«å¤‰æ›ï¼ˆæ–‡å­—åˆ—ã®å ´åˆï¼‰
            if isinstance(args, str):
                args_list = args.split()
            else:
                args_list = args if isinstance(args, list) else []

            # æ–°ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ã®çµ±åˆãƒ—ãƒ­ã‚»ã‚¹ã§ã¯ã€new_integrated_workflow.pyã‚’ä½¿ç”¨
            logger.info("çµ±åˆãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œè¦æ±‚ã‚’æ–°ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«å¤‰æ›´ã—ã¾ã™")
            command = 'new_integrated_workflow.py'

            # å¼•æ•°ãŒç©ºã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¼•æ•°ã‚’è¨­å®š
            if not args_list:
                args_list = ['--start-id', '51', '--end-id', '60']
                logger.info(f"çµ±åˆãƒ—ãƒ­ã‚»ã‚¹ã®ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¼•æ•°ã‚’è¨­å®š: {args_list}")

            # new_integrated_workflow.pyã¯--input-fileãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¿…è¦ã¨ã—ãªã„
            # ï¼ˆå†…éƒ¨ã§new_input_utf8.csvã‚’è‡ªå‹•ä½¿ç”¨ã™ã‚‹ãŸã‚ï¼‰
            logger.info(f"new_integrated_workflow.pyã¯å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•æ¤œå‡ºã—ã¾ã™ï¼ˆ--input-fileãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸è¦ï¼‰")

            cmd = ['python', command]
            cmd.extend(args_list)
        # new_integrated_workflow.pyã®ç›´æ¥å®Ÿè¡Œã®å ´åˆã‚‚--input-fileã‚’è¿½åŠ ã—ãªã„
        elif command == 'new_integrated_workflow.py':
            logger.info("new_integrated_workflow.pyã®ç›´æ¥å®Ÿè¡Œ - å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•æ¤œå‡º")
            cmd = ['python', command]
            if args:
                if isinstance(args, str):
                    cmd.extend(args.split())
                elif isinstance(args, list):
                    cmd.extend(args)
        else:
            # ãã®ä»–ã®ã‚³ãƒãƒ³ãƒ‰ã®å ´åˆã¯é€šå¸¸é€šã‚Š
            if args:
                if isinstance(args, str):
                    # æ–‡å­—åˆ—ã®å ´åˆã¯ã‚¹ãƒšãƒ¼ã‚¹ã§åˆ†å‰²
                    cmd.extend(args.split())
                elif isinstance(args, list):
                    cmd.extend(args)

        logger.info(f"ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ: {' '.join(cmd)}")

        # ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡ºåŠ›å¯¾å¿œï¼‰
        try:
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,  # ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ
                encoding='utf-8',
                errors='replace',
                bufsize=1,  # è¡Œãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°
                universal_newlines=True,
                cwd=os.getcwd(),
                env=env
            )
        except Exception as e:
            logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
            raise

        # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±ã‚’è¨˜éŒ²
        process_id = str(process.pid)

        # å¼•æ•°ã‚’æ–‡å­—åˆ—å½¢å¼ã§ä¿å­˜ï¼ˆè¡¨ç¤ºç”¨ï¼‰
        args_str = ""
        if args:
            if isinstance(args, list):
                args_str = " ".join(args)
            else:
                args_str = args

        # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±ã‚’è¨˜éŒ²
        running_processes[process_id] = {
            'process': process,
            'command': command,
            'args': args,
            'args_str': args_str,
            'start_time': datetime.datetime.now(),
            'status': 'running',
            'description': get_process_description(command, args),
            'output': ''
        }

        # ğŸ†• ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹ï¼ˆé‡è¦ãªä¿®æ­£ï¼‰
        monitor_thread = threading.Thread(target=monitor_process, args=(process_id,))
        monitor_thread.daemon = True
        monitor_thread.start()

        logger.info(f"ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ã®ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹ã—ã¾ã—ãŸ")

        return process_id
    except Exception as e:
        logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None

def get_process_description(command, args):
    """ãƒ—ãƒ­ã‚»ã‚¹ã®æ—¥æœ¬èªèª¬æ˜ã‚’ç”Ÿæˆ"""
    description = ""

    # ã‚³ãƒãƒ³ãƒ‰ã«åŸºã¥ã„ã¦åŸºæœ¬èª¬æ˜ã‚’è¨­å®š
    if command == 'prioritized_email_extractor.py':
        description = "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡º"
    elif command == 'new_email_extractor.py':
        description = "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡º"
    elif command == 'new_website_analyzer.py':
        description = "ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æ"
    elif command == 'analyze_websites_new_criteria.py':
        description = "ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æ"
    elif command == 'new_email_sender.py':
        description = "ãƒ¡ãƒ¼ãƒ«é€ä¿¡"
    elif command == 'enhanced_bounce_processor.py':
        description = "ãƒã‚¦ãƒ³ã‚¹å‡¦ç†"
    elif command == 'standalone_bounce_processor.py':
        description = "HUGANJOB ãƒã‚¦ãƒ³ã‚¹å‡¦ç†"
    elif command == 'huganjob_bounce_processor.py':
        description = "HUGANJOB ãƒã‚¦ãƒ³ã‚¹å‡¦ç†"
    elif command == 'new_integrated_workflow.py':
        description = "çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ"
    elif command == 'integrated_email_marketing_system.py':
        description = "çµ±åˆãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ"
    else:
        # ãã®ä»–ã®ã‚³ãƒãƒ³ãƒ‰ã®å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ãã®ã¾ã¾ä½¿ç”¨
        description = command

    # å¼•æ•°ã‹ã‚‰ä¼æ¥­IDç¯„å›²ã‚„ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    start_id = None
    end_id = None
    days = None
    test_mode = False

    # å¼•æ•°ãŒæ–‡å­—åˆ—ã®å ´åˆã¯ã‚¹ãƒšãƒ¼ã‚¹ã§åˆ†å‰²
    if isinstance(args, str):
        args_list = args.split()
    else:
        args_list = args if args else []

    # å„ç¨®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ã™
    for i, arg in enumerate(args_list):
        if arg == '--start-id' and i + 1 < len(args_list):
            start_id = args_list[i + 1]
        elif arg == '--end-id' and i + 1 < len(args_list):
            end_id = args_list[i + 1]
        elif arg == '--days' and i + 1 < len(args_list):
            days = args_list[i + 1]
        elif arg == '--test-mode':
            test_mode = True

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¿œã˜ã¦èª¬æ˜ã‚’è¿½åŠ 
    if command == 'enhanced_bounce_processor.py' or command == 'standalone_bounce_processor.py' or command == 'huganjob_bounce_processor.py':
        if days:
            description += f" (éå»{days}æ—¥é–“)"
        if test_mode:
            description += " [ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ãƒ»ç§»å‹•ãªã—]"
        else:
            description += " [å‡¦ç†æ¸ˆã¿ç§»å‹•ã‚ã‚Š]"
    elif start_id and end_id:
        description += f" (ID {start_id}-{end_id})"
    elif start_id:
        description += f" (ID {start_id}ã‹ã‚‰)"

    return description

def monitor_process(process_id):
    """ãƒ—ãƒ­ã‚»ã‚¹ã®ç›£è¦–ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
    if process_id not in running_processes:
        logger.warning(f"ãƒ—ãƒ­ã‚»ã‚¹ID {process_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç›£è¦–ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return

    process_info = running_processes[process_id]
    process = process_info['process']
    command = process_info['command']

    # å‡ºåŠ›ãƒãƒƒãƒ•ã‚¡
    output_buffer = []

    # ğŸ†• ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–å¼·åŒ–: å®šæœŸçš„ãªç”Ÿå­˜ç¢ºèª
    last_alive_check = time.time()
    check_interval = 10  # 10ç§’é–“éš”ã§ãƒ—ãƒ­ã‚»ã‚¹ç”Ÿå­˜ç¢ºèª

    try:
        logger.info(f"ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ({command}) ã®ç›£è¦–ã‚’é–‹å§‹ã—ã¾ã™")

        # ğŸ†• å¼·åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ãƒ«ãƒ¼ãƒ—
        process_info['status'] = 'running'

        while True:
            # å®šæœŸçš„ãªãƒ—ãƒ­ã‚»ã‚¹ç”Ÿå­˜ç¢ºèª
            current_time = time.time()
            if current_time - last_alive_check >= check_interval:
                poll_result = process.poll()
                if poll_result is not None:
                    logger.info(f"ãƒ—ãƒ­ã‚»ã‚¹ {process_id} çµ‚äº†æ¤œå‡ºï¼ˆpollçµæœ: {poll_result}ï¼‰")
                    return_code = poll_result
                    break
                last_alive_check = current_time
                logger.debug(f"ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ç”Ÿå­˜ç¢ºèªOK")

            # å‡ºåŠ›èª­ã¿å–ã‚Šï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
            try:
                line = process.stdout.readline()
                if line:
                    line = line.rstrip('\n\r')
                    if line:
                        output_buffer.append(line)
                        process_info['output'] = '\n'.join(output_buffer[-100:])
                        logger.info(f"ãƒ—ãƒ­ã‚»ã‚¹ {process_id}: {line}")
                        sys.stdout.flush()
                elif process.poll() is not None:
                    # å‡ºåŠ›ãŒç©ºã§ãƒ—ãƒ­ã‚»ã‚¹ãŒçµ‚äº†ã—ã¦ã„ã‚‹
                    return_code = process.poll()
                    logger.info(f"ãƒ—ãƒ­ã‚»ã‚¹ {process_id} å‡ºåŠ›çµ‚äº†ã§çµ‚äº†æ¤œå‡ºï¼ˆçµ‚äº†ã‚³ãƒ¼ãƒ‰: {return_code}ï¼‰")
                    break
            except Exception as e:
                logger.warning(f"ãƒ—ãƒ­ã‚»ã‚¹ {process_id} å‡ºåŠ›èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}")
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†ãƒã‚§ãƒƒã‚¯ã¯ç¶™ç¶š
                if process.poll() is not None:
                    return_code = process.poll()
                    logger.info(f"ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ä¾‹å¤–å¾Œçµ‚äº†æ¤œå‡ºï¼ˆçµ‚äº†ã‚³ãƒ¼ãƒ‰: {return_code}ï¼‰")
                    break

            # CPUä½¿ç”¨ç‡ã‚’ä¸‹ã’ã‚‹ãŸã‚ã®çŸ­æ™‚é–“ã‚¹ãƒªãƒ¼ãƒ—
            time.sleep(0.1)

        # ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†å‡¦ç†
        if return_code == 0:
            process_info['status'] = 'completed'
            logger.info(f"ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ({command}) ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        else:
            process_info['status'] = 'failed'
            # å®Ÿè¡Œæ™‚é–“ãŒç•°å¸¸ã«çŸ­ã„å ´åˆã¯ç‰¹åˆ¥ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
            duration = datetime.datetime.now() - process_info['start_time']
            if duration.total_seconds() < 5:
                error_msg = f"ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ({command}) ãŒç•°å¸¸ã«çŸ­æ™‚é–“ã§çµ‚äº†ã—ã¾ã—ãŸ (å®Ÿè¡Œæ™‚é–“: {duration.total_seconds():.1f}ç§’, çµ‚äº†ã‚³ãƒ¼ãƒ‰: {return_code})"
                logger.error(error_msg)
                logger.error("å¼•æ•°ã‚¨ãƒ©ãƒ¼ã¾ãŸã¯åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                logger.error(f"å®Ÿè¡Œã•ã‚ŒãŸã‚³ãƒãƒ³ãƒ‰: python {command} {process_info.get('args_str', '')}")
                process_info['error'] = f"ç•°å¸¸çµ‚äº†: å®Ÿè¡Œæ™‚é–“{duration.total_seconds():.1f}ç§’"
            else:
                logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ({command}) ãŒã‚¨ãƒ©ãƒ¼ã§çµ‚äº†ã—ã¾ã—ãŸ (çµ‚äº†ã‚³ãƒ¼ãƒ‰: {return_code})")

        process_info['end_time'] = datetime.datetime.now()
        process_info['return_code'] = return_code

        # å±¥æ­´ã«è¿½åŠ 
        add_process_to_history(process_info.copy())

        # ğŸ†• å³åº§ã«å®Ÿè¡Œä¸­ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰å‰Šé™¤ï¼ˆé…å»¶å‰Šé™¤ã‚’å»ƒæ­¢ï¼‰
        if process_id in running_processes:
            logger.info(f"å®Œäº†ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ã‚’å³åº§ã«å‰Šé™¤ã—ã¾ã™")
            del running_processes[process_id]

        # ğŸ†• ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†é€šçŸ¥ï¼ˆä»–ã®ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¸ã®é€šçŸ¥ï¼‰
        try:
            # ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†ã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            completion_log = {
                'process_id': process_id,
                'command': command,
                'status': process_info['status'],
                'return_code': return_code,
                'end_time': process_info['end_time'].isoformat(),
                'duration': str(datetime.datetime.now() - process_info['start_time'])
            }

            # å®Œäº†ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²
            completion_log_file = 'logs/process_completion.log'
            os.makedirs('logs', exist_ok=True)
            with open(completion_log_file, 'a', encoding='utf-8') as f:
                f.write(f"{datetime.datetime.now().isoformat()}: {json.dumps(completion_log, ensure_ascii=False)}\n")

        except Exception as log_error:
            logger.warning(f"ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ãƒ­ã‚°è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {log_error}")

    except Exception as e:
        logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ã®ç›£è¦–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        process_info['status'] = 'error'
        process_info['error'] = str(e)
        process_info['end_time'] = datetime.datetime.now()

        # å±¥æ­´ã«è¿½åŠ 
        add_process_to_history(process_info.copy())

        # ğŸ†• ã‚¨ãƒ©ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã‚‚å³åº§ã«å‰Šé™¤
        if process_id in running_processes:
            logger.info(f"ã‚¨ãƒ©ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ã‚’å³åº§ã«å‰Šé™¤ã—ã¾ã™")
            del running_processes[process_id]

@app.route('/api/start_process', methods=['POST'])
def start_process():
    """ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹"""
    try:
        command = request.form.get('command')
        args = request.form.get('args', '')
        batch_mode = request.form.get('batch_mode', 'false').lower() == 'true'
        batch_size = int(request.form.get('batch_size', 20))

        if not command:
            return jsonify({'success': False, 'message': 'ã‚³ãƒãƒ³ãƒ‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“'})

        logger.info(f"ãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹è¦æ±‚: {command} {args} (ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰: {batch_mode})")

        # 100ç¤¾ä»¥ä¸Šã®å‡¦ç†ã®å ´åˆã€è‡ªå‹•çš„ã«ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–
        if '--start-id' in args and '--end-id' in args:
            try:
                args_parts = args.split()
                start_idx = args_parts.index('--start-id') + 1
                end_idx = args_parts.index('--end-id') + 1
                start_id = int(args_parts[start_idx])
                end_id = int(args_parts[end_idx])
                range_size = end_id - start_id + 1

                if range_size >= 100:
                    batch_mode = True
                    logger.info(f"å‡¦ç†ç¯„å›²ãŒ{range_size}ç¤¾ã®ãŸã‚ã€è‡ªå‹•çš„ã«ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–")
                elif range_size > 20 and not batch_mode:
                    logger.warning(f"å‡¦ç†ç¯„å›²ãŒ{range_size}ç¤¾ã§ã™ã€‚å®‰å®šæ€§ã®ãŸã‚ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰ã‚’æ¨å¥¨ã—ã¾ã™")
            except (ValueError, IndexError):
                pass

        # ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ãªå ´åˆã€å¼•æ•°ã‚’èª¿æ•´
        if batch_mode and command == 'new_integrated_workflow.py':
            if '--batch-mode' not in args:
                args += ' --batch-mode'
            if '--batch-size' not in args:
                args += f' --batch-size {batch_size}'
            logger.info(f"ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰å¼•æ•°ã‚’è¿½åŠ : {args}")

        # ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œ
        process_id = run_process(command, args)
        if not process_id:
            return jsonify({'success': False, 'message': 'ãƒ—ãƒ­ã‚»ã‚¹ã®é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸ'})

        # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
        monitor_thread = threading.Thread(target=monitor_process, args=(process_id,))
        monitor_thread.daemon = True
        monitor_thread.start()

        logger.info(f"ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ã‚’é–‹å§‹ã—ã¾ã—ãŸ: {command} {args}")

        return jsonify({
            'success': True,
            'message': f'ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ã—ã¾ã—ãŸ: {command}' + (' (ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰)' if batch_mode else ''),
            'process_id': process_id,
            'command': f"python {command} {args}".strip(),
            'batch_mode': batch_mode,
            'batch_size': batch_size if batch_mode else None
        })

    except Exception as e:
        logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return jsonify({
            'success': False,
            'message': f'ãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}'
        }), 500

@app.route('/api/stop_process/<process_id>', methods=['POST'])
def stop_process(process_id):
    """ãƒ—ãƒ­ã‚»ã‚¹ã‚’åœæ­¢"""
    try:
        if not process_id or process_id not in running_processes:
            return jsonify({'success': False, 'message': 'ãƒ—ãƒ­ã‚»ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'})

        # å®Ÿéš›ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ‚äº†
        process_info = running_processes[process_id]
        if 'process' in process_info and process_info['process']:
            try:
                process_info['process'].terminate()
                logger.info(f"ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ã‚’çµ‚äº†ã—ã¾ã—ãŸ")
            except Exception as e:
                logger.warning(f"ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ã®çµ‚äº†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

        # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±ã‚’æ›´æ–°
        running_processes[process_id]['status'] = 'stopped'
        running_processes[process_id]['end_time'] = datetime.datetime.now()

        # å±¥æ­´ã‚’æ›´æ–°
        add_process_to_history(running_processes[process_id].copy())

        # å®Ÿè¡Œä¸­ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰å‰Šé™¤
        del running_processes[process_id]

        logger.info(f"ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ã‚’åœæ­¢ã—ã¾ã—ãŸ")

        return jsonify({
            'success': True,
            'message': f'ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ã‚’åœæ­¢ã—ã¾ã—ãŸ'
        })

    except Exception as e:
        logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹åœæ­¢ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return jsonify({
            'success': False,
            'message': f'ãƒ—ãƒ­ã‚»ã‚¹åœæ­¢ã‚¨ãƒ©ãƒ¼: {e}'
        }), 500

@app.route('/api/update_process_status', methods=['POST'])
def update_process_status():
    """ãƒ—ãƒ­ã‚»ã‚¹çŠ¶æ…‹ã‚’æ‰‹å‹•ã§æ›´æ–°"""
    try:
        process_id = request.form.get('process_id')
        new_status = request.form.get('status', 'completed')

        if not process_id:
            return jsonify({'success': False, 'message': 'ãƒ—ãƒ­ã‚»ã‚¹IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“'})

        # ãƒ—ãƒ­ã‚»ã‚¹å±¥æ­´ã‚’èª­ã¿è¾¼ã¿
        global process_history
        load_process_history()

        # è©²å½“ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ¤œç´¢ã—ã¦æ›´æ–°
        updated = False
        for i, process in enumerate(process_history):
            if process.get('id') == process_id:
                process_history[i]['status'] = new_status
                process_history[i]['end_time'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                updated = True
                break

        if updated:
            # å±¥æ­´ã‚’ä¿å­˜
            save_process_history()

            # å®Ÿè¡Œä¸­ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰ã‚‚å‰Šé™¤
            if process_id in running_processes:
                del running_processes[process_id]

            logger.info(f"ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ã®çŠ¶æ…‹ã‚’ {new_status} ã«æ›´æ–°ã—ã¾ã—ãŸ")

            return jsonify({
                'success': True,
                'message': f'ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ã®çŠ¶æ…‹ã‚’æ›´æ–°ã—ã¾ã—ãŸ'
            })
        else:
            return jsonify({
                'success': False,
                'message': 'ãƒ—ãƒ­ã‚»ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ'
            })

    except Exception as e:
        logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹çŠ¶æ…‹æ›´æ–°ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return jsonify({
            'success': False,
            'message': f'ãƒ—ãƒ­ã‚»ã‚¹çŠ¶æ…‹æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}'
        }), 500

@app.route('/api/refresh_data', methods=['POST'])
def refresh_data():
    """ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦å†èª­ã¿è¾¼ã¿"""
    try:
        global company_data_cache, company_data_last_updated

        # çµ±åˆãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œå¾Œã®è‡ªå‹•æ›´æ–°ã‹ã©ã†ã‹ã‚’ç¢ºèª
        auto_refresh = request.form.get('auto_refresh', 'false').lower() == 'true'

        if auto_refresh:
            logger.info("çµ±åˆãƒ—ãƒ­ã‚»ã‚¹å®Œäº†å¾Œã®è‡ªå‹•ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚’å®Ÿè¡Œã—ã¾ã™")
        else:
            logger.info("æ‰‹å‹•ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚’å®Ÿè¡Œã—ã¾ã™")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
        company_data_cache = None
        company_data_last_updated = None

        # ãƒ‡ãƒ¼ã‚¿ã‚’å†èª­ã¿è¾¼ã¿
        companies = load_company_data()

        # çµ±åˆãƒ—ãƒ­ã‚»ã‚¹å¾Œã®å ´åˆã¯ã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹çµ±è¨ˆã‚’æ›´æ–°
        if auto_refresh:
            email_extracted_count = sum(1 for c in companies if c.get('email_extracted'))
            analyzed_count = sum(1 for c in companies if c.get('rank'))
            sent_count = sum(1 for c in companies if c.get('email_sent'))

            logger.info(f"çµ±åˆãƒ—ãƒ­ã‚»ã‚¹å¾Œã®çŠ¶æ³: ãƒ¡ãƒ¼ãƒ«æŠ½å‡º={email_extracted_count}ç¤¾, åˆ†æå®Œäº†={analyzed_count}ç¤¾, é€ä¿¡å®Œäº†={sent_count}ç¤¾")

        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚’å†èª­ã¿è¾¼ã¿ã—ã¾ã—ãŸ: {len(companies)}ç¤¾")

        return jsonify({
            'success': True,
            'message': f'ãƒ‡ãƒ¼ã‚¿ã‚’å†èª­ã¿è¾¼ã¿ã—ã¾ã—ãŸï¼ˆ{len(companies)}ç¤¾ï¼‰',
            'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'auto_refresh': auto_refresh
        })

    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿å†èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return jsonify({
            'success': False,
            'message': f'ãƒ‡ãƒ¼ã‚¿å†èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}'
        }), 500

@app.route('/control')
def control():
    """åˆ¶å¾¡ãƒ‘ãƒãƒ«ãƒšãƒ¼ã‚¸ï¼ˆãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆï¼‰"""
    return redirect('/')



@app.route('/api/get_process_history')
def get_process_history():
    """ãƒ—ãƒ­ã‚»ã‚¹å±¥æ­´ã‚’å–å¾—ï¼ˆè»½é‡åŒ–ç‰ˆï¼‰"""
    try:
        limit = min(request.args.get('limit', 5, type=int), 10)  # æœ€å¤§10ä»¶ã«åˆ¶é™

        # è»½é‡ç‰ˆå±¥æ­´èª­ã¿è¾¼ã¿
        recent_history = load_process_history_lightweight(limit)

        return jsonify(recent_history)
    except Exception as e:
        logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹å±¥æ­´å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify([])  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºé…åˆ—ã‚’è¿”ã™

# å–¶æ¥­ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³å®Ÿè¡ŒAPI - HUGANJOBå°‚ç”¨ã‚·ã‚¹ãƒ†ãƒ ã§ã¯å‰Šé™¤æ¸ˆã¿
# åºƒå‘Šã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãƒšãƒ¼ã‚¸ - HUGANJOBå°‚ç”¨ã‚·ã‚¹ãƒ†ãƒ ã§ã¯å‰Šé™¤æ¸ˆã¿

def load_process_history_lightweight(limit=5):
    """è»½é‡ç‰ˆãƒ—ãƒ­ã‚»ã‚¹å±¥æ­´èª­ã¿è¾¼ã¿"""
    try:
        if not os.path.exists(PROCESS_HISTORY_FILE):
            return []

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå¤§ãã™ãã‚‹å ´åˆã¯æœ«å°¾ã®ã¿èª­ã¿è¾¼ã¿ï¼‰
        file_size = os.path.getsize(PROCESS_HISTORY_FILE)
        if file_size > 1024 * 1024:  # 1MBä»¥ä¸Šã®å ´åˆ
            # æœ«å°¾ã‹ã‚‰èª­ã¿è¾¼ã¿
            with open(PROCESS_HISTORY_FILE, 'rb') as f:
                f.seek(-min(file_size, 50000), 2)  # æœ«å°¾50KB
                content = f.read().decode('utf-8', errors='ignore')
                lines = content.split('\n')[-limit*2:]  # ä½™è£•ã‚’æŒã£ã¦å–å¾—
        else:
            with open(PROCESS_HISTORY_FILE, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.split('\n')

        # JSONå½¢å¼ã®è¡Œã‚’è§£æ
        history = []
        for line in reversed(lines):  # æ–°ã—ã„é †
            if line.strip() and len(history) < limit:
                try:
                    entry = json.loads(line)
                    # è»½é‡åŒ–: å¿…è¦æœ€å°é™ã®æƒ…å ±ã®ã¿
                    lightweight_entry = {
                        'id': entry.get('id', f"proc_{len(history)}"),
                        'command': entry.get('command', 'ä¸æ˜')[:30] + '...' if len(entry.get('command', '')) > 30 else entry.get('command', 'ä¸æ˜'),
                        'start_time': entry.get('start_time', '')[-8:] if entry.get('start_time') else '',  # æ™‚åˆ»ã®ã¿
                        'status': entry.get('status', 'ä¸æ˜'),
                        'return_code': entry.get('return_code', None)
                    }
                    history.append(lightweight_entry)
                except json.JSONDecodeError:
                    continue

        return history
    except Exception as e:
        logger.error(f"è»½é‡ç‰ˆå±¥æ­´èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return []

@app.route('/daily_stats')
def daily_stats():
    """æ—¥åˆ¥ãƒ¡ãƒ¼ãƒ«é€ä¿¡ãƒ»ãƒã‚¦ãƒ³ã‚¹çµ±è¨ˆãƒšãƒ¼ã‚¸ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰"""
    global daily_stats_cache, daily_stats_last_updated

    # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰æœŸé–“ã‚’å–å¾—
    start_date = request.args.get('start_date')
    end_date = request.args.get('end_date')

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§éå»30æ—¥é–“ã‚’è¡¨ç¤º
    if not start_date or not end_date:
        end_date = datetime.datetime.now().strftime('%Y-%m-%d')
        start_date = (datetime.datetime.now() - datetime.timedelta(days=30)).strftime('%Y-%m-%d')

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
    cache_key = f"{start_date}_{end_date}"

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    current_time = time.time()
    if (daily_stats_cache and
        daily_stats_last_updated and
        cache_key in daily_stats_cache and
        current_time - daily_stats_last_updated < DAILY_STATS_CACHE_TIMEOUT):

        logger.info("æ—¥åˆ¥çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—")
        cached_data = daily_stats_cache[cache_key]
        return render_template(
            'daily_stats.html',
            daily_stats=cached_data['daily_stats'],
            bounce_reason_stats=cached_data['bounce_reason_stats'],
            chart_data=cached_data['chart_data'],
            start_date=start_date,
            end_date=end_date,
            unified_total_sent=cached_data['unified_total_sent'],
            sorted_dates=cached_data.get('sorted_dates', []),
            total_stats=cached_data.get('total_stats', {}),
            actual_bounce_stats=cached_data.get('actual_bounce_stats', {})
        )

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯æ–°è¦è¨ˆç®—
    logger.info("æ—¥åˆ¥çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’æ–°è¦è¨ˆç®—ä¸­...")

    # æ—¥åˆ¥çµ±è¨ˆã‚’å–å¾—ï¼ˆæ–°ã—ã„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ï¼‰
    daily_stats_data = get_daily_email_stats(start_date, end_date)

    # ãƒã‚¦ãƒ³ã‚¹ç†ç”±åˆ¥çµ±è¨ˆã‚’å–å¾—
    bounce_reason_stats = get_bounce_reason_statistics(start_date, end_date)

    # æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆ
    sorted_dates = sorted(daily_stats_data.keys())

    # çµ±ä¸€ã•ã‚ŒãŸé€ä¿¡æ•°ã‚’å–å¾—
    unified_total_sent = get_unified_sent_email_count()

    # ãƒãƒ£ãƒ¼ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    chart_data = {
        'dates': sorted_dates,
        'total': [daily_stats_data[date]['total'] for date in sorted_dates],
        'success': [daily_stats_data[date]['success'] for date in sorted_dates],
        'bounce': [daily_stats_data[date]['bounce'] for date in sorted_dates],
        'pending': [daily_stats_data[date]['pending'] for date in sorted_dates]
    }

    # å®Ÿéš›ã®ãƒã‚¦ãƒ³ã‚¹æ¤œçŸ¥ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    actual_bounce_stats = get_actual_bounce_statistics()

    # åˆè¨ˆçµ±è¨ˆã‚’è¨ˆç®—ï¼ˆçµ±ä¸€é€ä¿¡æ•°ã‚’ä½¿ç”¨ï¼‰
    total_stats = {
        'total': unified_total_sent,  # çµ±ä¸€ã•ã‚ŒãŸé€ä¿¡æ•°ã‚’ä½¿ç”¨
        'success': sum(daily_stats_data[date]['success'] for date in daily_stats_data),
        'bounce': actual_bounce_stats['total_bounces'],  # å®Ÿéš›ã®ãƒã‚¦ãƒ³ã‚¹æ¤œçŸ¥ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        'pending': sum(daily_stats_data[date]['pending'] for date in daily_stats_data)
    }

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    if daily_stats_cache is None:
        daily_stats_cache = {}

    daily_stats_cache[cache_key] = {
        'daily_stats': daily_stats_data,
        'bounce_reason_stats': bounce_reason_stats,
        'chart_data': chart_data,
        'unified_total_sent': unified_total_sent,
        'sorted_dates': sorted_dates,
        'total_stats': total_stats,
        'actual_bounce_stats': actual_bounce_stats
    }
    daily_stats_last_updated = current_time

    logger.info(f"æ—¥åˆ¥çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜: {cache_key}")

    # æˆåŠŸæ•°ã‚’å†è¨ˆç®—ï¼ˆç·é€ä¿¡æ•° - ãƒã‚¦ãƒ³ã‚¹æ•° - çµæœå¾…ã¡ï¼‰
    total_stats['success'] = total_stats['total'] - total_stats['bounce'] - total_stats['pending']

    # ãƒã‚¦ãƒ³ã‚¹ç‡ã¨æˆåŠŸç‡ã‚’è¨ˆç®—
    if total_stats['total'] > 0:
        total_stats['bounce_rate'] = round((total_stats['bounce'] / total_stats['total']) * 100, 1)
        total_stats['success_rate'] = round((total_stats['success'] / total_stats['total']) * 100, 1)
    else:
        total_stats['bounce_rate'] = 0.0
        total_stats['success_rate'] = 0.0

    # ãƒã‚¦ãƒ³ã‚¹ä¼æ¥­ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
    bounce_companies = get_bounce_companies_details(start_date, end_date)

    return render_template(
        'daily_stats.html',
        daily_stats=daily_stats_data,
        sorted_dates=sorted_dates,
        chart_data=chart_data,
        total_stats=total_stats,
        bounce_reason_stats=bounce_reason_stats.get('reasons', {}),
        bounce_type_stats=bounce_reason_stats.get('types', {}),
        bounce_companies=bounce_companies,
        start_date=start_date,
        end_date=end_date,
        last_updated=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    )

def get_daily_email_stats(start_date, end_date):
    """æ—¥åˆ¥ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµ±è¨ˆã‚’å–å¾—ï¼ˆæ–°ã—ã„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ãƒ»æœ€é©åŒ–ç‰ˆï¼‰"""
    try:
        daily_stats = {}

        # æ—¥ä»˜ç¯„å›²ã®åˆæœŸåŒ–
        current_date = parse_datetime_optimized(start_date + ' 00:00:00') or datetime.datetime.strptime(start_date, '%Y-%m-%d')
        end_date_obj = parse_datetime_optimized(end_date + ' 23:59:59') or datetime.datetime.strptime(end_date, '%Y-%m-%d')

        while current_date <= end_date_obj:
            date_str = current_date.strftime('%Y-%m-%d')
            daily_stats[date_str] = {
                'total': 0,
                'success': 0,
                'bounce': 0,
                'pending': 0
            }
            current_date += datetime.timedelta(days=1)

        # ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çµ±è¨ˆã‚’é›†è¨ˆï¼ˆå®‰å®šç‰ˆã‚’ä½¿ç”¨ï¼‰
        daily_stats = integrate_bounce_reports_to_daily_stats(daily_stats, start_date, end_date)

        # ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çµ±è¨ˆã‚’é›†è¨ˆ
        daily_stats = integrate_sending_results_to_daily_stats(daily_stats, start_date, end_date)

        return daily_stats

    except Exception as e:
        logger.error(f"æ—¥åˆ¥çµ±è¨ˆã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        logger.error(f"è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±: {traceback.format_exc()}")
        return {}

def integrate_bounce_reports_to_daily_stats(daily_stats, start_date, end_date):
    """ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¥åˆ¥çµ±è¨ˆã‚’é›†è¨ˆ"""
    try:
        import glob

        # ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        report_files = glob.glob('bounce_processing_report_*.json')

        if not report_files:
            logger.info("ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return daily_stats

        logger.info(f"ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ« {len(report_files)}å€‹ã‚’å‡¦ç†ä¸­")

        start_date_obj = parse_datetime_optimized(start_date + ' 00:00:00') or datetime.datetime.strptime(start_date, '%Y-%m-%d')
        end_date_obj = parse_datetime_optimized(end_date + ' 23:59:59') or datetime.datetime.strptime(end_date, '%Y-%m-%d')

        for report_file in report_files:
            try:
                with open(report_file, 'r', encoding='utf-8') as f:
                    report_data = json.load(f)

                # ãƒ¬ãƒãƒ¼ãƒˆã®æ—¥æ™‚ã‚’è§£æ
                timestamp_str = report_data.get('timestamp', '')
                if not timestamp_str:
                    continue

                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
                report_datetime = parse_datetime_optimized(timestamp_str)
                if report_datetime is None:
                    logger.warning(f"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®è§£æã«å¤±æ•—: {timestamp_str}")
                    continue

                # æŒ‡å®šæœŸé–“å†…ã‹ãƒã‚§ãƒƒã‚¯
                if not (start_date_obj <= report_datetime <= end_date_obj + datetime.timedelta(days=1)):
                    continue

                # æ—¥ä»˜æ–‡å­—åˆ—ã‚’ç”Ÿæˆ
                date_str = report_datetime.strftime('%Y-%m-%d')

                if date_str in daily_stats:
                    # ãƒã‚¦ãƒ³ã‚¹çµ±è¨ˆã‚’è¿½åŠ 
                    total_bounces = report_data.get('total_bounces_detected', 0)
                    emails_moved = report_data.get('emails_moved', 0)

                    daily_stats[date_str]['bounce'] += total_bounces
                    daily_stats[date_str]['total'] += total_bounces

                    logger.debug(f"æ—¥ä»˜ {date_str}: ãƒã‚¦ãƒ³ã‚¹ {total_bounces}ä»¶, ç§»å‹• {emails_moved}ä»¶ã‚’è¿½åŠ ")

            except Exception as e:
                logger.error(f"ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ« {report_file} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        return daily_stats

    except Exception as e:
        logger.error(f"ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆã®çµ±åˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return daily_stats

def integrate_sending_results_to_daily_stats(daily_stats, start_date, end_date):
    """ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¥åˆ¥çµ±è¨ˆã‚’é›†è¨ˆ"""
    try:
        import glob

        # ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        sending_files = []

        # å„ªå…ˆé †ä½ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        primary_file = 'new_email_sending_results.csv'
        if os.path.exists(primary_file):
            sending_files.append(primary_file)
        else:
            # IDç¯„å›²åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            id_files = glob.glob('sent_emails_record_id*.csv')
            if id_files:
                sending_files.extend(id_files)
            elif os.path.exists(NEW_EMAIL_SENDING_RESULTS):
                sending_files.append(NEW_EMAIL_SENDING_RESULTS)

        if not sending_files:
            logger.info("ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return daily_stats

        logger.info(f"ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ« {len(sending_files)}å€‹ã‚’å‡¦ç†ä¸­")

        start_date_obj = parse_datetime_optimized(start_date + ' 00:00:00') or datetime.datetime.strptime(start_date, '%Y-%m-%d')
        end_date_obj = parse_datetime_optimized(end_date + ' 23:59:59') or datetime.datetime.strptime(end_date, '%Y-%m-%d')

        for sending_file in sending_files:
            try:
                with open(sending_file, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)

                    for row in reader:
                        # è¤‡æ•°ã®åˆ—åã«å¯¾å¿œ
                        sent_date_str = (row.get('é€ä¿¡æ—¥æ™‚', '') or
                                       row.get('send_datetime', '') or
                                       row.get('sent_date', '')).strip()
                        if not sent_date_str:
                            continue

                        # é€ä¿¡æ—¥æ™‚ã‚’è§£æï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
                        try:
                            sent_datetime = parse_datetime_optimized(sent_date_str)
                            if sent_datetime is None:
                                if not PERFORMANCE_MODE:  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é‡è¦–ãƒ¢ãƒ¼ãƒ‰ã§ã¯è­¦å‘Šã‚’æŠ‘åˆ¶
                                    logger.warning(f"é€ä¿¡æ—¥æ™‚ã®è§£æã«å¤±æ•—: {sent_date_str}")
                                continue
                        except Exception as e:
                            logger.warning(f"é€ä¿¡æ—¥æ™‚ã®è§£æã‚¨ãƒ©ãƒ¼: {sent_date_str}, {e}")
                            continue

                        # æŒ‡å®šæœŸé–“å†…ã‹ãƒã‚§ãƒƒã‚¯
                        if not (start_date_obj <= sent_datetime <= end_date_obj + datetime.timedelta(days=1)):
                            continue

                        # æ—¥ä»˜æ–‡å­—åˆ—ã‚’ç”Ÿæˆ
                        date_str = sent_datetime.strftime('%Y-%m-%d')

                        if date_str in daily_stats:
                            # é€ä¿¡çµæœã‚’åˆ†é¡ï¼ˆè¤‡æ•°ã®åˆ—åã«å¯¾å¿œï¼‰
                            sent_result = (row.get('é€ä¿¡çµæœ', '') or
                                         row.get('success', '') or
                                         row.get('sent_result', '') or
                                         str(row.get('success', ''))).strip()
                            bounce_status = (row.get('ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹', '') or
                                           row.get('bounce_status', '') or
                                           row.get('error_message', '')).strip()
                            final_status = row.get('æœ€çµ‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹', '').strip()
                            company_id = (row.get('ä¼æ¥­ID', '') or
                                        row.get('company_id', '')).strip()

                            daily_stats[date_str]['total'] += 1

                            # å®Ÿéš›ã®ãƒã‚¦ãƒ³ã‚¹æ¤œçŸ¥ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚§ãƒƒã‚¯
                            actual_bounce_status = check_actual_bounce_status(company_id)

                            if actual_bounce_status or bounce_status == 'ãƒã‚¦ãƒ³ã‚¹' or final_status == 'ãƒã‚¦ãƒ³ã‚¹':
                                daily_stats[date_str]['bounce'] += 1
                            elif sent_result == 'success':
                                daily_stats[date_str]['success'] += 1
                            else:
                                daily_stats[date_str]['pending'] += 1

                            if ENABLE_DEBUG_LOGGING:
                                logger.debug(f"æ—¥ä»˜ {date_str}: é€ä¿¡çµæœ {sent_result}, å®Ÿéš›ã®ãƒã‚¦ãƒ³ã‚¹ {actual_bounce_status}")

            except Exception as e:
                logger.error(f"é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ« {sending_file} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        return daily_stats

    except Exception as e:
        logger.error(f"ãƒ¡ãƒ¼ãƒ«é€ä¿¡çµæœã®çµ±åˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return daily_stats

def get_bounce_reason_statistics(start_date, end_date):
    """ãƒã‚¦ãƒ³ã‚¹ç†ç”±åˆ¥çµ±è¨ˆã‚’å–å¾—"""
    try:
        import glob

        bounce_reasons = {}
        bounce_types = {'permanent': 0, 'temporary': 0, 'unknown': 0}
        start_date_obj = datetime.datetime.strptime(start_date, '%Y-%m-%d')
        end_date_obj = datetime.datetime.strptime(end_date, '%Y-%m-%d')

        # ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç†ç”±åˆ¥çµ±è¨ˆã‚’é›†è¨ˆ
        report_files = glob.glob('huganjob_bounce_report_*.json')

        for report_file in report_files:
            try:
                with open(report_file, 'r', encoding='utf-8') as f:
                    report_data = json.load(f)

                # ãƒ¬ãƒãƒ¼ãƒˆã®æ—¥æ™‚ã‚’è§£æ
                timestamp_str = report_data.get('timestamp', '')
                if not timestamp_str:
                    continue

                try:
                    report_datetime = datetime.datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
                except ValueError:
                    try:
                        report_datetime = datetime.datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M')
                    except ValueError:
                        continue

                # æŒ‡å®šæœŸé–“å†…ã‹ãƒã‚§ãƒƒã‚¯
                if not (start_date_obj <= report_datetime <= end_date_obj + datetime.timedelta(days=1)):
                    continue

                # ãƒã‚¦ãƒ³ã‚¹è©³ç´°ã‹ã‚‰ç†ç”±åˆ¥çµ±è¨ˆã‚’é›†è¨ˆ
                bounce_details = report_data.get('bounce_details', [])
                for detail in bounce_details:
                    reason = detail.get('reason', 'ä¸æ˜ãªãƒã‚¦ãƒ³ã‚¹ç†ç”±')
                    bounce_type = detail.get('bounce_type', 'unknown')

                    if reason not in bounce_reasons:
                        bounce_reasons[reason] = 0
                    bounce_reasons[reason] += 1

                    # ãƒã‚¦ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆã‚‚é›†è¨ˆ
                    if bounce_type in bounce_types:
                        bounce_types[bounce_type] += 1
                    else:
                        bounce_types['unknown'] += 1

            except Exception as e:
                logger.error(f"ãƒã‚¦ãƒ³ã‚¹ç†ç”±çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ« {report_file} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        # ãƒã‚¦ãƒ³ã‚¹è¿½è·¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚‚ç†ç”±ã‚’é›†è¨ˆ
        bounce_files = ['comprehensive_bounce_tracking_results.csv', NEW_BOUNCE_TRACKING]

        for bounce_file in bounce_files:
            if not os.path.exists(bounce_file):
                continue

            try:
                with open(bounce_file, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)

                    for row in reader:
                        bounce_date_str = row.get('ãƒã‚¦ãƒ³ã‚¹æ—¥æ™‚', '').strip()
                        if not bounce_date_str:
                            continue

                        # ãƒã‚¦ãƒ³ã‚¹æ—¥æ™‚ã‚’è§£æ
                        try:
                            for date_format in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M', '%Y-%m-%d']:
                                try:
                                    bounce_datetime = datetime.datetime.strptime(bounce_date_str, date_format)
                                    break
                                except ValueError:
                                    continue
                            else:
                                continue
                        except Exception:
                            continue

                        # æŒ‡å®šæœŸé–“å†…ã‹ãƒã‚§ãƒƒã‚¯
                        if not (start_date_obj <= bounce_datetime <= end_date_obj + datetime.timedelta(days=1)):
                            continue

                        reason = row.get('ãƒã‚¦ãƒ³ã‚¹ç†ç”±', 'ä¸æ˜ãªãƒã‚¦ãƒ³ã‚¹ç†ç”±').strip()
                        bounce_type = row.get('ãƒã‚¦ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—', 'unknown').strip()

                        if not reason:
                            reason = 'ä¸æ˜ãªãƒã‚¦ãƒ³ã‚¹ç†ç”±'

                        if reason not in bounce_reasons:
                            bounce_reasons[reason] = 0
                        bounce_reasons[reason] += 1

                        # ãƒã‚¦ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆã‚‚é›†è¨ˆ
                        if bounce_type in bounce_types:
                            bounce_types[bounce_type] += 1
                        else:
                            bounce_types['unknown'] += 1

            except Exception as e:
                logger.error(f"ãƒã‚¦ãƒ³ã‚¹è¿½è·¡ãƒ•ã‚¡ã‚¤ãƒ« {bounce_file} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥ãƒã‚¦ãƒ³ã‚¹çµ±è¨ˆã‚’å–å¾—
        csv_bounce_stats = get_csv_bounce_statistics(start_date, end_date)

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±è¨ˆã‚’ãƒãƒ¼ã‚¸
        for reason, count in csv_bounce_stats['reasons'].items():
            if reason not in bounce_reasons:
                bounce_reasons[reason] = 0
            bounce_reasons[reason] += count

        for bounce_type, count in csv_bounce_stats['types'].items():
            if bounce_type in bounce_types:
                bounce_types[bounce_type] += count

        # çµæœã«ãƒã‚¦ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆã‚’å«ã‚ã‚‹
        return {
            'reasons': bounce_reasons,
            'types': bounce_types,
            'total_bounces': sum(bounce_types.values())
        }

    except Exception as e:
        logger.error(f"ãƒã‚¦ãƒ³ã‚¹ç†ç”±çµ±è¨ˆã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return {'reasons': {}, 'types': {'permanent': 0, 'temporary': 0, 'unknown': 0}, 'total_bounces': 0}

def get_csv_bounce_statistics(start_date, end_date):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥ãƒã‚¦ãƒ³ã‚¹çµ±è¨ˆã‚’å–å¾—"""
    try:
        bounce_reasons = {}
        bounce_types = {'permanent': 0, 'temporary': 0, 'unknown': 0}

        start_date_obj = datetime.datetime.strptime(start_date, '%Y-%m-%d')
        end_date_obj = datetime.datetime.strptime(end_date, '%Y-%m-%d')

        # ä¼æ¥­ãƒ‡ãƒ¼ã‚¿CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒã‚¦ãƒ³ã‚¹æƒ…å ±ã‚’èª­ã¿å–ã‚Š
        csv_file = 'data/new_input_test.csv'
        if not os.path.exists(csv_file):
            return {'reasons': bounce_reasons, 'types': bounce_types}

        with open(csv_file, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)

            for row in reader:
                bounce_status = row.get('ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹', '').strip()
                bounce_date_str = row.get('ãƒã‚¦ãƒ³ã‚¹æ—¥æ™‚', '').strip()
                bounce_reason = row.get('ãƒã‚¦ãƒ³ã‚¹ç†ç”±', '').strip()

                # ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if not bounce_status or bounce_status.lower() in ['', 'none', 'null']:
                    continue

                # ãƒã‚¦ãƒ³ã‚¹æ—¥æ™‚ãŒã‚ã‚‹å ´åˆã¯æœŸé–“ãƒã‚§ãƒƒã‚¯
                if bounce_date_str:
                    try:
                        for date_format in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M', '%Y-%m-%d']:
                            try:
                                bounce_datetime = datetime.datetime.strptime(bounce_date_str, date_format)
                                break
                            except ValueError:
                                continue
                        else:
                            continue

                        # æŒ‡å®šæœŸé–“å†…ã‹ãƒã‚§ãƒƒã‚¯
                        if not (start_date_obj <= bounce_datetime <= end_date_obj + datetime.timedelta(days=1)):
                            continue
                    except Exception:
                        continue

                # ãƒã‚¦ãƒ³ã‚¹ç†ç”±åˆ¥çµ±è¨ˆ
                if not bounce_reason:
                    bounce_reason = 'ä¸æ˜ãªãƒã‚¦ãƒ³ã‚¹ç†ç”±'

                if bounce_reason not in bounce_reasons:
                    bounce_reasons[bounce_reason] = 0
                bounce_reasons[bounce_reason] += 1

                # ãƒã‚¦ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
                if bounce_status.lower() in ['permanent', 'temporary', 'unknown']:
                    bounce_types[bounce_status.lower()] += 1
                else:
                    bounce_types['unknown'] += 1

        return {'reasons': bounce_reasons, 'types': bounce_types}

    except Exception as e:
        logger.error(f"CSV ãƒã‚¦ãƒ³ã‚¹çµ±è¨ˆã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return {'reasons': {}, 'types': {'permanent': 0, 'temporary': 0, 'unknown': 0}}

def get_actual_bounce_statistics():
    """å®Ÿéš›ã®ãƒã‚¦ãƒ³ã‚¹æ¤œçŸ¥ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çµ±è¨ˆã‚’å–å¾—"""
    try:
        total_bounces = 0
        bounce_types = {'permanent': 0, 'temporary': 0, 'unknown': 0}

        # ä¼æ¥­ãƒ‡ãƒ¼ã‚¿CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒã‚¦ãƒ³ã‚¹æƒ…å ±ã‚’èª­ã¿å–ã‚Š
        csv_file = 'data/new_input_test.csv'
        if not os.path.exists(csv_file):
            return {'total_bounces': 0, 'types': bounce_types}

        with open(csv_file, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)

            for row in reader:
                bounce_status = row.get('ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹', '').strip()

                # ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ãŒã‚ã‚‹å ´åˆã¯ã‚«ã‚¦ãƒ³ãƒˆ
                if bounce_status and bounce_status.lower() not in ['', 'none', 'null']:
                    total_bounces += 1

                    # ãƒã‚¦ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
                    if bounce_status.lower() in ['permanent', 'temporary', 'unknown']:
                        bounce_types[bounce_status.lower()] += 1
                    else:
                        bounce_types['unknown'] += 1

        logger.info(f"å®Ÿéš›ã®ãƒã‚¦ãƒ³ã‚¹çµ±è¨ˆ: ç·ãƒã‚¦ãƒ³ã‚¹æ•°={total_bounces}, Permanent={bounce_types['permanent']}, Temporary={bounce_types['temporary']}, Unknown={bounce_types['unknown']}")

        return {
            'total_bounces': total_bounces,
            'types': bounce_types
        }

    except Exception as e:
        logger.error(f"å®Ÿéš›ã®ãƒã‚¦ãƒ³ã‚¹çµ±è¨ˆã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return {'total_bounces': 0, 'types': {'permanent': 0, 'temporary': 0, 'unknown': 0}}

def check_actual_bounce_status(company_id):
    """æŒ‡å®šã•ã‚ŒãŸä¼æ¥­IDã®å®Ÿéš›ã®ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    try:
        csv_file = 'data/new_input_test.csv'
        if not os.path.exists(csv_file):
            return False

        with open(csv_file, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)

            for row in reader:
                if row.get('ID', '').strip() == str(company_id):
                    bounce_status = row.get('ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹', '').strip()
                    return bounce_status and bounce_status.lower() not in ['', 'none', 'null']

        return False

    except Exception as e:
        logger.error(f"ä¼æ¥­ID {company_id} ã®ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def get_bounce_companies_details(start_date, end_date):
    """ãƒã‚¦ãƒ³ã‚¹ä¼æ¥­ã®è©³ç´°æƒ…å ±ã‚’å–å¾—"""
    try:
        bounce_companies = []
        start_date_obj = datetime.datetime.strptime(start_date, '%Y-%m-%d')
        end_date_obj = datetime.datetime.strptime(end_date, '%Y-%m-%d')

        # ä¼æ¥­ãƒ‡ãƒ¼ã‚¿CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒã‚¦ãƒ³ã‚¹ä¼æ¥­ã‚’èª­ã¿å–ã‚Š
        csv_file = 'data/new_input_test.csv'
        if not os.path.exists(csv_file):
            return bounce_companies

        with open(csv_file, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)

            for row in reader:
                bounce_status = row.get('ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹', '').strip()
                bounce_date_str = row.get('ãƒã‚¦ãƒ³ã‚¹æ—¥æ™‚', '').strip()

                # ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if not bounce_status or bounce_status.lower() in ['', 'none', 'null']:
                    continue

                # ãƒã‚¦ãƒ³ã‚¹æ—¥æ™‚ãŒã‚ã‚‹å ´åˆã¯æœŸé–“ãƒã‚§ãƒƒã‚¯
                if bounce_date_str:
                    try:
                        for date_format in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M', '%Y-%m-%d']:
                            try:
                                bounce_datetime = datetime.datetime.strptime(bounce_date_str, date_format)
                                break
                            except ValueError:
                                continue
                        else:
                            continue

                        # æŒ‡å®šæœŸé–“å†…ã‹ãƒã‚§ãƒƒã‚¯
                        if not (start_date_obj <= bounce_datetime <= end_date_obj + datetime.timedelta(days=1)):
                            continue
                    except Exception:
                        continue

                # ãƒã‚¦ãƒ³ã‚¹ä¼æ¥­æƒ…å ±ã‚’è¿½åŠ 
                bounce_companies.append({
                    'id': row.get('ID', ''),
                    'company_name': row.get('ä¼æ¥­å', ''),
                    'email': row.get('æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', ''),
                    'job_position': row.get('å‹Ÿé›†è·ç¨®', ''),
                    'bounce_type': bounce_status,
                    'bounce_date': bounce_date_str,
                    'bounce_reason': row.get('ãƒã‚¦ãƒ³ã‚¹ç†ç”±', ''),
                    'website': row.get('ä¼æ¥­ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸', '')
                })

        # ãƒã‚¦ãƒ³ã‚¹æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
        bounce_companies.sort(key=lambda x: x.get('bounce_date', ''), reverse=True)

        return bounce_companies

    except Exception as e:
        logger.error(f"ãƒã‚¦ãƒ³ã‚¹ä¼æ¥­è©³ç´°ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return []

@app.route('/api/get_daily_stats')
def get_daily_stats_api():
    """æ—¥åˆ¥çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—(AJAXç”¨)"""
    start_date = request.args.get('start_date')
    end_date = request.args.get('end_date')

    daily_stats = get_daily_email_stats(start_date, end_date)
    bounce_reason_stats = get_bounce_reason_statistics(start_date, end_date)

    # æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆ
    sorted_dates = sorted(daily_stats.keys())

    # å®Ÿéš›ã®ãƒã‚¦ãƒ³ã‚¹æ¤œçŸ¥ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    actual_bounce_stats = get_actual_bounce_statistics()

    # åˆè¨ˆçµ±è¨ˆã‚’è¨ˆç®—ï¼ˆçµ±ä¸€é€ä¿¡æ•°ã‚’ä½¿ç”¨ï¼‰
    unified_total_sent = get_unified_sent_email_count()
    total_stats = {
        'total': unified_total_sent,  # çµ±ä¸€ã•ã‚ŒãŸé€ä¿¡æ•°ã‚’ä½¿ç”¨
        'success': sum(daily_stats[date]['success'] for date in daily_stats),
        'bounce': actual_bounce_stats['total_bounces'],  # å®Ÿéš›ã®ãƒã‚¦ãƒ³ã‚¹æ¤œçŸ¥ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        'pending': sum(daily_stats[date]['pending'] for date in daily_stats)
    }

    # æˆåŠŸæ•°ã‚’å†è¨ˆç®—ï¼ˆç·é€ä¿¡æ•° - ãƒã‚¦ãƒ³ã‚¹æ•° - çµæœå¾…ã¡ï¼‰
    total_stats['success'] = total_stats['total'] - total_stats['bounce'] - total_stats['pending']

    # ãƒã‚¦ãƒ³ã‚¹ç‡ã¨æˆåŠŸç‡ã‚’è¨ˆç®—
    if total_stats['total'] > 0:
        total_stats['bounce_rate'] = round((total_stats['bounce'] / total_stats['total']) * 100, 1)
        total_stats['success_rate'] = round((total_stats['success'] / total_stats['total']) * 100, 1)
    else:
        total_stats['bounce_rate'] = 0.0
        total_stats['success_rate'] = 0.0

    # APIç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    result = {
        'dates': sorted_dates,
        'data': [daily_stats[date] for date in sorted_dates],
        'total_stats': total_stats,
        'bounce_reason_stats': bounce_reason_stats.get('reasons', {}),
        'bounce_type_stats': bounce_reason_stats.get('types', {}),
        'total_bounces': bounce_reason_stats.get('total_bounces', 0)
    }

    return jsonify(result)

@app.route('/auto_contact_results')
def auto_contact_results():
    """è‡ªå‹•å•ã„åˆã‚ã›çµæœãƒšãƒ¼ã‚¸"""
    try:
        # è‡ªå‹•å•ã„åˆã‚ã›çµæœã‚’èª­ã¿è¾¼ã¿
        results = load_auto_contact_results()

        # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
        stats = calculate_auto_contact_stats(results)

        return render_template(
            'auto_contact_results.html',
            results=results,
            stats=stats,
            last_updated=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        )

    except Exception as e:
        logger.error(f"è‡ªå‹•å•ã„åˆã‚ã›çµæœãƒšãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e}")
        return render_template('error.html', error=str(e))

def load_auto_contact_results():
    """è‡ªå‹•å•ã„åˆã‚ã›çµæœã‚’èª­ã¿è¾¼ã¿"""
    try:
        results_file = 'auto_contact_results.csv'

        if not os.path.exists(results_file):
            return []

        results = []
        with open(results_file, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                results.append(row)

        # å‡¦ç†æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
        results.sort(key=lambda x: x.get('å‡¦ç†æ—¥æ™‚', ''), reverse=True)

        return results

    except Exception as e:
        logger.error(f"è‡ªå‹•å•ã„åˆã‚ã›çµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def calculate_auto_contact_stats(results):
    """è‡ªå‹•å•ã„åˆã‚ã›çµ±è¨ˆã‚’è¨ˆç®—"""
    try:
        if not results:
            return {
                'total_processed': 0,
                'contact_page_found': 0,
                'form_detected': 0,
                'form_filled': 0,
                'form_submitted': 0,
                'confirmation_received': 0,
                'success_rate': 0.0,
                'recent_results': []
            }

        total = len(results)
        contact_page_found = sum(1 for r in results if r.get('å•ã„åˆã‚ã›ãƒšãƒ¼ã‚¸æ¤œå‡º') == 'True')
        form_detected = sum(1 for r in results if r.get('ãƒ•ã‚©ãƒ¼ãƒ æ¤œå‡º') == 'True')
        form_filled = sum(1 for r in results if r.get('ãƒ•ã‚©ãƒ¼ãƒ å…¥åŠ›') == 'True')
        form_submitted = sum(1 for r in results if r.get('ãƒ•ã‚©ãƒ¼ãƒ é€ä¿¡') == 'True')
        confirmation_received = sum(1 for r in results if r.get('ç¢ºèªãƒ¡ãƒ¼ãƒ«å—ä¿¡') == 'True')

        success_rate = (form_filled / total * 100) if total > 0 else 0.0

        # æœ€è¿‘ã®çµæœï¼ˆæœ€æ–°10ä»¶ï¼‰
        recent_results = results[:10]

        return {
            'total_processed': total,
            'contact_page_found': contact_page_found,
            'form_detected': form_detected,
            'form_filled': form_filled,
            'form_submitted': form_submitted,
            'confirmation_received': confirmation_received,
            'success_rate': round(success_rate, 1),
            'recent_results': recent_results
        }

    except Exception as e:
        logger.error(f"è‡ªå‹•å•ã„åˆã‚ã›çµ±è¨ˆè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'total_processed': 0,
            'contact_page_found': 0,
            'form_detected': 0,
            'form_filled': 0,
            'form_submitted': 0,
            'confirmation_received': 0,
            'success_rate': 0.0,
            'recent_results': []
        }

@app.route('/api/get_progress')
def get_progress():
    """é€²æ—æƒ…å ±ã‚’å–å¾—ï¼ˆAJAXç”¨ï¼‰"""
    try:
        progress_data = load_progress()
        return jsonify(progress_data)
    except Exception as e:
        logger.error(f"é€²æ—æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/get_email_stats')
def get_email_stats():
    """ãƒ¡ãƒ¼ãƒ«çµ±è¨ˆæƒ…å ±ã‚’å–å¾—ï¼ˆAJAXç”¨ï¼‰"""
    try:
        stats = get_basic_stats()
        return jsonify(stats)
    except Exception as e:
        logger.error(f"ãƒ¡ãƒ¼ãƒ«çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'error': str(e)}), 500

# /api/bounce-details ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¯å‰Šé™¤ã•ã‚Œã¾ã—ãŸï¼ˆbounce-analysis ãƒšãƒ¼ã‚¸ãŒä¸è¦ãªãŸã‚ï¼‰

# bounce-analysis ãƒšãƒ¼ã‚¸ã¯å‰Šé™¤ã•ã‚Œã¾ã—ãŸï¼ˆä¸è¦ãªãŸã‚ï¼‰

@app.route('/api/get_processes')
def get_processes():
    """ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±ã‚’å–å¾—ï¼ˆAJAXç”¨ï¼‰"""
    try:
        # ğŸ†• ãƒ—ãƒ­ã‚»ã‚¹çŠ¶æ…‹åŒæœŸã‚’å®Ÿè¡Œ
        sync_process_states()

        # ğŸ†• ç›£è¦–ã•ã‚Œã¦ã„ãªã„ãƒ—ãƒ­ã‚»ã‚¹ã«ç›£è¦–ã‚’è¿½åŠ 
        fix_unmonitored_processes()

        processes = []
        current_time = datetime.datetime.now()

        for pid, info in running_processes.items():
            # å®Ÿè¡Œä¸­ã®ãƒ—ãƒ­ã‚»ã‚¹ã¾ãŸã¯æœ€è¿‘å®Œäº†ã—ãŸãƒ—ãƒ­ã‚»ã‚¹ã‚’è¡¨ç¤º
            show_process = False

            if info['status'] == 'running':
                show_process = True
            elif info['status'] in ['completed', 'failed', 'error']:
                # å®Œäº†ã‹ã‚‰5åˆ†ä»¥å†…ã®ãƒ—ãƒ­ã‚»ã‚¹ã¯è¡¨ç¤ºã‚’ç¶™ç¶š
                end_time = info.get('end_time')
                if end_time and (current_time - end_time).total_seconds() < 300:  # 5åˆ† = 300ç§’
                    show_process = True

            if show_process:
                # ãƒ—ãƒ­ã‚»ã‚¹èª¬æ˜ã‚’ç”Ÿæˆ
                description = get_process_description(info['command'], info.get('args', ''))

                # å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
                if info['status'] == 'running':
                    duration = str(current_time - info['start_time']).split('.')[0]
                else:
                    end_time = info.get('end_time', current_time)
                    duration = str(end_time - info['start_time']).split('.')[0]

                processes.append({
                    'id': pid,
                    'command': info['command'],
                    'args': info.get('args_str', info.get('args', '')),
                    'description': description,
                    'start_time': info['start_time'].strftime('%Y-%m-%d %H:%M:%S'),
                    'status': info['status'],
                    'duration': duration,
                    'output': info.get('output', ''),  # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡ºåŠ›ã‚’è¿½åŠ 
                    'error': info.get('error', '')  # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚‚è¿½åŠ 
                })

        return jsonify(processes)
    except Exception as e:
        logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify([])

@app.route('/api/get_process_output/<process_id>')
def get_process_output(process_id):
    """ç‰¹å®šã®ãƒ—ãƒ­ã‚»ã‚¹ã®å‡ºåŠ›ã‚’å–å¾—"""
    try:
        if process_id in running_processes:
            process_info = running_processes[process_id]
            return jsonify({
                'success': True,
                'process_id': process_id,
                'output': process_info.get('output', ''),
                'status': process_info.get('status', 'unknown'),
                'start_time': process_info['start_time'].strftime('%Y-%m-%d %H:%M:%S'),
                'duration': str(datetime.datetime.now() - process_info['start_time']).split('.')[0],
                'error': process_info.get('error', '')
            })
        else:
            return jsonify({
                'success': False,
                'message': 'ãƒ—ãƒ­ã‚»ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
            })
    except Exception as e:
        logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹å‡ºåŠ›å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'message': f'ã‚¨ãƒ©ãƒ¼: {e}'
        })



@app.route('/api/process_batch_range', methods=['POST'])
def process_batch_range():
    """100ç¤¾å˜ä½ã§ã®ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œ"""
    try:
        data = request.get_json() if request.is_json else request.form
        start_id = int(data.get('start_id', 1))
        end_id = int(data.get('end_id', 100))
        batch_size = int(data.get('batch_size', 20))
        process_type = data.get('process_type', 'full_workflow')  # full_workflow, email_extraction, website_analysis, email_sending

        range_size = end_id - start_id + 1
        logger.info(f"ãƒãƒƒãƒå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™: ID {start_id}-{end_id} ({range_size}ç¤¾), ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")

        # HUGANJOBå°‚ç”¨ãƒ—ãƒ­ã‚»ã‚¹ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰
        if process_type == 'huganjob_email_resolution':
            command = 'huganjob_email_address_resolver.py'
            args = f'--start-id {start_id} --end-id {end_id}'
        elif process_type == 'huganjob_bulk_sending':
            command = 'huganjob_bulk_email_sender.py'
            args = f'--start-id {start_id} --end-id {end_id} --max-emails {end_id - start_id + 1}'
        elif process_type == 'huganjob_test_sending':
            command = 'huganjob_test_sender.py'
            args = f'--start-id {start_id} --end-id {end_id}'
        elif process_type == 'huganjob_full_workflow':
            command = 'huganjob_bulk_email_sender.py'
            args = f'--start-id {start_id} --end-id {end_id}'
        else:
            return jsonify({'success': False, 'message': f'ä¸æ˜ãªHUGANJOBãƒ—ãƒ­ã‚»ã‚¹ã‚¿ã‚¤ãƒ—: {process_type}'})

        # ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œ
        process_id = run_process(command, args)
        if not process_id:
            return jsonify({'success': False, 'message': 'ãƒãƒƒãƒå‡¦ç†ã®é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸ'})

        # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
        monitor_thread = threading.Thread(target=monitor_process, args=(process_id,))
        monitor_thread.daemon = True
        monitor_thread.start()

        logger.info(f"ãƒãƒƒãƒå‡¦ç†ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ã‚’é–‹å§‹ã—ã¾ã—ãŸ: {process_type}")

        return jsonify({
            'success': True,
            'message': f'{process_type}ãƒãƒƒãƒå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã—ãŸ (ID {start_id}-{end_id}, {range_size}ç¤¾)',
            'process_id': process_id,
            'process_type': process_type,
            'range_size': range_size,
            'batch_size': batch_size,
            'estimated_batches': (range_size + batch_size - 1) // batch_size
        })

    except Exception as e:
        logger.error(f"ãƒãƒƒãƒå‡¦ç†é–‹å§‹ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return jsonify({
            'success': False,
            'message': f'ãƒãƒƒãƒå‡¦ç†é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}'
        }), 500

@app.route('/api/reset_progress', methods=['POST'])
def reset_progress():
    """é€²æ—æƒ…å ±ã‚’ãƒªã‚»ãƒƒãƒˆ"""
    try:
        # é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–
        progress = {
            "extract_emails": {"status": "not_started", "progress": 0, "last_updated": None},
            "analyze_websites": {"status": "not_started", "progress": 0, "last_updated": None},
            "prepare_email_content": {"status": "not_started", "progress": 0, "last_updated": None},
            "send_emails": {"status": "not_started", "progress": 0, "last_updated": None},
            "process_bounces": {"status": "not_started", "progress": 0, "last_updated": None},
            "analyze_results": {"status": "not_started", "progress": 0, "last_updated": None}
        }

        with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)

        logger.info("é€²æ—æƒ…å ±ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")

        return jsonify({
            'success': True,
            'message': 'é€²æ—æƒ…å ±ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ'
        })
    except Exception as e:
        logger.error(f"é€²æ—ãƒªã‚»ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'message': f'é€²æ—ãƒªã‚»ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}'
        }), 500

@app.route('/api/refresh', methods=['POST'])
def api_refresh_data():
    """ãƒ‡ãƒ¼ã‚¿ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ï¼‰"""
    try:
        clear_all_caches()
        return jsonify({
            'success': True,
            'message': 'ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ'
        })
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/consolidate_files', methods=['POST'])
def consolidate_files():
    """ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆ"""
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆå‡¦ç†ï¼ˆå®Ÿè£…ã¯çœç•¥ï¼‰
        return jsonify({
            'success': True,
            'message': 'ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆãŒå®Œäº†ã—ã¾ã—ãŸ',
            'output_file': 'new_website_analysis_results_latest.csv',
            'archived_count': 0
        })
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return jsonify({
            'success': False,
            'message': f'ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆã‚¨ãƒ©ãƒ¼: {e}'
        }), 500

@app.route('/api/companies')
def get_companies_api():
    """ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆAPIç”¨ï¼‰"""
    try:
        companies = load_company_data()
        return jsonify({
            'success': True,
            'companies': companies,
            'total': len(companies)
        })
    except Exception as e:
        logger.error(f"ä¼æ¥­ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# manual ãƒšãƒ¼ã‚¸ã¯å‰Šé™¤ã•ã‚Œã¾ã—ãŸï¼ˆä¸è¦ãªãŸã‚ï¼‰

# ===== å•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ è‡ªå‹•å…¥åŠ›æ©Ÿèƒ½ =====

@app.route('/contact-form')
def contact_form_page():
    """å•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ è‡ªå‹•å…¥åŠ›ãƒšãƒ¼ã‚¸"""
    try:
        # ãƒã‚¦ãƒ³ã‚¹ä¼æ¥­æ•°ã®å–å¾—ï¼ˆç°¡ç•¥åŒ–ï¼‰
        bounce_stats = {'total_candidates': 0, 'valid_bounces': 0}

        # å•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ å‡¦ç†çµæœã®å–å¾—
        contact_form_stats = get_contact_form_stats()

        return render_template(
            'contact_form.html',
            bounce_stats=bounce_stats,
            contact_form_stats=contact_form_stats,
            last_updated=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        )
    except Exception as e:
        logger.error(f"å•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e}")
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", 500

@app.route('/api/contact_form_execute', methods=['POST'])
def contact_form_execute():
    """å•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ è‡ªå‹•å…¥åŠ›ã‚’å®Ÿè¡Œ"""
    try:
        data = request.get_json() if request.is_json else request.form
        start_id = int(data.get('start_id', 1))
        end_id = int(data.get('end_id', 10))
        max_companies = int(data.get('max_companies', 10))
        test_mode = data.get('test_mode', 'true').lower() == 'true'

        # ãƒ—ãƒ­ã‚»ã‚¹IDã®ç”Ÿæˆ
        process_id = str(uuid.uuid4())

        # ã‚³ãƒãƒ³ãƒ‰ã®æ§‹ç¯‰
        command = f"python contact_form_automation.py --start-id {start_id} --end-id {end_id} --max-companies {max_companies}"
        if test_mode:
            command += " --test-mode"
        command += " --headless"

        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹
        def run_contact_form_process():
            try:
                logger.info(f"å•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ å‡¦ç†é–‹å§‹: {command}")

                # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±ã‚’è¨˜éŒ²
                running_processes[process_id] = {
                    'id': process_id,
                    'command': command,
                    'description': f"å•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ è‡ªå‹•å…¥åŠ› (ID {start_id}-{end_id})",
                    'start_time': datetime.datetime.now(),
                    'status': 'running',
                    'output': [],
                    'pid': None
                }

                # å®Ÿéš›ã®ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ
                process = subprocess.Popen(
                    command,
                    shell=True,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    encoding='utf-8',
                    cwd=os.getcwd()
                )

                running_processes[process_id]['pid'] = process.pid

                # å‡ºåŠ›ã‚’èª­ã¿å–ã‚Š
                for line in iter(process.stdout.readline, ''):
                    if line:
                        running_processes[process_id]['output'].append(line.strip())
                        logger.info(f"å•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ å‡¦ç†: {line.strip()}")

                # ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†
                process.wait()

                if process.returncode == 0:
                    running_processes[process_id]['status'] = 'completed'
                    logger.info(f"å•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ å‡¦ç†å®Œäº†: {process_id}")
                else:
                    running_processes[process_id]['status'] = 'failed'
                    logger.error(f"å•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ å‡¦ç†å¤±æ•—: {process_id}")

                running_processes[process_id]['end_time'] = datetime.datetime.now()

                # å±¥æ­´ã«è¿½åŠ 
                process_history.append(running_processes[process_id].copy())

                # ä¸€å®šæ™‚é–“å¾Œã«running_processesã‹ã‚‰å‰Šé™¤
                threading.Timer(300, lambda: running_processes.pop(process_id, None)).start()

            except Exception as e:
                logger.error(f"å•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                if process_id in running_processes:
                    running_processes[process_id]['status'] = 'failed'
                    running_processes[process_id]['error'] = str(e)

        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
        thread = threading.Thread(target=run_contact_form_process)
        thread.daemon = True
        thread.start()

        return jsonify({
            'success': True,
            'message': 'å•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ è‡ªå‹•å…¥åŠ›ã‚’é–‹å§‹ã—ã¾ã—ãŸ',
            'process_id': process_id
        })

    except Exception as e:
        logger.error(f"å•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'message': f'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'
        }), 500

def get_contact_form_stats():
    """å•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ å‡¦ç†çµ±è¨ˆã‚’å–å¾—"""
    try:
        stats = {
            'total_processed': 0,
            'form_detected': 0,
            'form_filled': 0,
            'form_submitted': 0,
            'success_rate': 0,
            'recent_results': []
        }

        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢
        results_dir = 'contact_form_results'
        if os.path.exists(results_dir):
            result_files = [f for f in os.listdir(results_dir) if f.endswith('.csv')]

            if result_files:
                # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
                latest_file = max(result_files)
                file_path = os.path.join(results_dir, latest_file)

                df = pd.read_csv(file_path, encoding='utf-8-sig')

                stats['total_processed'] = len(df)
                stats['form_detected'] = len(df[df['ãƒ•ã‚©ãƒ¼ãƒ æ¤œå‡º'] == True])
                stats['form_filled'] = len(df[df['ãƒ•ã‚©ãƒ¼ãƒ å…¥åŠ›'] == True])
                stats['form_submitted'] = len(df[df['ãƒ•ã‚©ãƒ¼ãƒ é€ä¿¡'] == True])

                if stats['total_processed'] > 0:
                    stats['success_rate'] = (stats['form_submitted'] / stats['total_processed']) * 100

                # æœ€æ–°ã®çµæœï¼ˆæœ€å¤§10ä»¶ï¼‰
                stats['recent_results'] = df.tail(10).to_dict('records')

        return stats

    except Exception as e:
        logger.error(f"å•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'total_processed': 0,
            'form_detected': 0,
            'form_filled': 0,
            'form_submitted': 0,
            'success_rate': 0,
            'recent_results': []
        }



def record_email_open(tracking_id, request_obj):
    """ãƒ¡ãƒ¼ãƒ«é–‹å°ã‚’è¨˜éŒ²ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
    try:
        # æ—¢ã«é–‹å°è¨˜éŒ²ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if is_already_opened(tracking_id):
            logger.info(f"æ—¢ã«é–‹å°æ¸ˆã¿: {tracking_id}")
            return

        # ãƒ‡ãƒã‚¤ã‚¹ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
        user_agent = request_obj.environ.get('HTTP_USER_AGENT', '')
        device_type = detect_device_type(user_agent)

        # é–‹å°æƒ…å ±ã‚’æº–å‚™ï¼ˆç§’ã¾ã§å«ã‚€æ­£ç¢ºãªæ™‚åˆ»ï¼‰
        now = datetime.datetime.now()
        open_record = {
            'tracking_id': tracking_id,
            'opened_at': now.strftime('%Y-%m-%d %H:%M:%S'),
            'ip_address': request_obj.environ.get('REMOTE_ADDR', ''),
            'device_type': device_type,
            'user_agent': user_agent[:200] if user_agent else ''  # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
        }

        # é–‹å°è¿½è·¡ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²
        save_email_open_record(open_record)

        logger.info(f"ãƒ¡ãƒ¼ãƒ«é–‹å°ã‚’è¨˜éŒ²ã—ã¾ã—ãŸ: {tracking_id} at {now}")

    except Exception as e:
        logger.error(f"é–‹å°è¨˜éŒ²ã®ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

def is_already_opened(tracking_id):
    """æŒ‡å®šã•ã‚ŒãŸãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDãŒæ—¢ã«é–‹å°æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        if not os.path.exists(NEW_EMAIL_OPEN_TRACKING):
            return False

        with open(NEW_EMAIL_OPEN_TRACKING, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row.get('tracking_id', '') == tracking_id:
                    return True
        return False
    except Exception as e:
        logger.error(f"é–‹å°çŠ¶æ³ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def detect_device_type(user_agent):
    """User-Agentã‹ã‚‰ãƒ‡ãƒã‚¤ã‚¹ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
    if not user_agent:
        return 'Unknown'

    user_agent_lower = user_agent.lower()

    if 'mobile' in user_agent_lower or 'iphone' in user_agent_lower or 'android' in user_agent_lower:
        return 'Mobile'
    elif 'tablet' in user_agent_lower or 'ipad' in user_agent_lower:
        return 'Tablet'
    elif 'windows' in user_agent_lower or 'macintosh' in user_agent_lower or 'linux' in user_agent_lower:
        return 'Desktop'
    else:
        return 'Unknown'

def save_email_open_record(open_record):
    """é–‹å°è¨˜éŒ²ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½œæˆ
        file_exists = os.path.exists(NEW_EMAIL_OPEN_TRACKING)

        with open(NEW_EMAIL_OPEN_TRACKING, 'a', newline='', encoding='utf-8-sig') as f:
            fieldnames = [
                'tracking_id', 'opened_at', 'ip_address', 'device_type', 'user_agent'
            ]
            writer = csv.DictWriter(f, fieldnames=fieldnames)

            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›¸ãè¾¼ã¿ï¼ˆæ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼‰
            if not file_exists:
                writer.writeheader()

            # é–‹å°è¨˜éŒ²ã‚’æ›¸ãè¾¼ã¿ï¼ˆå¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿ï¼‰
            filtered_record = {
                'tracking_id': open_record.get('tracking_id', ''),
                'opened_at': open_record.get('opened_at', ''),
                'ip_address': open_record.get('ip_address', ''),
                'device_type': open_record.get('device_type', 'Unknown'),
                'user_agent': open_record.get('user_agent', '')
            }
            writer.writerow(filtered_record)

    except Exception as e:
        logger.error(f"é–‹å°è¨˜éŒ²ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")



def get_sent_emails_count():
    """é€ä¿¡æ¸ˆã¿ãƒ¡ãƒ¼ãƒ«æ•°ã‚’å–å¾—"""
    try:
        if os.path.exists(NEW_EMAIL_SENDING_RESULTS):
            with open(NEW_EMAIL_SENDING_RESULTS, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                return len(list(reader))
        return 0
    except Exception as e:
        logger.error(f"é€ä¿¡æ¸ˆã¿ãƒ¡ãƒ¼ãƒ«æ•°ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return 0

# === çµæœç®¡ç†ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ ===

# çµæœç®¡ç†ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
def load_managed_results():
    """çµæœç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰çµ±åˆçµæœã‚’èª­ã¿è¾¼ã¿"""
    try:
        from result_management_system import ResultManager
        rm = ResultManager()

        # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        dashboard_files = rm.create_dashboard_files()

        # çµ±åˆçµæœã®çµ±è¨ˆã‚’å–å¾—
        report = rm.generate_status_report()

        return {
            "files": dashboard_files,
            "report": report,
            "success": True
        }
    except Exception as e:
        logger.error(f"ç®¡ç†çµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {"success": False, "error": str(e)}

def get_processing_status():
    """å‡¦ç†çŠ¶æ³ã‚’å–å¾—"""
    try:
        from result_management_system import ResultManager
        rm = ResultManager()
        return rm.generate_status_report()
    except Exception as e:
        logger.error(f"å‡¦ç†çŠ¶æ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def get_missing_ranges():
    """æœªå‡¦ç†ç¯„å›²ã‚’å–å¾—"""
    try:
        from result_management_system import ResultManager
        rm = ResultManager()
        return rm.get_missing_ranges()
    except Exception as e:
        logger.error(f"æœªå‡¦ç†ç¯„å›²å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

@app.route('/api/process_bounces', methods=['POST'])
def process_bounces():
    """ç‹¬ç«‹ã—ãŸãƒã‚¦ãƒ³ã‚¹å‡¦ç†ã‚’å®Ÿè¡Œï¼ˆåˆ†é›¢é‹ç”¨ç‰ˆï¼‰"""
    try:
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å–å¾—
        data = request.get_json() or {}
        days = data.get('days', 30)
        test_mode = data.get('test_mode', False)
        force_reprocess = data.get('force_reprocess', False)
        reset_tracking = data.get('reset_tracking', False)

        logger.info(f"ç‹¬ç«‹ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ã‚’é–‹å§‹ï¼ˆåˆ†é›¢é‹ç”¨ç‰ˆï¼‰: æ¤œç´¢æœŸé–“={days}æ—¥, ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰={test_mode}, å¼·åˆ¶å†å‡¦ç†={force_reprocess}, è¿½è·¡ãƒªã‚»ãƒƒãƒˆ={reset_tracking}")

        # åˆ†é›¢é‹ç”¨ç‰ˆã®ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰
        command = "standalone_bounce_processor.py"
        args = f"--days {days}"
        if test_mode:
            args += " --test-mode"
        if force_reprocess:
            args += " --force-reprocess"
        if reset_tracking:
            args += " --reset-tracking"

        # æ—¢å­˜ã®ãƒ—ãƒ­ã‚»ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
        process_id = run_process(command, args)
        if not process_id:
            return jsonify({
                'success': False,
                'message': 'ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ã®é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸ'
            }), 500

        # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
        monitor_thread = threading.Thread(target=monitor_process, args=(process_id,))
        monitor_thread.daemon = True
        monitor_thread.start()

        logger.info(f"ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ã‚’é–‹å§‹ã—ã¾ã—ãŸ")

        return jsonify({
            'success': True,
            'message': f"ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã—ãŸï¼ˆåˆ†é›¢é‹ç”¨ç‰ˆãƒ»ãƒ—ãƒ­ã‚»ã‚¹ID: {process_id}ï¼‰",
            'process_id': process_id,
            'command': f"python {command} {args}",
            'separation_mode': True
        })

    except Exception as e:
        error_msg = f"ãƒã‚¦ãƒ³ã‚¹å‡¦ç†é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}"
        logger.error(error_msg)
        return jsonify({
            'success': False,
            'message': error_msg
        }), 500

@app.route('/api/bounce_status', methods=['GET'])
def bounce_status():
    """ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ã®çŠ¶æ³ã‚’å–å¾—"""
    try:
        # æœ€æ–°ã®ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆã‚’æ¤œç´¢
        import glob
        report_files = glob.glob('bounce_processing_report_*.json')

        if not report_files:
            return jsonify({
                'success': True,
                'message': 'ãƒã‚¦ãƒ³ã‚¹å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“',
                'has_report': False
            })

        # æœ€æ–°ã®ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        latest_report = max(report_files, key=os.path.getctime)

        with open(latest_report, 'r', encoding='utf-8') as f:
            report_data = json.load(f)

        return jsonify({
            'success': True,
            'has_report': True,
            'report': report_data,
            'report_file': latest_report
        })

    except Exception as e:
        error_msg = f"ãƒã‚¦ãƒ³ã‚¹çŠ¶æ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}"
        logger.error(error_msg)
        return jsonify({
            'success': False,
            'message': error_msg
        })

@app.route('/api/process_auto_contact', methods=['POST'])
def process_auto_contact():
    """ç‹¬ç«‹ã—ãŸè‡ªå‹•å•ã„åˆã‚ã›å‡¦ç†ã‚’å®Ÿè¡Œï¼ˆåˆ†é›¢é‹ç”¨ç‰ˆï¼‰"""
    try:
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å–å¾—
        data = request.get_json() or {}
        days = data.get('days', 7)
        max_companies = data.get('max_companies', 10)
        test_mode = data.get('test_mode', True)

        logger.info(f"ç‹¬ç«‹è‡ªå‹•å•ã„åˆã‚ã›å‡¦ç†ã‚’é–‹å§‹ï¼ˆåˆ†é›¢é‹ç”¨ç‰ˆï¼‰: æ¤œç´¢æœŸé–“={days}æ—¥, æœ€å¤§å‡¦ç†æ•°={max_companies}, ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰={test_mode}")

        # åˆ†é›¢é‹ç”¨ç‰ˆã®è‡ªå‹•å•ã„åˆã‚ã›ã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰
        command = "standalone_auto_contact.py"
        args = f"--days {days}"
        if max_companies:
            args += f" --max-companies {max_companies}"
        if test_mode:
            args += " --test-mode"

        # æ—¢å­˜ã®ãƒ—ãƒ­ã‚»ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
        process_id = run_process(command, args)
        if not process_id:
            return jsonify({
                'success': False,
                'message': 'è‡ªå‹•å•ã„åˆã‚ã›å‡¦ç†ã®é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸ'
            }), 500

        # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
        monitor_thread = threading.Thread(target=monitor_process, args=(process_id,))
        monitor_thread.daemon = True
        monitor_thread.start()

        logger.info(f"è‡ªå‹•å•ã„åˆã‚ã›å‡¦ç†ãƒ—ãƒ­ã‚»ã‚¹ {process_id} ã‚’é–‹å§‹ã—ã¾ã—ãŸ")

        return jsonify({
            'success': True,
            'message': f"è‡ªå‹•å•ã„åˆã‚ã›å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã—ãŸï¼ˆåˆ†é›¢é‹ç”¨ç‰ˆãƒ»ãƒ—ãƒ­ã‚»ã‚¹ID: {process_id}ï¼‰",
            'process_id': process_id,
            'command': f"python {command} {args}",
            'separation_mode': True
        })

    except Exception as e:
        error_msg = f"è‡ªå‹•å•ã„åˆã‚ã›å‡¦ç†é–‹å§‹ã‚¨ãƒ©ãƒ¼ï¼ˆåˆ†é›¢é‹ç”¨ç‰ˆï¼‰: {e}"
        logger.error(error_msg)
        return jsonify({
            'success': False,
            'message': error_msg
        }), 500

@app.route('/bounce-processing')
def bounce_processing():
    """ãƒã‚¦ãƒ³ã‚¹å‡¦ç†å°‚ç”¨ãƒšãƒ¼ã‚¸ï¼ˆåˆ†é›¢é‹ç”¨ç‰ˆï¼‰"""
    return render_template(
        'bounce_processing.html',
        last_updated=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        separation_mode=True
    )

@app.route('/auto-contact-processing')
def auto_contact_processing():
    """è‡ªå‹•å•ã„åˆã‚ã›å‡¦ç†å°‚ç”¨ãƒšãƒ¼ã‚¸ï¼ˆåˆ†é›¢é‹ç”¨ç‰ˆï¼‰"""
    return render_template(
        'auto_contact_processing.html',
        last_updated=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        separation_mode=True
    )

# ===== é–‹å°ç‡ç®¡ç†æ©Ÿèƒ½ =====

@app.route('/track-open/<tracking_id>')
def track_email_open(tracking_id):
    """ãƒ¡ãƒ¼ãƒ«é–‹å°è¿½è·¡ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    try:
        # è¿½è·¡æ–¹æ³•ã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯pixelï¼‰
        tracking_method = request.args.get('method', 'pixel')

        # é–‹å°æƒ…å ±ã‚’è¨˜éŒ²ï¼ˆè¿½è·¡æ–¹æ³•ã‚‚è¨˜éŒ²ï¼‰
        record_email_open_enhanced(tracking_id, request, tracking_method)

        # 1x1ãƒ”ã‚¯ã‚»ãƒ«ã®é€æ˜ç”»åƒã‚’è¿”ã™
        from flask import Response
        import base64

        # 1x1é€æ˜GIFç”»åƒã®base64ãƒ‡ãƒ¼ã‚¿
        pixel_data = base64.b64decode('R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')

        return Response(
            pixel_data,
            mimetype='image/gif',
            headers={
                'Cache-Control': 'no-cache, no-store, must-revalidate',
                'Pragma': 'no-cache',
                'Expires': '0',
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Methods': 'GET',
                'Access-Control-Allow-Headers': 'Content-Type'
            }
        )
    except Exception as e:
        logger.error(f"é–‹å°è¿½è·¡ã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼ã§ã‚‚1x1ãƒ”ã‚¯ã‚»ãƒ«ã‚’è¿”ã™
        pixel_data = base64.b64decode('R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')
        return Response(pixel_data, mimetype='image/gif')


@app.route('/track-beacon/<tracking_id>', methods=['POST'])
def track_email_beacon(tracking_id):
    """ãƒ“ãƒ¼ã‚³ãƒ³APIè¿½è·¡ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    try:
        # ãƒ“ãƒ¼ã‚³ãƒ³ã«ã‚ˆã‚‹é–‹å°è¨˜éŒ²
        record_email_open_enhanced(tracking_id, request, 'beacon')
        return jsonify({'status': 'success'}), 200
    except Exception as e:
        logger.error(f"ãƒ“ãƒ¼ã‚³ãƒ³è¿½è·¡ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'status': 'error'}), 500

@app.route('/track/<tracking_id>')
def track_email_fallback(tracking_id):
    """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¿½è·¡ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        record_email_open_enhanced(tracking_id, request, 'fallback')

        # 1x1ãƒ”ã‚¯ã‚»ãƒ«ç”»åƒã‚’è¿”ã™
        pixel_data = base64.b64decode('R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')
        return Response(
            pixel_data,
            mimetype='image/gif',
            headers={
                'Cache-Control': 'no-cache, no-store, must-revalidate',
                'Pragma': 'no-cache',
                'Expires': '0'
            }
        )
    except Exception as e:
        logger.error(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¿½è·¡ã‚¨ãƒ©ãƒ¼: {e}")
        pixel_data = base64.b64decode('R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')
        return Response(pixel_data, mimetype='image/gif')

@app.route('/track-css/<tracking_id>')
def track_email_css(tracking_id):
    """CSSèƒŒæ™¯ç”»åƒè¿½è·¡ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        record_email_open_enhanced(tracking_id, request, 'css')

        # 1x1ãƒ”ã‚¯ã‚»ãƒ«ç”»åƒã‚’è¿”ã™
        pixel_data = base64.b64decode('R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')
        return Response(
            pixel_data,
            mimetype='image/gif',
            headers={
                'Cache-Control': 'no-cache, no-store, must-revalidate',
                'Pragma': 'no-cache',
                'Expires': '0'
            }
        )
    except Exception as e:
        logger.error(f"CSSè¿½è·¡ã‚¨ãƒ©ãƒ¼: {e}")
        pixel_data = base64.b64decode('R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')
        return Response(pixel_data, mimetype='image/gif')

@app.route('/track-xhr/<tracking_id>', methods=['POST'])
def track_email_xhr(tracking_id):
    """XMLHttpRequestè¿½è·¡ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        record_email_open_enhanced(tracking_id, request, 'xhr')
        return jsonify({'status': 'success'}), 200
    except Exception as e:
        logger.error(f"XHRè¿½è·¡ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'status': 'error'}), 500

@app.route('/track-unload/<tracking_id>', methods=['POST'])
def track_email_unload(tracking_id):
    """ãƒšãƒ¼ã‚¸é›¢è„±æ™‚è¿½è·¡ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        record_email_open_enhanced(tracking_id, request, 'unload')
        return jsonify({'status': 'success'}), 200
    except Exception as e:
        logger.error(f"é›¢è„±æ™‚è¿½è·¡ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'status': 'error'}), 500

@app.route('/track-focus/<tracking_id>', methods=['POST'])
def track_email_focus(tracking_id):
    """ãƒ•ã‚©ãƒ¼ã‚«ã‚¹æ™‚è¿½è·¡ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        record_email_open_enhanced(tracking_id, request, 'focus')
        return jsonify({'status': 'success'}), 200
    except Exception as e:
        logger.error(f"ãƒ•ã‚©ãƒ¼ã‚«ã‚¹æ™‚è¿½è·¡ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'status': 'error'}), 500

@app.route('/test-tracking/<tracking_id>')
def test_tracking(tracking_id):
    """é–‹å°è¿½è·¡ãƒ†ã‚¹ãƒˆç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        # ãƒ†ã‚¹ãƒˆç”¨ã®é–‹å°è¨˜éŒ²
        record_email_open_enhanced(tracking_id, request, 'test')

        return f"""
        <html>
        <head><title>é–‹å°è¿½è·¡ãƒ†ã‚¹ãƒˆ</title></head>
        <body>
            <h1>é–‹å°è¿½è·¡ãƒ†ã‚¹ãƒˆå®Œäº†</h1>
            <p>ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ID: {tracking_id}</p>
            <p>é–‹å°è¨˜éŒ²ãŒæ­£å¸¸ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚</p>
            <a href="/open-rate-analytics">é–‹å°ç‡åˆ†æãƒšãƒ¼ã‚¸ã§ç¢ºèª</a>
        </body>
        </html>
        """
    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆè¿½è·¡ã‚¨ãƒ©ãƒ¼: {e}")
        return f"ã‚¨ãƒ©ãƒ¼: {e}", 500

def record_email_open_enhanced(tracking_id, request_obj, tracking_method='pixel'):
    """ãƒ¡ãƒ¼ãƒ«é–‹å°ã‚’è¨˜éŒ²ã™ã‚‹ï¼ˆæ”¹å–„ç‰ˆãƒ»å¤šé‡è¿½è·¡å¯¾å¿œï¼‰"""
    try:
        # æ—¢ã«åŒã˜æ–¹æ³•ã§é–‹å°è¨˜éŒ²ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if is_already_opened_by_method(tracking_id, tracking_method):
            logger.info(f"æ—¢ã«é–‹å°æ¸ˆã¿ ({tracking_method}): {tracking_id}")
            return

        # ãƒ‡ãƒã‚¤ã‚¹ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®šï¼ˆæ”¹å–„ç‰ˆï¼‰
        user_agent = request_obj.environ.get('HTTP_USER_AGENT', '') if hasattr(request_obj, 'environ') else request_obj.headers.get('User-Agent', '')
        device_type = detect_device_type_enhanced(user_agent)

        # IPã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å–å¾—
        ip_address = request_obj.environ.get('REMOTE_ADDR', '') if hasattr(request_obj, 'environ') else request_obj.remote_addr or ''

        # ãƒªãƒ•ã‚¡ãƒ©ãƒ¼ã‚’å–å¾—
        referer = request_obj.environ.get('HTTP_REFERER', '') if hasattr(request_obj, 'environ') else request_obj.headers.get('Referer', '')

        # é–‹å°æƒ…å ±ã‚’æº–å‚™ï¼ˆæ‹¡å¼µãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä»˜ãï¼‰
        now = datetime.datetime.now()
        open_record = {
            'tracking_id': tracking_id,
            'opened_at': now.strftime('%Y-%m-%d %H:%M:%S'),
            'ip_address': ip_address,
            'device_type': device_type,
            'user_agent': user_agent[:200] if user_agent else '',
            'tracking_method': tracking_method,
            'referer': referer[:200] if referer else ''
        }

        # é–‹å°è¿½è·¡ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²ï¼ˆæ‹¡å¼µç‰ˆï¼‰
        save_email_open_record_enhanced(open_record)

        logger.info(f"ãƒ¡ãƒ¼ãƒ«é–‹å°ã‚’è¨˜éŒ²ã—ã¾ã—ãŸ ({tracking_method}): {tracking_id} at {now} [{device_type}]")

        # çµ±è¨ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
        clear_stats_cache()

    except Exception as e:
        logger.error(f"é–‹å°è¨˜éŒ²ã®ä¿å­˜ã‚¨ãƒ©ãƒ¼ ({tracking_method}): {e}")

def is_already_opened_by_method(tracking_id, tracking_method):
    """æŒ‡å®šã•ã‚ŒãŸãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDã¨æ–¹æ³•ã§æ—¢ã«é–‹å°æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        if not os.path.exists(NEW_EMAIL_OPEN_TRACKING):
            return False

        with open(NEW_EMAIL_OPEN_TRACKING, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if (row.get('tracking_id', '') == tracking_id and
                    row.get('tracking_method', 'pixel') == tracking_method):
                    return True
        return False
    except Exception as e:
        logger.error(f"é–‹å°çŠ¶æ³ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def detect_device_type_enhanced(user_agent):
    """User-Agentã‹ã‚‰ãƒ‡ãƒã‚¤ã‚¹ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®šï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    if not user_agent:
        return 'Unknown'

    user_agent_lower = user_agent.lower()

    # ãƒœãƒƒãƒˆæ¤œå‡º
    if any(bot in user_agent_lower for bot in ['bot', 'crawler', 'spider', 'scraper']):
        return 'Bot'

    # ãƒ¢ãƒã‚¤ãƒ«æ¤œå‡º
    if any(mobile in user_agent_lower for mobile in ['mobile', 'iphone', 'android', 'ipod', 'blackberry', 'windows phone']):
        return 'Mobile'

    # ã‚¿ãƒ–ãƒ¬ãƒƒãƒˆæ¤œå‡º
    if any(tablet in user_agent_lower for tablet in ['tablet', 'ipad']):
        return 'Tablet'

    # ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—æ¤œå‡º
    if any(desktop in user_agent_lower for desktop in ['windows', 'macintosh', 'linux', 'chrome', 'firefox', 'safari', 'edge']):
        return 'Desktop'

    return 'Unknown'

def save_email_open_record_enhanced(open_record):
    """é–‹å°è¨˜éŒ²ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½œæˆ
        file_exists = os.path.exists(NEW_EMAIL_OPEN_TRACKING)

        with open(NEW_EMAIL_OPEN_TRACKING, 'a', newline='', encoding='utf-8-sig') as f:
            fieldnames = [
                'tracking_id', 'opened_at', 'ip_address', 'device_type', 'user_agent', 'tracking_method', 'referer'
            ]
            writer = csv.DictWriter(f, fieldnames=fieldnames)

            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›¸ãè¾¼ã¿ï¼ˆæ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼‰
            if not file_exists:
                writer.writeheader()

            # é–‹å°è¨˜éŒ²ã‚’æ›¸ãè¾¼ã¿ï¼ˆå…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰
            filtered_record = {
                'tracking_id': open_record.get('tracking_id', ''),
                'opened_at': open_record.get('opened_at', ''),
                'ip_address': open_record.get('ip_address', ''),
                'device_type': open_record.get('device_type', 'Unknown'),
                'user_agent': open_record.get('user_agent', ''),
                'tracking_method': open_record.get('tracking_method', 'pixel'),
                'referer': open_record.get('referer', '')
            }
            writer.writerow(filtered_record)

    except Exception as e:
        logger.error(f"é–‹å°è¨˜éŒ²ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

def clear_stats_cache():
    """çµ±è¨ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹"""
    global stats_cache
    try:
        if 'stats_cache' in globals() and stats_cache is not None:
            stats_cache.clear()
        else:
            stats_cache = {}
    except Exception as e:
        logger.error(f"çµ±è¨ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}")
        stats_cache = {}

@app.route('/open-rate-analytics')
def open_rate_analytics():
    """é–‹å°ç‡åˆ†æãƒšãƒ¼ã‚¸ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰"""
    global open_rate_cache, open_rate_last_updated

    try:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        current_time = time.time()
        if (open_rate_cache and
            open_rate_last_updated and
            current_time - open_rate_last_updated < OPEN_RATE_CACHE_TIMEOUT):

            logger.info("é–‹å°ç‡åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—")
            return render_template(
                'open_rate_analytics.html',
                open_rate_stats=open_rate_cache['open_rate_stats'],
                daily_open_rates=open_rate_cache['daily_open_rates'],
                company_open_status=open_rate_cache['company_open_status'],
                unopened_emails=open_rate_cache['unopened_emails']
            )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯æ–°è¦è¨ˆç®—
        logger.info("é–‹å°ç‡åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’æ–°è¦è¨ˆç®—ä¸­...")

        # é–‹å°ç‡çµ±è¨ˆã‚’å–å¾—
        open_rate_stats = get_comprehensive_open_rate_stats()

        # æ—¥åˆ¥é–‹å°ç‡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        daily_open_rates = get_daily_open_rate_stats()

        # ä¼æ¥­åˆ¥é–‹å°çŠ¶æ³ã‚’å–å¾—
        company_open_status = get_company_open_status()

        # æœªé–‹å°ãƒ¡ãƒ¼ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
        unopened_emails = get_unopened_emails_list()

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        open_rate_cache = {
            'open_rate_stats': open_rate_stats,
            'daily_open_rates': daily_open_rates,
            'company_open_status': company_open_status,
            'unopened_emails': unopened_emails
        }
        open_rate_last_updated = current_time

        logger.info("é–‹å°ç‡åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜")

        return render_template(
            'open_rate_analytics.html',
            open_rate_stats=open_rate_stats,
            daily_open_rates=daily_open_rates,
            company_open_status=company_open_status,
            unopened_emails=unopened_emails,
            last_updated=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        )
    except Exception as e:
        logger.error(f"é–‹å°ç‡åˆ†æãƒšãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e}")
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", 500

def ensure_open_tracking_file_exists():
    """é–‹å°ç‡è¿½è·¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ"""
    try:
        if not os.path.exists(NEW_EMAIL_OPEN_TRACKING):
            # dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            os.makedirs(os.path.dirname(NEW_EMAIL_OPEN_TRACKING), exist_ok=True)

            # ç©ºã®é–‹å°ç‡è¿½è·¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            with open(NEW_EMAIL_OPEN_TRACKING, 'w', encoding='utf-8-sig', newline='') as f:
                fieldnames = [
                    'tracking_id', 'opened_at', 'ip_address', 'device_type', 'user_agent', 'tracking_method', 'referer'
                ]
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()

            logger.info(f"é–‹å°ç‡è¿½è·¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ: {NEW_EMAIL_OPEN_TRACKING}")
            return True
        return True
    except Exception as e:
        logger.error(f"é–‹å°ç‡è¿½è·¡ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

def get_comprehensive_open_rate_stats():
    """åŒ…æ‹¬çš„ãªé–‹å°ç‡çµ±è¨ˆã‚’å–å¾—ï¼ˆãƒã‚¦ãƒ³ã‚¹ä¼æ¥­ã‚’é™¤å¤–ï¼‰"""
    try:
        # é–‹å°ç‡è¿½è·¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        ensure_open_tracking_file_exists()

        # é€ä¿¡æ¸ˆã¿ãƒ¡ãƒ¼ãƒ«æ•°ã‚’å–å¾—
        sent_emails = get_sent_emails_with_tracking()

        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
        logger.info(f"å–å¾—ã—ãŸé€ä¿¡æ¸ˆã¿ãƒ¡ãƒ¼ãƒ«æ•°: {len(sent_emails)}ä»¶")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥ãƒã‚¦ãƒ³ã‚¹æ•°ã‚’å–å¾—
        bounced_count = get_csv_bounce_count()
        logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ã—ãŸãƒã‚¦ãƒ³ã‚¹æ•°: {bounced_count}ä»¶")

        # ãƒã‚¦ãƒ³ã‚¹ä¼æ¥­ã‚’é™¤å¤–ã—ãŸé€ä¿¡æ¸ˆã¿ãƒ¡ãƒ¼ãƒ«ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        valid_sent_emails = []
        csv_bounced_count = 0

        for email in sent_emails:
            bounce_status = check_bounce_status(email['company_id'])
            if bounce_status['is_bounced']:
                csv_bounced_count += 1
            else:
                valid_sent_emails.append(email)

        total_sent = len(sent_emails)
        temp_valid_sent_count = len(valid_sent_emails)

        # CSVã‹ã‚‰å–å¾—ã—ãŸãƒã‚¦ãƒ³ã‚¹æ•°ã‚’å„ªå…ˆä½¿ç”¨
        final_bounced_count = max(bounced_count, csv_bounced_count)
        final_valid_sent_count = total_sent - final_bounced_count

        logger.info(f"ç·é€ä¿¡æ•°: {total_sent}ä»¶, æœ‰åŠ¹é€ä¿¡æ•°: {final_valid_sent_count}ä»¶, ãƒã‚¦ãƒ³ã‚¹æ•°: {final_bounced_count}ä»¶")

        # é–‹å°è¨˜éŒ²ã‚’å–å¾—
        open_records = get_all_open_records()
        logger.info(f"å–å¾—ã—ãŸé–‹å°è¨˜éŒ²æ•°: {len(open_records)}ä»¶")

        # ãƒã‚¦ãƒ³ã‚¹ä¼æ¥­ã®é–‹å°è¨˜éŒ²ã‚’é™¤å¤–
        valid_open_records = []
        for record in open_records:
            # ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDã‹ã‚‰ä¼æ¥­IDã‚’é€†å¼•ã
            company_id = None
            for email in sent_emails:
                if email.get('tracking_id') == record.get('tracking_id'):
                    company_id = email.get('company_id')
                    break

            if company_id:
                bounce_status = check_bounce_status(company_id)
                if not bounce_status['is_bounced']:
                    valid_open_records.append(record)

        # ãƒ¦ãƒ‹ãƒ¼ã‚¯é–‹å°æ•°ã‚’è¨ˆç®—ï¼ˆãƒã‚¦ãƒ³ã‚¹é™¤å¤–å¾Œï¼‰
        unique_opens = len(set(record.get('tracking_id', '') for record in valid_open_records if record.get('tracking_id')))
        total_opens = len(valid_open_records)

        logger.info(f"æœ‰åŠ¹é–‹å°è¨˜éŒ²æ•°: {len(valid_open_records)}ä»¶, ãƒ¦ãƒ‹ãƒ¼ã‚¯é–‹å°æ•°: {unique_opens}ä»¶")

        # é–‹å°ç‡ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
        if total_sent == 0:
            logger.warning("é€ä¿¡æ¸ˆã¿ãƒ¡ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å®Ÿéš›ã®é€ä¿¡æ•°ã‚’å–å¾—ã—ã¾ã™ã€‚")
            # å®Ÿéš›ã®é€ä¿¡æ•°ã‚’å–å¾—
            total_sent = get_unified_sent_email_count()
            final_valid_sent_count = total_sent - final_bounced_count
            logger.info(f"å®Ÿéš›ã®é€ä¿¡æ•°ã‚’å–å¾—: {total_sent}ä»¶, æœ‰åŠ¹é€ä¿¡æ•°: {final_valid_sent_count}ä»¶")

        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ 
        logger.info(f"é–‹å°ç‡è¨ˆç®—å‰ã®å€¤: total_sent={total_sent}, valid_sent_count={final_valid_sent_count}, unique_opens={unique_opens}, bounced_count={final_bounced_count}")

        # ãƒ‡ãƒã‚¤ã‚¹åˆ¥çµ±è¨ˆï¼ˆãƒã‚¦ãƒ³ã‚¹é™¤å¤–ï¼‰
        device_stats = calculate_device_based_stats(valid_open_records)

        # é–‹å°ç‡ã‚’è¨ˆç®—ï¼ˆãƒã‚¦ãƒ³ã‚¹ä¼æ¥­ã‚’é™¤å¤–ï¼‰
        raw_open_rate = (unique_opens / final_valid_sent_count * 100) if final_valid_sent_count > 0 else 0.0

        # çµ±è¨ˆçš„è£œæ­£ã‚’é©ç”¨ã—ãŸé–‹å°ç‡ã‚’è¨ˆç®—
        corrected_open_rate = calculate_corrected_open_rate(raw_open_rate, final_valid_sent_count, unique_opens)

        # æ¨å®šå®Ÿéš›é–‹å°ç‡ã‚’è¨ˆç®—ï¼ˆä¼æ¥­ãƒ¡ãƒ¼ãƒ«ç’°å¢ƒã‚’è€ƒæ…®ï¼‰
        estimated_actual_rate = estimate_actual_open_rate(raw_open_rate, device_stats)

        # é–‹å°ç‡è¨ˆç®—çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
        logger.info(f"é–‹å°ç‡è¨ˆç®—çµæœ: raw_open_rate={raw_open_rate}%, corrected_open_rate={corrected_open_rate}%, estimated_actual_rate={estimated_actual_rate}%")

        # ãƒ©ãƒ³ã‚¯åˆ¥é–‹å°ç‡ã‚’è¨ˆç®—ï¼ˆãƒã‚¦ãƒ³ã‚¹é™¤å¤–ï¼‰ - HUGANJOBã‚·ã‚¹ãƒ†ãƒ ã§ã¯ä½¿ç”¨ã—ãªã„
        rank_stats = {}

        # æ™‚é–“å¸¯åˆ¥çµ±è¨ˆï¼ˆãƒã‚¦ãƒ³ã‚¹é™¤å¤–ï¼‰
        hourly_stats = calculate_hourly_open_stats(valid_open_records)

        # è¿½è·¡æ–¹æ³•åˆ¥çµ±è¨ˆ
        method_stats = calculate_method_based_stats(valid_open_records)

        return {
            'total_sent': total_sent,
            'valid_sent_count': final_valid_sent_count,  # ãƒã‚¦ãƒ³ã‚¹é™¤å¤–å¾Œã®é€ä¿¡æ•°
            'bounced_count': final_bounced_count,  # ãƒã‚¦ãƒ³ã‚¹æ•°
            'unique_opens': unique_opens,
            'total_opens': total_opens,
            'raw_open_rate': round(raw_open_rate, 2),  # ç”Ÿã®é–‹å°ç‡
            'corrected_open_rate': round(corrected_open_rate, 2),  # çµ±è¨ˆçš„è£œæ­£å¾Œ
            'estimated_actual_rate': round(estimated_actual_rate, 2),  # æ¨å®šå®Ÿéš›é–‹å°ç‡
            'open_rate': round(raw_open_rate, 2),  # ãƒ¡ã‚¤ãƒ³è¡¨ç¤ºç”¨ï¼ˆç”Ÿã®é–‹å°ç‡ï¼‰
            'bounce_rate': round((final_bounced_count / total_sent * 100) if total_sent > 0 else 0.0, 2),
            'rank_stats': rank_stats,
            'device_stats': device_stats,
            'hourly_stats': hourly_stats,
            'method_stats': method_stats,
            'last_updated': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

    except Exception as e:
        logger.error(f"åŒ…æ‹¬çš„é–‹å°ç‡çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§ã‚‚CSVã‹ã‚‰ãƒã‚¦ãƒ³ã‚¹æ•°ã‚’å–å¾—
        csv_bounce_count = get_csv_bounce_count()
        return {
            'total_sent': 0,
            'valid_sent_count': 0,
            'bounced_count': csv_bounce_count,
            'unique_opens': 0,
            'total_opens': 0,
            'raw_open_rate': 0.0,
            'corrected_open_rate': 0.0,
            'estimated_actual_rate': 0.0,
            'open_rate': 0.0,
            'bounce_rate': 0.0,
            'rank_stats': {},
            'device_stats': {},
            'hourly_stats': {},
            'method_stats': {},
            'last_updated': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

def calculate_corrected_open_rate(raw_rate, sent_count, open_count):
    """çµ±è¨ˆçš„è£œæ­£ã‚’é©ç”¨ã—ãŸé–‹å°ç‡ã‚’è¨ˆç®—"""
    try:
        # ä¼æ¥­ãƒ¡ãƒ¼ãƒ«ç’°å¢ƒã§ã®ç”»åƒãƒ–ãƒ­ãƒƒã‚¯ç‡ã‚’è€ƒæ…®ï¼ˆæ¨å®š70-80%ï¼‰
        image_block_rate = 0.75

        # å°ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®è£œæ­£
        if sent_count < 100:
            confidence_factor = 0.8
        elif sent_count < 500:
            confidence_factor = 0.9
        else:
            confidence_factor = 1.0

        # è£œæ­£è¨ˆç®—
        if raw_rate > 0:
            # å®Ÿéš›ã®é–‹å°ç‡ã¯ç”»åƒãƒ–ãƒ­ãƒƒã‚¯ã‚’è€ƒæ…®ã—ã¦æ¨å®š
            corrected_rate = raw_rate / (1 - image_block_rate) * confidence_factor
            # ç¾å®Ÿçš„ãªä¸Šé™ã‚’è¨­å®šï¼ˆ30%ï¼‰
            corrected_rate = min(corrected_rate, 30.0)
        else:
            # é–‹å°ãŒ0ã®å ´åˆã¯è£œæ­£ã‚‚0ã¨ã™ã‚‹ï¼ˆæ¨å®šå€¤ã¯åˆ¥é€”è¨ˆç®—ï¼‰
            corrected_rate = 0.0

        return corrected_rate

    except Exception as e:
        logger.error(f"é–‹å°ç‡è£œæ­£è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return raw_rate

def estimate_actual_open_rate(raw_rate, device_stats):
    """æ¨å®šå®Ÿéš›é–‹å°ç‡ã‚’è¨ˆç®—ï¼ˆä¼æ¥­ãƒ¡ãƒ¼ãƒ«ç’°å¢ƒã‚’è€ƒæ…®ï¼‰"""
    try:
        # ãƒ‡ãƒã‚¤ã‚¹åˆ¥ã®æ¤œå‡ºç‡ã‚’è€ƒæ…®
        mobile_rate = device_stats.get('Mobile', {}).get('count', 0)
        desktop_rate = device_stats.get('Desktop', {}).get('count', 0)
        total_detected = mobile_rate + desktop_rate

        if total_detected > 0:
            # ãƒ¢ãƒã‚¤ãƒ«ã¯æ¤œå‡ºç‡ãŒé«˜ã„ï¼ˆ60%ï¼‰ã€ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã¯ä½ã„ï¼ˆ20%ï¼‰
            mobile_detection_rate = 0.6
            desktop_detection_rate = 0.2

            # åŠ é‡å¹³å‡ã§å®Ÿéš›ã®æ¤œå‡ºç‡ã‚’è¨ˆç®—
            weighted_detection_rate = (
                (mobile_rate * mobile_detection_rate + desktop_rate * desktop_detection_rate) / total_detected
            )

            # å®Ÿéš›ã®é–‹å°ç‡ã‚’æ¨å®š
            estimated_rate = raw_rate / weighted_detection_rate if weighted_detection_rate > 0 else raw_rate * 5
        else:
            # ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ãŒãªã„å ´åˆã¯ä¿å®ˆçš„ãªæ¨å®š
            estimated_rate = raw_rate * 4  # 25%ã®æ¤œå‡ºç‡ã‚’ä»®å®š

        # ç¾å®Ÿçš„ãªç¯„å›²ã«åˆ¶é™
        estimated_rate = min(estimated_rate, 50.0)  # æœ€å¤§50%
        estimated_rate = max(estimated_rate, raw_rate)  # ç”Ÿã®å€¤ã‚ˆã‚Šä½ãã¯ãªã‚‰ãªã„

        return estimated_rate

    except Exception as e:
        logger.error(f"å®Ÿéš›é–‹å°ç‡æ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
        return raw_rate * 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨å®š

def calculate_method_based_stats(open_records):
    """è¿½è·¡æ–¹æ³•åˆ¥çµ±è¨ˆã‚’è¨ˆç®—"""
    try:
        method_stats = {}

        for record in open_records:
            method = record.get('tracking_method', 'pixel')
            if method not in method_stats:
                method_stats[method] = {'count': 0, 'percentage': 0}
            method_stats[method]['count'] += 1

        total = len(open_records)
        for method in method_stats:
            method_stats[method]['percentage'] = round(
                (method_stats[method]['count'] / total * 100) if total > 0 else 0, 1
            )

        return method_stats

    except Exception as e:
        logger.error(f"è¿½è·¡æ–¹æ³•åˆ¥çµ±è¨ˆè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def get_unified_sent_email_count():
    """çµ±ä¸€ã•ã‚ŒãŸé€ä¿¡ãƒ¡ãƒ¼ãƒ«æ•°ã‚’å–å¾—ï¼ˆå…¨ã‚·ã‚¹ãƒ†ãƒ å…±é€šï¼‰"""
    try:
        total_sent = 0

        # ãƒ¡ã‚¤ãƒ³ã®é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
        if os.path.exists(NEW_EMAIL_SENDING_RESULTS):
            with open(NEW_EMAIL_SENDING_RESULTS, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    send_result = row.get('é€ä¿¡çµæœ', '').strip()
                    if send_result == 'success':
                        total_sent += 1

        # HUGANJOBã®é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç¢ºèª
        huganjob_files = [f for f in os.listdir('.') if f.startswith('huganjob_sending_results_') and f.endswith('.csv')]
        for huganjob_file in huganjob_files:
            try:
                with open(huganjob_file, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        success = row.get('success', '').strip()
                        if success == 'True':
                            total_sent += 1
            except Exception as e:
                logger.warning(f"HUGANJOBãƒ•ã‚¡ã‚¤ãƒ« {huganjob_file} èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        logger.info(f"çµ±ä¸€é€ä¿¡æ•°å–å¾—å®Œäº†: {total_sent}ä»¶")
        return total_sent

    except Exception as e:
        logger.error(f"çµ±ä¸€é€ä¿¡æ•°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return 0

def get_sent_emails_with_tracking():
    """ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDä»˜ãã®é€ä¿¡æ¸ˆã¿ãƒ¡ãƒ¼ãƒ«ã‚’å–å¾—ï¼ˆè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰"""
    try:
        sent_emails = []

        # ãƒ¡ã‚¤ãƒ³ã®é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
        if os.path.exists(NEW_EMAIL_SENDING_RESULTS):
            with open(NEW_EMAIL_SENDING_RESULTS, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    # ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDãŒã‚ã‚Šã€é€ä¿¡æˆåŠŸã—ãŸãƒ¡ãƒ¼ãƒ«ã®ã¿
                    tracking_id = row.get('ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ID', '').strip()
                    send_result = row.get('é€ä¿¡çµæœ', '').strip()
                    if tracking_id and send_result == 'success':
                        # ãƒ©ãƒ³ã‚¯æƒ…å ±ãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                        rank = row.get('ãƒ©ãƒ³ã‚¯', 'A')
                        if not rank or rank.strip() == '':
                            rank = 'A'

                        sent_emails.append({
                            'tracking_id': tracking_id,
                            'company_id': row.get('ä¼æ¥­ID', ''),
                            'company_name': row.get('ä¼æ¥­å', ''),
                            'email': row.get('ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', ''),
                            'rank': rank,
                            'sent_at': row.get('é€ä¿¡æ—¥æ™‚', '')
                        })

        # HUGANJOBã®é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç¢ºèª
        huganjob_files = [f for f in os.listdir('.') if f.startswith('huganjob_sending_results_') and f.endswith('.csv')]
        for huganjob_file in huganjob_files:
            try:
                with open(huganjob_file, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        tracking_id = row.get('tracking_id', '').strip()
                        success = row.get('success', '').strip()
                        if tracking_id and success == 'True':
                            sent_emails.append({
                                'tracking_id': tracking_id,
                                'company_id': row.get('company_id', ''),
                                'company_name': row.get('company_name', ''),
                                'email': row.get('email_address', ''),
                                'rank': 'A',  # HUGANJOBãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ãƒ©ãƒ³ã‚¯æƒ…å ±ãŒãªã„ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                                'sent_at': row.get('send_datetime', '')
                            })
            except Exception as e:
                logger.warning(f"HUGANJOBãƒ•ã‚¡ã‚¤ãƒ« {huganjob_file} èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        logger.info(f"é€ä¿¡æ¸ˆã¿ãƒ¡ãƒ¼ãƒ«å–å¾—å®Œäº†: {len(sent_emails)}ä»¶")
        return sent_emails

    except Exception as e:
        logger.error(f"é€ä¿¡æ¸ˆã¿ãƒ¡ãƒ¼ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
        return []

def get_all_open_records():
    """å…¨ã¦ã®é–‹å°è¨˜éŒ²ã‚’å–å¾—"""
    try:
        open_records = []

        if os.path.exists(NEW_EMAIL_OPEN_TRACKING):
            with open(NEW_EMAIL_OPEN_TRACKING, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    # ç©ºè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if row.get('tracking_id', '').strip():
                        open_records.append(row)

        logger.info(f"é–‹å°è¨˜éŒ²å–å¾—å®Œäº†: {len(open_records)}ä»¶")
        return open_records

    except Exception as e:
        logger.error(f"é–‹å°è¨˜éŒ²å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
        return []

def calculate_rank_based_open_rates(sent_emails, open_records):
    """ãƒ©ãƒ³ã‚¯åˆ¥é–‹å°ç‡ã‚’è¨ˆç®—"""
    try:
        # ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDã‹ã‚‰ä¼æ¥­ãƒ©ãƒ³ã‚¯ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
        tracking_to_rank = {email['tracking_id']: email['rank'] for email in sent_emails}

        # ãƒ©ãƒ³ã‚¯åˆ¥çµ±è¨ˆã‚’åˆæœŸåŒ–
        rank_stats = {'A': {'sent': 0, 'opened': 0}, 'B': {'sent': 0, 'opened': 0}, 'C': {'sent': 0, 'opened': 0}}

        # é€ä¿¡æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        for email in sent_emails:
            rank = email['rank']
            if rank in rank_stats:
                rank_stats[rank]['sent'] += 1

        # é–‹å°æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰
        opened_tracking_ids = set(record['tracking_id'] for record in open_records)
        for tracking_id in opened_tracking_ids:
            rank = tracking_to_rank.get(tracking_id)
            if rank and rank in rank_stats:
                rank_stats[rank]['opened'] += 1

        # é–‹å°ç‡ã‚’è¨ˆç®—
        for rank in rank_stats:
            sent = rank_stats[rank]['sent']
            opened = rank_stats[rank]['opened']
            rank_stats[rank]['open_rate'] = round((opened / sent * 100) if sent > 0 else 0.0, 2)

        return rank_stats

    except Exception as e:
        logger.error(f"ãƒ©ãƒ³ã‚¯åˆ¥é–‹å°ç‡è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def calculate_device_based_stats(open_records):
    """ãƒ‡ãƒã‚¤ã‚¹åˆ¥çµ±è¨ˆã‚’è¨ˆç®—"""
    try:
        device_stats = {}

        for record in open_records:
            device = record.get('device_type', 'Unknown')
            if device not in device_stats:
                device_stats[device] = 0
            device_stats[device] += 1

        return device_stats

    except Exception as e:
        logger.error(f"ãƒ‡ãƒã‚¤ã‚¹åˆ¥çµ±è¨ˆè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def calculate_hourly_open_stats(open_records):
    """æ™‚é–“å¸¯åˆ¥é–‹å°çµ±è¨ˆã‚’è¨ˆç®—ï¼ˆç¾å®Ÿçš„ãªåˆ†å¸ƒã«ä¿®æ­£ï¼‰"""
    try:
        hourly_stats = {str(i): 0 for i in range(24)}
        valid_records = []

        for record in open_records:
            try:
                opened_at = record['opened_at']

                # ç•°å¸¸ãªæ™‚åˆ»ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ:10ã§çµ‚ã‚ã‚‹ï¼‰ã¯é™¤å¤–
                if opened_at.endswith(':10'):
                    continue

                open_time = datetime.datetime.strptime(opened_at, '%Y-%m-%d %H:%M:%S')
                hour = open_time.hour

                # ç¾å®Ÿçš„ã§ãªã„æ™‚é–“å¸¯ï¼ˆæ·±å¤œ0-5æ™‚ï¼‰ã¯é™¤å¤–
                if 0 <= hour <= 5:
                    continue

                valid_records.append(record)
                hourly_stats[str(hour)] += 1
            except:
                continue

        # çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ 
        total_valid = len(valid_records)
        for hour_str in hourly_stats:
            count = hourly_stats[hour_str]
            hourly_stats[hour_str] = {
                'count': count,
                'percentage': round((count / total_valid * 100) if total_valid > 0 else 0, 2),
                'is_realistic': int(hour_str) >= 6  # 6æ™‚ä»¥é™ã‚’ç¾å®Ÿçš„ã¨ã™ã‚‹
            }

        return hourly_stats

    except Exception as e:
        logger.error(f"æ™‚é–“å¸¯åˆ¥çµ±è¨ˆè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def get_daily_open_rate_stats(days=30):
    """æ—¥åˆ¥é–‹å°ç‡çµ±è¨ˆã‚’å–å¾—ï¼ˆç¾å®Ÿçš„ãªåˆ†å¸ƒã«ä¿®æ­£ï¼‰"""
    try:
        # éå»Næ—¥é–“ã®æ—¥ä»˜ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
        end_date = datetime.datetime.now()
        start_date = end_date - datetime.timedelta(days=days)

        daily_stats = {}
        current_date = start_date

        while current_date <= end_date:
            date_str = current_date.strftime('%Y-%m-%d')
            daily_stats[date_str] = {
                'sent': 0,
                'opened': 0,
                'open_rate': 0.0,
                'valid_opens': 0  # æœ‰åŠ¹ãªé–‹å°æ•°
            }
            current_date += datetime.timedelta(days=1)

        # é€ä¿¡æ¸ˆã¿ãƒ¡ãƒ¼ãƒ«ã®æ—¥åˆ¥é›†è¨ˆï¼ˆãƒã‚¦ãƒ³ã‚¹ä¼æ¥­ã‚’é™¤å¤–ï¼‰
        sent_emails = get_sent_emails_with_tracking()
        for email in sent_emails:
            try:
                # ãƒã‚¦ãƒ³ã‚¹çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯
                bounce_status = check_bounce_status(email['company_id'])
                if bounce_status['is_bounced']:
                    continue  # ãƒã‚¦ãƒ³ã‚¹ä¼æ¥­ã¯é™¤å¤–

                sent_date = datetime.datetime.strptime(email['sent_at'], '%Y-%m-%d %H:%M:%S')
                date_str = sent_date.strftime('%Y-%m-%d')
                if date_str in daily_stats:
                    daily_stats[date_str]['sent'] += 1
            except:
                continue

        # é–‹å°è¨˜éŒ²ã®æ—¥åˆ¥é›†è¨ˆï¼ˆç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–ï¼‰
        open_records = get_all_open_records()
        opened_by_date = {}
        valid_opened_by_date = {}

        for record in open_records:
            try:
                opened_at = record['opened_at']

                # ç•°å¸¸ãªæ™‚åˆ»ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ:10ã§çµ‚ã‚ã‚‹ï¼‰ã¯é™¤å¤–
                if opened_at.endswith(':10'):
                    continue

                opened_date = datetime.datetime.strptime(opened_at, '%Y-%m-%d %H:%M:%S')

                # ç¾å®Ÿçš„ã§ãªã„æ™‚é–“å¸¯ï¼ˆæ·±å¤œ0-5æ™‚ï¼‰ã¯é™¤å¤–
                if 0 <= opened_date.hour <= 5:
                    continue

                date_str = opened_date.strftime('%Y-%m-%d')

                # å…¨é–‹å°è¨˜éŒ²
                if date_str not in opened_by_date:
                    opened_by_date[date_str] = set()
                opened_by_date[date_str].add(record['tracking_id'])

                # æœ‰åŠ¹ãªé–‹å°è¨˜éŒ²
                if date_str not in valid_opened_by_date:
                    valid_opened_by_date[date_str] = set()
                valid_opened_by_date[date_str].add(record['tracking_id'])

            except:
                continue

        # é–‹å°ç‡ã‚’è¨ˆç®—
        for date_str in daily_stats:
            sent = daily_stats[date_str]['sent']
            opened = len(opened_by_date.get(date_str, set()))
            valid_opens = len(valid_opened_by_date.get(date_str, set()))

            daily_stats[date_str]['opened'] = opened
            daily_stats[date_str]['valid_opens'] = valid_opens
            daily_stats[date_str]['open_rate'] = round((valid_opens / sent * 100) if sent > 0 else 0.0, 2)

            # ç¾å®Ÿçš„ãªé–‹å°ç‡ã®ä¸Šé™ã‚’è¨­å®šï¼ˆ50%ã‚’è¶…ãˆã‚‹å ´åˆã¯ç•°å¸¸ã¨ã¿ãªã™ï¼‰
            if daily_stats[date_str]['open_rate'] > 50.0:
                daily_stats[date_str]['is_suspicious'] = True
                daily_stats[date_str]['open_rate'] = min(daily_stats[date_str]['open_rate'], 30.0)  # ä¸Šé™ã‚’30%ã«åˆ¶é™
            else:
                daily_stats[date_str]['is_suspicious'] = False

        return daily_stats

    except Exception as e:
        logger.error(f"æ—¥åˆ¥é–‹å°ç‡çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def get_csv_bounce_count():
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥ãƒã‚¦ãƒ³ã‚¹æ•°ã‚’å–å¾—"""
    try:
        csv_file = 'data/new_input_test.csv'
        if not os.path.exists(csv_file):
            logger.warning(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_file}")
            return 0

        bounce_count = 0
        with open(csv_file, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                bounce_status = row.get('ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹', '').strip()
                # ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹ãŒ'permanent'ã€'temporary'ã€'unknown'ã®ã„ãšã‚Œã‹ã®å ´åˆã¯ãƒã‚¦ãƒ³ã‚¹ã¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆ
                if bounce_status and bounce_status.lower() in ['permanent', 'temporary', 'unknown']:
                    bounce_count += 1

        logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ã—ãŸãƒã‚¦ãƒ³ã‚¹æ•°: {bounce_count}ä»¶")
        return bounce_count

    except Exception as e:
        logger.error(f"CSVãƒã‚¦ãƒ³ã‚¹æ•°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return 0

def check_bounce_status(company_id):
    """ä¼æ¥­ã®ãƒã‚¦ãƒ³ã‚¹çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯"""
    try:
        # ã¾ãšCSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥ãƒã‚§ãƒƒã‚¯
        csv_file = 'data/new_input_test.csv'
        if os.path.exists(csv_file):
            with open(csv_file, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if str(row.get('ID', '')) == str(company_id):
                        bounce_status = row.get('ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹', '').strip()
                        if bounce_status and bounce_status.lower() in ['permanent', 'temporary', 'unknown']:
                            return {
                                'is_bounced': True,
                                'reason': row.get('ãƒã‚¦ãƒ³ã‚¹ç†ç”±', ''),
                                'bounce_type': bounce_status,
                                'detected_at': row.get('ãƒã‚¦ãƒ³ã‚¹æ—¥æ™‚', ''),
                                'status': bounce_status
                            }

        # åŒ…æ‹¬çš„ãƒã‚¦ãƒ³ã‚¹æ¤œå‡ºçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
        comprehensive_bounce_file = 'comprehensive_bounce_tracking_results.csv'
        if os.path.exists(comprehensive_bounce_file):
            with open(comprehensive_bounce_file, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if str(row.get('ä¼æ¥­ID', '')) == str(company_id):
                        return {
                            'is_bounced': True,
                            'reason': row.get('ãƒã‚¦ãƒ³ã‚¹ç†ç”±', ''),
                            'bounce_type': row.get('ãƒã‚¦ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—', ''),
                            'detected_at': row.get('ãƒã‚¦ãƒ³ã‚¹æ—¥æ™‚', ''),
                            'status': row.get('ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹', '')
                        }

        # æ¨™æº–ãƒã‚¦ãƒ³ã‚¹è¿½è·¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ãƒã‚§ãƒƒã‚¯
        if os.path.exists(NEW_BOUNCE_TRACKING):
            with open(NEW_BOUNCE_TRACKING, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if str(row.get('ä¼æ¥­ID', '')) == str(company_id):
                        return {
                            'is_bounced': True,
                            'reason': row.get('ãƒã‚¦ãƒ³ã‚¹ç†ç”±', ''),
                            'bounce_type': row.get('ãƒã‚¦ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—', ''),
                            'detected_at': row.get('ãƒã‚¦ãƒ³ã‚¹æ—¥æ™‚', ''),
                            'status': 'bounced'
                        }

        return {'is_bounced': False}

    except Exception as e:
        logger.error(f"ãƒã‚¦ãƒ³ã‚¹çŠ¶æ³ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ (ä¼æ¥­ID: {company_id}): {e}")
        return {'is_bounced': False}

def get_company_open_status_detail(company_id):
    """æŒ‡å®šä¼æ¥­ã®é–‹å°çŠ¶æ³ã‚’å–å¾—ï¼ˆãƒã‚¦ãƒ³ã‚¹çŠ¶æ³ã‚’æœ€å„ªå…ˆã§ãƒã‚§ãƒƒã‚¯ï¼‰"""
    try:
        # 1. ãƒã‚¦ãƒ³ã‚¹çŠ¶æ³ã‚’æœ€å„ªå…ˆã§ãƒã‚§ãƒƒã‚¯
        bounce_status = check_bounce_status(company_id)
        if bounce_status['is_bounced']:
            return {
                'has_tracking': True,
                'is_opened': False,  # å¼·åˆ¶çš„ã«False
                'bounce_reason': bounce_status['reason'],
                'bounce_type': bounce_status.get('bounce_type', ''),
                'bounce_detected_at': bounce_status.get('detected_at', ''),
                'message': f'ãƒ¡ãƒ¼ãƒ«ãŒãƒã‚¦ãƒ³ã‚¹ã—ãŸãŸã‚é–‹å°ä¸å¯ (ç†ç”±: {bounce_status["reason"]})',
                'is_bounced': True
            }

        # 2. é€ä¿¡æ¸ˆã¿ãƒ¡ãƒ¼ãƒ«ã‹ã‚‰ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDã‚’å–å¾—
        tracking_id = None
        sent_date = None
        email_address = None

        if os.path.exists(NEW_EMAIL_SENDING_RESULTS):
            with open(NEW_EMAIL_SENDING_RESULTS, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if str(row.get('ä¼æ¥­ID', '')) == str(company_id):
                        tracking_id = row.get('ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ID', '').strip()
                        sent_date = row.get('é€ä¿¡æ—¥æ™‚', '')
                        email_address = row.get('ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', '')
                        break

        if not tracking_id:
            return {
                'has_tracking': False,
                'is_opened': False,
                'sent_date': sent_date,
                'email_address': email_address,
                'message': 'ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“',
                'is_bounced': False
            }

        # 3. é–‹å°è¨˜éŒ²ã‚’å–å¾—
        open_records = []
        if os.path.exists(NEW_EMAIL_OPEN_TRACKING):
            with open(NEW_EMAIL_OPEN_TRACKING, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if row.get('tracking_id', '') == tracking_id:
                        open_records.append({
                            'opened_at': row.get('opened_at', ''),
                            'ip_address': row.get('ip_address', ''),
                            'device_type': row.get('device_type', 'Unknown'),
                            'user_agent': row.get('user_agent', '')
                        })

        # 4. é–‹å°çŠ¶æ³ã‚’åˆ†æ
        is_opened = len(open_records) > 0

        if is_opened:
            # é–‹å°æ™‚åˆ»ã‚’ã‚½ãƒ¼ãƒˆ
            sorted_opens = sorted(open_records, key=lambda x: x['opened_at'])
            first_open = sorted_opens[0]
            latest_open = sorted_opens[-1]

            # ãƒ‡ãƒã‚¤ã‚¹çµ±è¨ˆ
            device_counts = {}
            for record in open_records:
                device = record['device_type']
                device_counts[device] = device_counts.get(device, 0) + 1

            # é€ä¿¡ã‹ã‚‰ã®çµŒéæ—¥æ•°è¨ˆç®—
            days_since_sent = None
            if sent_date:
                try:
                    sent_dt = datetime.datetime.strptime(sent_date, '%Y-%m-%d %H:%M:%S')
                    first_open_dt = datetime.datetime.strptime(first_open['opened_at'], '%Y-%m-%d %H:%M:%S')
                    days_since_sent = (first_open_dt - sent_dt).days
                except:
                    days_since_sent = None

            return {
                'has_tracking': True,
                'is_opened': True,
                'tracking_id': tracking_id,
                'sent_date': sent_date,
                'email_address': email_address,
                'open_count': len(open_records),
                'first_opened_at': first_open['opened_at'],
                'latest_opened_at': latest_open['opened_at'],
                'days_since_sent': days_since_sent,
                'device_counts': device_counts,
                'open_records': open_records,
                'primary_device': max(device_counts.items(), key=lambda x: x[1])[0] if device_counts else 'Unknown',
                'is_bounced': False
            }
        else:
            # æœªé–‹å°ã®å ´åˆã®çµŒéæ—¥æ•°è¨ˆç®—
            days_since_sent = None
            if sent_date:
                try:
                    sent_dt = datetime.datetime.strptime(sent_date, '%Y-%m-%d %H:%M:%S')
                    now_dt = datetime.datetime.now()
                    days_since_sent = (now_dt - sent_dt).days
                except:
                    days_since_sent = None

            return {
                'has_tracking': True,
                'is_opened': False,
                'tracking_id': tracking_id,
                'sent_date': sent_date,
                'email_address': email_address,
                'days_since_sent': days_since_sent,
                'message': 'ãƒ¡ãƒ¼ãƒ«ã¯æœªé–‹å°ã§ã™',
                'is_bounced': False
            }

    except Exception as e:
        logger.error(f"ä¼æ¥­{company_id}ã®é–‹å°çŠ¶æ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'has_tracking': False,
            'is_opened': False,
            'error': str(e),
            'message': 'ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ',
            'is_bounced': False
        }

def get_company_open_status(limit=100):
    """ä¼æ¥­åˆ¥é–‹å°çŠ¶æ³ã‚’å–å¾—ï¼ˆãƒã‚¦ãƒ³ã‚¹ä¼æ¥­ã‚’è€ƒæ…®ï¼‰"""
    try:
        sent_emails = get_sent_emails_with_tracking()
        open_records = get_all_open_records()

        # ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDã‹ã‚‰é–‹å°æƒ…å ±ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        opened_tracking_ids = set(record['tracking_id'] for record in open_records)

        # ä¼æ¥­åˆ¥é–‹å°çŠ¶æ³ã‚’ä½œæˆ
        company_status = []

        for email in sent_emails[:limit]:  # æœ€æ–°ã®Nä»¶ã«åˆ¶é™
            company_id = email['company_id']

            # ãƒã‚¦ãƒ³ã‚¹çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯
            bounce_status = check_bounce_status(company_id)
            is_bounced = bounce_status['is_bounced']

            # ãƒã‚¦ãƒ³ã‚¹ä¼æ¥­ã¯å¼·åˆ¶çš„ã«æœªé–‹å°æ‰±ã„
            if is_bounced:
                is_opened = False
                open_time = None
                bounce_reason = bounce_status.get('reason', '')
            else:
                is_opened = email['tracking_id'] in opened_tracking_ids
                # é–‹å°æ™‚åˆ»ã‚’å–å¾—
                open_time = None
                if is_opened:
                    for record in open_records:
                        if record['tracking_id'] == email['tracking_id']:
                            open_time = record['opened_at']
                            break
                bounce_reason = None

            company_status.append({
                'company_id': company_id,
                'company_name': email['company_name'],
                'email': email['email'],
                'rank': email['rank'],
                'sent_at': email['sent_at'],
                'is_opened': is_opened,
                'opened_at': open_time,
                'tracking_id': email['tracking_id'],
                'is_bounced': is_bounced,
                'bounce_reason': bounce_reason
            })

        # é–‹å°çŠ¶æ³ã§ã‚½ãƒ¼ãƒˆï¼ˆãƒã‚¦ãƒ³ã‚¹â†’æœªé–‹å°â†’é–‹å°ã®é †ï¼‰
        company_status.sort(key=lambda x: (x['is_opened'], not x['is_bounced'], x['sent_at']))

        return company_status

    except Exception as e:
        logger.error(f"ä¼æ¥­åˆ¥é–‹å°çŠ¶æ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def get_unopened_emails_list(days_threshold=7):
    """æœªé–‹å°ãƒ¡ãƒ¼ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆæŒ‡å®šæ—¥æ•°çµŒéå¾Œï¼‰"""
    try:
        sent_emails = get_sent_emails_with_tracking()
        open_records = get_all_open_records()

        # é–‹å°æ¸ˆã¿ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDã®ã‚»ãƒƒãƒˆ
        opened_tracking_ids = set(record['tracking_id'] for record in open_records)

        # æŒ‡å®šæ—¥æ•°å‰ã®æ—¥æ™‚
        threshold_date = datetime.datetime.now() - datetime.timedelta(days=days_threshold)

        unopened_emails = []

        for email in sent_emails:
            # æœªé–‹å°ã‹ã¤æŒ‡å®šæ—¥æ•°çµŒéã—ãŸãƒ¡ãƒ¼ãƒ«
            if email['tracking_id'] not in opened_tracking_ids:
                try:
                    sent_date = datetime.datetime.strptime(email['sent_at'], '%Y-%m-%d %H:%M:%S')
                    if sent_date <= threshold_date:
                        days_since_sent = (datetime.datetime.now() - sent_date).days

                        unopened_emails.append({
                            'company_id': email['company_id'],
                            'company_name': email['company_name'],
                            'email': email['email'],
                            'rank': email['rank'],
                            'sent_at': email['sent_at'],
                            'days_since_sent': days_since_sent,
                            'tracking_id': email['tracking_id']
                        })
                except:
                    continue

        # é€ä¿¡æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼ˆå¤ã„é †ï¼‰
        unopened_emails.sort(key=lambda x: x['sent_at'])

        return unopened_emails

    except Exception as e:
        logger.error(f"æœªé–‹å°ãƒ¡ãƒ¼ãƒ«ãƒªã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def check_data_integrity():
    """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆãƒã‚¦ãƒ³ã‚¹ä¼æ¥­ã®é–‹å°è¨˜éŒ²çŸ›ç›¾ã‚’æ¤œå‡ºï¼‰"""
    try:
        inconsistent_companies = []

        # é€ä¿¡æ¸ˆã¿ãƒ¡ãƒ¼ãƒ«ã‚’å–å¾—
        sent_emails = get_sent_emails_with_tracking()

        # é–‹å°è¨˜éŒ²ã‚’å–å¾—
        open_records = get_all_open_records()
        opened_tracking_ids = set(record['tracking_id'] for record in open_records)

        # å„ä¼æ¥­ã«ã¤ã„ã¦æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯
        for email in sent_emails:
            company_id = email['company_id']
            tracking_id = email['tracking_id']

            # ãƒã‚¦ãƒ³ã‚¹çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯
            bounce_status = check_bounce_status(company_id)
            is_bounced = bounce_status['is_bounced']

            # é–‹å°çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯
            is_opened = tracking_id in opened_tracking_ids

            # çŸ›ç›¾ã‚’æ¤œå‡ºï¼ˆãƒã‚¦ãƒ³ã‚¹æ¸ˆã¿ãªã®ã«é–‹å°æ¸ˆã¿ï¼‰
            if is_bounced and is_opened:
                inconsistent_companies.append({
                    'company_id': company_id,
                    'company_name': email['company_name'],
                    'email_address': email['email'],
                    'tracking_id': tracking_id,
                    'bounce_reason': bounce_status.get('reason', ''),
                    'bounce_detected_at': bounce_status.get('detected_at', ''),
                    'issue': 'ãƒã‚¦ãƒ³ã‚¹æ¸ˆã¿ãªã®ã«é–‹å°æ¸ˆã¿',
                    'detected_at': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                })

        return {
            'total_checked': len(sent_emails),
            'inconsistent_count': len(inconsistent_companies),
            'inconsistent_companies': inconsistent_companies,
            'integrity_rate': round(((len(sent_emails) - len(inconsistent_companies)) / len(sent_emails) * 100) if len(sent_emails) > 0 else 100.0, 2),
            'checked_at': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'total_checked': 0,
            'inconsistent_count': 0,
            'inconsistent_companies': [],
            'integrity_rate': 0.0,
            'error': str(e),
            'checked_at': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

def get_bounce_open_inconsistency_summary():
    """ãƒã‚¦ãƒ³ã‚¹ãƒ»é–‹å°ã®çŸ›ç›¾ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
    try:
        integrity_check = check_data_integrity()

        summary = {
            'has_issues': integrity_check['inconsistent_count'] > 0,
            'total_issues': integrity_check['inconsistent_count'],
            'integrity_rate': integrity_check['integrity_rate'],
            'sample_issues': integrity_check['inconsistent_companies'][:5],  # æœ€åˆã®5ä»¶ã®ã‚µãƒ³ãƒ—ãƒ«
            'last_checked': integrity_check['checked_at']
        }

        return summary

    except Exception as e:
        logger.error(f"çŸ›ç›¾ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'has_issues': False,
            'total_issues': 0,
            'integrity_rate': 100.0,
            'sample_issues': [],
            'error': str(e),
            'last_checked': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

def clean_open_tracking_data():
    """é–‹å°è¿½è·¡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆé‡è¤‡é™¤å»ã€ç•°å¸¸ãƒ‡ãƒ¼ã‚¿é™¤å»ï¼‰"""
    try:
        if not os.path.exists(NEW_EMAIL_OPEN_TRACKING):
            return {
                'success': False,
                'message': 'é–‹å°è¿½è·¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“',
                'original_count': 0,
                'cleaned_count': 0,
                'removed_count': 0
            }

        # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        original_records = []
        with open(NEW_EMAIL_OPEN_TRACKING, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                original_records.append(row)

        original_count = len(original_records)
        logger.info(f"é–‹å°è¿½è·¡ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹: {original_count}ä»¶")

        # æœ‰åŠ¹ãªé€ä¿¡æ¸ˆã¿ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDã‚’å–å¾—
        valid_tracking_ids = set()
        if os.path.exists(NEW_EMAIL_SENDING_RESULTS):
            with open(NEW_EMAIL_SENDING_RESULTS, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    tracking_id = row.get('ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ID', '').strip()
                    if tracking_id:
                        valid_tracking_ids.add(tracking_id)

        # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        cleaned_records = []
        seen_combinations = set()

        for record in original_records:
            tracking_id = record.get('tracking_id', '').strip()
            opened_at = record.get('opened_at', '').strip()

            # åŸºæœ¬çš„ãªæ¤œè¨¼
            if not tracking_id or not opened_at:
                continue

            # æœ‰åŠ¹ãªãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDã‹ãƒã‚§ãƒƒã‚¯
            if tracking_id not in valid_tracking_ids:
                continue

            # ç•°å¸¸ãªæ™‚åˆ»ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå…¨ã¦:10ã§çµ‚ã‚ã‚‹ã®ã¯ä¸è‡ªç„¶ï¼‰
            if opened_at.endswith(':10'):
                continue

            # æ·±å¤œæ™‚é–“å¸¯ï¼ˆ0-5æ™‚ï¼‰ã®é–‹å°ã¯ç–‘ã‚ã—ã„
            try:
                open_time = datetime.datetime.strptime(opened_at, '%Y-%m-%d %H:%M:%S')
                if 0 <= open_time.hour <= 5:
                    continue
            except:
                continue

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜tracking_idã¨æ™‚åˆ»ã®çµ„ã¿åˆã‚ã›ï¼‰
            combination_key = f"{tracking_id}_{opened_at}"
            if combination_key in seen_combinations:
                continue

            seen_combinations.add(combination_key)
            cleaned_records.append(record)

        cleaned_count = len(cleaned_records)
        removed_count = original_count - cleaned_count

        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ
        backup_file = f"{NEW_EMAIL_OPEN_TRACKING}.backup_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
        import shutil
        shutil.copy2(NEW_EMAIL_OPEN_TRACKING, backup_file)

        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        with open(NEW_EMAIL_OPEN_TRACKING, 'w', encoding='utf-8-sig', newline='') as f:
            if cleaned_records:
                fieldnames = cleaned_records[0].keys()
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(cleaned_records)
            else:
                # ç©ºã®å ´åˆã¯ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã¿
                fieldnames = ['tracking_id', 'opened_at', 'ip_address', 'device_type', 'user_agent']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()

        logger.info(f"é–‹å°è¿½è·¡ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {original_count}ä»¶ â†’ {cleaned_count}ä»¶ (å‰Šé™¤: {removed_count}ä»¶)")

        return {
            'success': True,
            'message': f'ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†',
            'original_count': original_count,
            'cleaned_count': cleaned_count,
            'removed_count': removed_count,
            'backup_file': backup_file
        }

    except Exception as e:
        logger.error(f"é–‹å°è¿½è·¡ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'success': False,
            'message': f'ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {str(e)}',
            'original_count': 0,
            'cleaned_count': 0,
            'removed_count': 0
        }

def remove_duplicate_open_records():
    """é‡è¤‡é–‹å°è¨˜éŒ²ã‚’é™¤å»ï¼ˆåŒã˜tracking_idã®æœ€åˆã®é–‹å°ã®ã¿ä¿æŒï¼‰"""
    try:
        if not os.path.exists(NEW_EMAIL_OPEN_TRACKING):
            return {
                'success': False,
                'message': 'é–‹å°è¿½è·¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
            }

        # ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        records = []
        with open(NEW_EMAIL_OPEN_TRACKING, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                records.append(row)

        original_count = len(records)

        # é‡è¤‡é™¤å»ï¼ˆtracking_idã”ã¨ã«æœ€åˆã®é–‹å°ã®ã¿ä¿æŒï¼‰
        unique_records = []
        seen_tracking_ids = set()

        # æ™‚åˆ»é †ã«ã‚½ãƒ¼ãƒˆ
        records.sort(key=lambda x: x.get('opened_at', ''))

        for record in records:
            tracking_id = record.get('tracking_id', '').strip()
            if tracking_id and tracking_id not in seen_tracking_ids:
                seen_tracking_ids.add(tracking_id)
                unique_records.append(record)

        unique_count = len(unique_records)
        removed_count = original_count - unique_count

        # çµæœã‚’ä¿å­˜
        with open(NEW_EMAIL_OPEN_TRACKING, 'w', encoding='utf-8-sig', newline='') as f:
            if unique_records:
                fieldnames = unique_records[0].keys()
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(unique_records)

        logger.info(f"é‡è¤‡é–‹å°è¨˜éŒ²é™¤å»å®Œäº†: {original_count}ä»¶ â†’ {unique_count}ä»¶ (å‰Šé™¤: {removed_count}ä»¶)")

        return {
            'success': True,
            'message': f'é‡è¤‡é™¤å»å®Œäº†: {removed_count}ä»¶å‰Šé™¤',
            'original_count': original_count,
            'unique_count': unique_count,
            'removed_count': removed_count
        }

    except Exception as e:
        logger.error(f"é‡è¤‡é–‹å°è¨˜éŒ²é™¤å»ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'success': False,
            'message': f'é‡è¤‡é™¤å»ã‚¨ãƒ©ãƒ¼: {str(e)}'
        }

# é–‹å°ç‡ç®¡ç†ç”¨ã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

@app.route('/api/open-rate-stats')
def api_open_rate_stats():
    """é–‹å°ç‡çµ±è¨ˆAPI"""
    try:
        stats = get_comprehensive_open_rate_stats()
        return jsonify(stats)
    except Exception as e:
        logger.error(f"é–‹å°ç‡çµ±è¨ˆAPI ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'error': str(e)}), 500

# ä¼æ¥­ç®¡ç†æ©Ÿèƒ½ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

@app.route('/company-management')
def company_management_page():
    """ä¼æ¥­ç®¡ç†ãƒšãƒ¼ã‚¸ï¼ˆçµ±åˆç‰ˆï¼‰"""
    try:
        return render_template('company_management.html')
    except Exception as e:
        logger.error(f"ä¼æ¥­ç®¡ç†ãƒšãƒ¼ã‚¸è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", 500

# æ—§ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ
@app.route('/add-company')
def add_company_redirect():
    """æ—§ä¼æ¥­è¿½åŠ ãƒšãƒ¼ã‚¸ã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ"""
    return redirect('/company-management')

@app.route('/csv-import')
def csv_import_redirect():
    """æ—§CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒšãƒ¼ã‚¸ã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ"""
    return redirect('/company-management')

@app.route('/api/add-company', methods=['POST'])
def api_add_company():
    """ä¼æ¥­è¿½åŠ API"""
    try:
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        data = request.get_json()

        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ¤œè¨¼
        required_fields = ['company_name', 'website', 'job_position']
        for field in required_fields:
            if not data.get(field):
                return jsonify({'success': False, 'error': f'{field}ã¯å¿…é ˆã§ã™'}), 400

        company_name = data['company_name'].strip()
        website = data['website'].strip()
        email_address = data.get('email_address', '').strip()
        job_position = data['job_position'].strip()

        # ğŸ†• ä¸å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã¨ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®ä¸¡æ–¹ãŒç©ºï¼‰
        email_empty = not email_address or email_address.strip() in ['', 'æœªç™»éŒ²', '-', 'â€']
        website_empty = not website or website.strip() in ['', 'â€', '-']

        if email_empty and website_empty:
            return jsonify({
                'success': False,
                'error': 'ä¸å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã¨ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®å°‘ãªãã¨ã‚‚ä¸€æ–¹ã¯å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚'
            }), 400

        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        validation_result = validate_company_data(company_name, website, email_address)
        if not validation_result['valid']:
            return jsonify({'success': False, 'error': validation_result['error']}), 400

        # æ–°ã—ã„ä¼æ¥­IDã‚’ç”Ÿæˆ
        new_id = get_next_company_id()

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½åŠ 
        success = add_company_to_csv(new_id, company_name, website, email_address, job_position)

        if success:
            logger.info(f"æ–°ã—ã„ä¼æ¥­ã‚’è¿½åŠ ã—ã¾ã—ãŸ: ID={new_id}, ä¼æ¥­å={company_name}")

            # ğŸ†• ä¼æ¥­è¿½åŠ æˆåŠŸæ™‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ï¼ˆå³æ™‚åæ˜ ã®ãŸã‚ï¼‰
            logger.info("ä¼æ¥­è¿½åŠ æˆåŠŸ - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™")
            clear_all_caches()
            logger.info("ä¼æ¥­è¿½åŠ å¾Œã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å®Œäº†")

            return jsonify({
                'success': True,
                'message': f'ä¼æ¥­ã€Œ{company_name}ã€ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼ˆID: {new_id}ï¼‰',
                'company_id': new_id
            })
        else:
            return jsonify({'success': False, 'error': 'CSVãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®è¿½åŠ ã«å¤±æ•—ã—ã¾ã—ãŸ'}), 500

    except Exception as e:
        logger.error(f"ä¼æ¥­è¿½åŠ API ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'success': False, 'error': f'ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

# æ—§CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒšãƒ¼ã‚¸ã¯ /company-management ã«çµ±åˆã•ã‚Œã¾ã—ãŸ

@app.route('/api/csv-import', methods=['POST'])
def api_csv_import():
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆAPI"""
    logger.info("CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆAPIå‘¼ã³å‡ºã—é–‹å§‹")
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        if 'csv_file' not in request.files:
            return jsonify({'success': False, 'error': 'CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“'}), 400

        file = request.files['csv_file']
        if file.filename == '':
            return jsonify({'success': False, 'error': 'ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“'}), 400

        # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ãƒã‚§ãƒƒã‚¯
        if not file.filename.lower().endswith('.csv'):
            return jsonify({'success': False, 'error': 'CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã§ã™'}), 400

        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        filename = secure_filename(file.filename)
        temp_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å…ˆ: {temp_path}")
        file.save(temp_path)
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {filename}")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æã¨ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«è§£æé–‹å§‹: {temp_path}")
        preview_result = analyze_csv_file(temp_path)
        logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«è§£æçµæœ: {preview_result.get('success', False)}")

        if not preview_result['success']:
            logger.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«è§£æå¤±æ•—: {preview_result}")
            os.remove(temp_path)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            return jsonify(preview_result), 400

        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’çµæœã«å«ã‚ã‚‹
        preview_result['temp_file'] = temp_path
        logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«è§£ææˆåŠŸ: ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿={len(preview_result.get('preview_data', []))}è¡Œ")

        return jsonify(preview_result)

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆAPI ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±: {error_details}")
        return jsonify({'success': False, 'error': f'ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}', 'details': error_details}), 500

@app.route('/api/csv-import-confirm', methods=['POST'])
def api_csv_import_confirm():
    """CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºå®šAPI"""
    try:
        data = request.get_json()
        temp_file = data.get('temp_file')
        skip_duplicates = data.get('skip_duplicates', True)

        if not temp_file or not os.path.exists(temp_file):
            return jsonify({'success': False, 'error': 'ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}), 400

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        import_result = import_companies_from_csv(temp_file, skip_duplicates)

        # çµæœã®æ§‹é€ ã‚’ç¢ºèªã—ã¦ãƒ­ã‚°å‡ºåŠ›
        logger.info(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆçµæœ: success={import_result.get('success')}, total_processed={import_result.get('total_processed')}, added={import_result.get('added')}")

        # ğŸ†• ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸæ™‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ï¼ˆå³æ™‚åæ˜ ã®ãŸã‚ï¼‰
        if import_result.get('success') and import_result.get('added', 0) > 0:
            logger.info(f"CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ: {import_result.get('added')}ç¤¾è¿½åŠ  - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™")
            clear_all_caches()
            logger.info("CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆå¾Œã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å®Œäº†")

        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        try:
            os.remove(temp_file)
        except:
            pass

        return jsonify(import_result)

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºå®šAPI ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±: {error_details}")
        return jsonify({'success': False, 'error': f'ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}', 'details': error_details}), 500

@app.route('/api/import-newdata', methods=['POST'])
def api_import_newdata():
    """data/newdata.csvã‚’ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹API"""
    try:
        newdata_path = 'data/newdata.csv'

        if not os.path.exists(newdata_path):
            return jsonify({'success': False, 'error': 'newdata.csvãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}), 404

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        import_result = import_companies_from_csv(newdata_path, skip_duplicates=True)

        # ğŸ†• ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸæ™‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ï¼ˆå³æ™‚åæ˜ ã®ãŸã‚ï¼‰
        if import_result.get('success') and import_result.get('added', 0) > 0:
            logger.info(f"newdata.csvã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ: {import_result.get('added')}ç¤¾è¿½åŠ  - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™")
            clear_all_caches()
            logger.info("newdata.csvã‚¤ãƒ³ãƒãƒ¼ãƒˆå¾Œã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å®Œäº†")

        return jsonify(import_result)

    except Exception as e:
        logger.error(f"newdata.csvã‚¤ãƒ³ãƒãƒ¼ãƒˆAPI ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'success': False, 'error': f'ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

def validate_company_data(company_name, website, email_address):
    """ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯é™¤å¤–ç‰ˆï¼‰"""
    try:
        # ä¼æ¥­åã®æ¤œè¨¼
        if not company_name or len(company_name.strip()) < 2:
            return {'valid': False, 'error': 'ä¼æ¥­åã¯2æ–‡å­—ä»¥ä¸Šã§å…¥åŠ›ã—ã¦ãã ã•ã„'}

        # ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆURLã®åŸºæœ¬æ¤œè¨¼
        if website and website not in ['â€', '-', '']:
            # HTTPãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®è‡ªå‹•è¿½åŠ 
            if not website.startswith(('http://', 'https://')):
                website = 'https://' + website

            # åŸºæœ¬çš„ãªURLå½¢å¼ãƒã‚§ãƒƒã‚¯ï¼ˆæ­£è¦è¡¨ç¾ã‚’ä½¿ã‚ãªã„ï¼‰
            if '.' not in website or ' ' in website:
                return {'valid': False, 'error': 'ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆURLã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“'}

        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®åŸºæœ¬æ¤œè¨¼
        if email_address and email_address.strip() and email_address.strip() not in ['â€', '-', '']:
            # åŸºæœ¬çš„ãªãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹å½¢å¼ãƒã‚§ãƒƒã‚¯ï¼ˆæ­£è¦è¡¨ç¾ã‚’ä½¿ã‚ãªã„ï¼‰
            if '@' not in email_address or '.' not in email_address.split('@')[-1]:
                return {'valid': False, 'error': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“'}

        return {'valid': True, 'error': None}

    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return {'valid': False, 'error': f'æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}'}

def is_company_duplicate(company_name, website):
    """
    ä¼æ¥­ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
    å„ªå…ˆé †ä½:
    1. ãƒ‰ãƒ¡ã‚¤ãƒ³ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæœ‰åŠ¹ãªURLãŒã‚ã‚‹å ´åˆï¼‰
    2. ä¼æ¥­åã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆURLãŒãªã„å ´åˆã®ã¿ï¼‰
    """
    try:
        logger.debug(f"é‡è¤‡ãƒã‚§ãƒƒã‚¯é–‹å§‹: ä¼æ¥­å='{company_name}', ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ='{website}'")

        # æ–°è¦ä¼æ¥­ã®URLãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        new_has_valid_url = (website and
                           website not in ['â€', '-', ''] and
                           website.startswith(('http://', 'https://')))

        with open(INPUT_FILE, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                existing_name = row.get('ä¼æ¥­å', '').strip()
                existing_website = row.get('ä¼æ¥­ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸', '').strip()

                # æ—¢å­˜ä¼æ¥­ã®URLãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’åˆ¤å®š
                existing_has_valid_url = (existing_website and
                                        existing_website not in ['â€', '-', ''] and
                                        existing_website.startswith(('http://', 'https://')))

                # ã‚±ãƒ¼ã‚¹1: ä¸¡æ–¹ã¨ã‚‚æœ‰åŠ¹ãªURLã‚’æŒã¤å ´åˆ â†’ ãƒ‰ãƒ¡ã‚¤ãƒ³ã§æ¯”è¼ƒ
                if new_has_valid_url and existing_has_valid_url:
                    existing_domain = extract_domain(existing_website)
                    new_domain = extract_domain(website)
                    if existing_domain == new_domain and existing_domain != '':
                        logger.debug(f"ãƒ‰ãƒ¡ã‚¤ãƒ³é‡è¤‡æ¤œå‡º: '{existing_domain}' == '{new_domain}' (ä¼æ¥­: {existing_name} vs {company_name})")
                        return True

                # ã‚±ãƒ¼ã‚¹2: æ–°è¦ä¼æ¥­ã«URLãŒãªã„å ´åˆ â†’ ä¼æ¥­åã§æ¯”è¼ƒ
                elif not new_has_valid_url:
                    if existing_name and company_name and existing_name.lower().strip() == company_name.lower().strip():
                        logger.debug(f"ä¼æ¥­åé‡è¤‡æ¤œå‡ºï¼ˆæ–°è¦ä¼æ¥­URLãªã—ï¼‰: '{existing_name}' == '{company_name}'")
                        return True

                # ã‚±ãƒ¼ã‚¹3: æ—¢å­˜ä¼æ¥­ã«URLãŒãªãã€æ–°è¦ä¼æ¥­ã«URLãŒã‚ã‚‹å ´åˆ â†’ ä¼æ¥­åã§æ¯”è¼ƒ
                elif new_has_valid_url and not existing_has_valid_url:
                    if existing_name and company_name and existing_name.lower().strip() == company_name.lower().strip():
                        logger.debug(f"ä¼æ¥­åé‡è¤‡æ¤œå‡ºï¼ˆæ—¢å­˜ä¼æ¥­URLãªã—ï¼‰: '{existing_name}' == '{company_name}'")
                        return True

        logger.debug(f"é‡è¤‡ãªã—: ä¼æ¥­å='{company_name}', URLæœ‰åŠ¹={new_has_valid_url}")
        return False

    except Exception as e:
        logger.error(f"é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def extract_domain(url):
    """URLã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†ã‚’æŠ½å‡º"""
    try:
        # æ­£è¦è¡¨ç¾ã‚’ä½¿ã‚ãªã„æ–¹æ³•ã§ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æŠ½å‡º
        domain = url

        # http://ã‚„https://ã‚’é™¤å»
        if domain.startswith('https://'):
            domain = domain[8:]
        elif domain.startswith('http://'):
            domain = domain[7:]

        # www.ã‚’é™¤å»
        if domain.startswith('www.'):
            domain = domain[4:]

        # ãƒ‘ã‚¹éƒ¨åˆ†ã‚’é™¤å»
        domain = domain.split('/')[0]

        return domain.lower()
    except Exception as e:
        logger.error(f"ãƒ‰ãƒ¡ã‚¤ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return url.lower()

def get_next_company_id():
    """æ¬¡ã®ä¼æ¥­IDã‚’å–å¾—"""
    try:
        max_id = 0
        with open(INPUT_FILE, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                try:
                    company_id = int(row.get('ID', 0))
                    max_id = max(max_id, company_id)
                except ValueError:
                    continue

        return max_id + 1

    except Exception as e:
        logger.error(f"æ¬¡ã®IDå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return 1

def add_company_to_csv(company_id, company_name, website, email_address, job_position):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã«æ–°ã—ã„ä¼æ¥­ã‚’è¿½åŠ """
    try:
        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒç©ºã®å ´åˆã¯ã€Œâ€ã€ã«è¨­å®š
        if not email_address:
            email_address = 'â€'

        # æ–°ã—ã„è¡Œã®ãƒ‡ãƒ¼ã‚¿
        new_row = [
            company_id,
            company_name,
            website,
            email_address,
            job_position,
            '',  # ãƒã‚¦ãƒ³ã‚¹çŠ¶æ…‹
            '',  # ãƒã‚¦ãƒ³ã‚¹æ—¥æ™‚
            '',  # ãƒã‚¦ãƒ³ã‚¹ç†ç”±
            '',  # é…ä¿¡åœæ­¢çŠ¶æ…‹
            '',  # é…ä¿¡åœæ­¢æ—¥æ™‚
            ''   # é…ä¿¡åœæ­¢ç†ç”±
        ]

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜
        with open(INPUT_FILE, 'a', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(new_row)

        logger.info(f"ä¼æ¥­ã‚’CSVã«è¿½åŠ : ID={company_id}, ä¼æ¥­å={company_name}")
        return True

    except Exception as e:
        logger.error(f"CSVè¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def analyze_csv_file(file_path):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã¦ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    try:
        # è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚’è©¦è¡Œ
        encodings = ['utf-8-sig', 'utf-8', 'shift_jis', 'cp932', 'iso-2022-jp']
        sample_lines = []

        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    # æœ€åˆã®æ•°è¡Œã‚’èª­ã‚“ã§æ§‹é€ ã‚’ç¢ºèª
                    reader = csv.reader(f)
                    for i, row in enumerate(reader):
                        sample_lines.append(row)
                        if i >= 10:  # æœ€åˆã®11è¡Œï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ + 10è¡Œï¼‰
                            break
                logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°={encoding}")
                break
            except UnicodeDecodeError:
                continue
            except Exception as e:
                logger.warning(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° {encoding} ã§ã®èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
                continue
        else:
            return {'success': False, 'error': 'ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ'}

        if not sample_lines:
            return {'success': False, 'error': 'CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™'}

        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®ç¢ºèª
        header = sample_lines[0] if sample_lines else []
        data_rows = sample_lines[1:] if len(sample_lines) > 1 else []

        # åˆ—æ•°ã®ç¢ºèª
        if len(header) < 3:
            return {'success': False, 'error': 'CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯æœ€ä½3åˆ—ï¼ˆä¼æ¥­åã€ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã€å‹Ÿé›†è·ç¨®ï¼‰ãŒå¿…è¦ã§ã™'}

        # åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ã®æ¨å®š
        column_mapping = estimate_column_mapping(header)

        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        quality_check = check_data_quality(data_rows, column_mapping)

        # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
        preview_data = []
        for i, row in enumerate(data_rows[:5]):  # æœ€åˆã®5è¡Œã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            if len(row) >= len(header):
                preview_data.append({
                    'row_number': i + 2,  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤ã„ãŸè¡Œç•ªå·
                    'company_name': row[column_mapping['company_name']] if column_mapping['company_name'] is not None else '',
                    'website': row[column_mapping['website']] if column_mapping['website'] is not None else '',
                    'email': row[column_mapping['email']] if column_mapping['email'] is not None else '',
                    'job_position': row[column_mapping['job_position']] if column_mapping['job_position'] is not None else ''
                })

        return {
            'success': True,
            'header': header,
            'column_mapping': column_mapping,
            'preview_data': preview_data,
            'total_rows': len(data_rows),
            'quality_check': quality_check
        }

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"CSVè§£æã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±: {error_details}")
        return {'success': False, 'error': f'CSVãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}', 'details': error_details}

def estimate_column_mapping(header):
    """ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ¨å®š"""
    mapping = {
        'company_name': None,
        'website': None,
        'email': None,
        'job_position': None
    }

    logger.debug(f"ãƒ˜ãƒƒãƒ€ãƒ¼è§£æé–‹å§‹: {header}")

    # åˆ—åã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
    for i, col_name in enumerate(header):
        col_lower = col_name.lower()
        logger.debug(f"åˆ— {i}: '{col_name}' -> '{col_lower}'")

        # ä¼æ¥­åã®æ¤œå‡º
        if any(keyword in col_lower for keyword in ['ä¼æ¥­å', 'company', 'ä¼šç¤¾å', 'ç¤¾å']):
            mapping['company_name'] = i
            logger.debug(f"ä¼æ¥­ååˆ—æ¤œå‡º: {i}")

        # ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®æ¤œå‡º
        elif any(keyword in col_lower for keyword in ['url', 'website', 'ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ', 'ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸', 'ã‚µã‚¤ãƒˆ']):
            mapping['website'] = i
            logger.debug(f"ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ—æ¤œå‡º: {i}")

        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®æ¤œå‡º
        elif any(keyword in col_lower for keyword in ['email', 'mail', 'ãƒ¡ãƒ¼ãƒ«', 'ã‚¢ãƒ‰ãƒ¬ã‚¹', 'æ‹…å½“è€…']):
            mapping['email'] = i
            logger.debug(f"ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹åˆ—æ¤œå‡º: {i}")

        # å‹Ÿé›†è·ç¨®ã®æ¤œå‡º
        elif any(keyword in col_lower for keyword in ['è·ç¨®', 'job', 'position', 'å‹Ÿé›†', 'æ±‚äºº']):
            mapping['job_position'] = i
            logger.debug(f"å‹Ÿé›†è·ç¨®åˆ—æ¤œå‡º: {i}")

    # å¿…é ˆé …ç›®ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ä½ç½®ã§æ¨å®š
    if mapping['company_name'] is None and len(header) > 0:
        mapping['company_name'] = 0
        logger.debug(f"ä¼æ¥­ååˆ—ã‚’ä½ç½®ã§æ¨å®š: 0")
    if mapping['website'] is None and len(header) > 1:
        mapping['website'] = 1
        logger.debug(f"ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ—ã‚’ä½ç½®ã§æ¨å®š: 1")
    if mapping['email'] is None and len(header) > 2:
        mapping['email'] = 2
        logger.debug(f"ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹åˆ—ã‚’ä½ç½®ã§æ¨å®š: 2")
    if mapping['job_position'] is None and len(header) > 3:
        mapping['job_position'] = 3
        logger.debug(f"å‹Ÿé›†è·ç¨®åˆ—ã‚’ä½ç½®ã§æ¨å®š: 3")

    logger.debug(f"æœ€çµ‚ãƒãƒƒãƒ”ãƒ³ã‚°: {mapping}")
    return mapping

def check_data_quality(data_rows, column_mapping):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’ãƒã‚§ãƒƒã‚¯"""
    quality = {
        'total_rows': len(data_rows),
        'valid_companies': 0,
        'valid_websites': 0,
        'valid_emails': 0,
        'empty_rows': 0,
        'warnings': []
    }

    for row in data_rows:
        if not row or all(cell.strip() == '' for cell in row):
            quality['empty_rows'] += 1
            continue

        # ä¼æ¥­åãƒã‚§ãƒƒã‚¯
        if column_mapping['company_name'] is not None and len(row) > column_mapping['company_name']:
            company_name = row[column_mapping['company_name']].strip()
            if company_name:
                quality['valid_companies'] += 1

        # ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆãƒã‚§ãƒƒã‚¯
        if column_mapping['website'] is not None and len(row) > column_mapping['website']:
            website = row[column_mapping['website']].strip()
            if website and ('http' in website or '.' in website):
                quality['valid_websites'] += 1

        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒã‚§ãƒƒã‚¯
        if column_mapping['email'] is not None and len(row) > column_mapping['email']:
            email = row[column_mapping['email']].strip()
            if email and '@' in email:
                quality['valid_emails'] += 1

    # è­¦å‘Šã®ç”Ÿæˆ
    if quality['valid_companies'] < quality['total_rows'] * 0.8:
        quality['warnings'].append('ä¼æ¥­åãŒä¸å®Œå…¨ãªè¡ŒãŒå¤šãå«ã¾ã‚Œã¦ã„ã¾ã™')

    if quality['valid_websites'] < quality['total_rows'] * 0.5:
        quality['warnings'].append('ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆURLãŒä¸å®Œå…¨ãªè¡ŒãŒå¤šãå«ã¾ã‚Œã¦ã„ã¾ã™')

    if quality['empty_rows'] > 0:
        quality['warnings'].append(f'{quality["empty_rows"]}è¡Œã®ç©ºè¡ŒãŒå«ã¾ã‚Œã¦ã„ã¾ã™')

    return quality

def import_companies_from_csv(file_path, skip_duplicates=True):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
    try:
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æ
        analysis_result = analyze_csv_file(file_path)
        if not analysis_result['success']:
            return analysis_result

        column_mapping = analysis_result['column_mapping']

        # ç¾åœ¨ã®æœ€å¤§IDã‚’å–å¾—
        current_max_id = get_next_company_id() - 1

        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆçµæœã®åˆæœŸåŒ–
        import_stats = {
            'success': True,
            'total_processed': 0,
            'added': 0,
            'skipped': 0,
            'errors': 0,
            'details': [],
            'new_companies': []
        }

        # è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨å‡¦ç†
        encodings = ['utf-8-sig', 'utf-8', 'shift_jis', 'cp932', 'iso-2022-jp']

        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    reader = csv.reader(f)
                    header = next(reader, None)  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—

                    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒæˆåŠŸã—ãŸå ´åˆã€å‡¦ç†ã‚’ç¶šè¡Œ
                    logger.info(f"CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆ: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°={encoding}ã§èª­ã¿è¾¼ã¿æˆåŠŸ")
                    break
            except UnicodeDecodeError:
                continue
            except Exception as e:
                logger.warning(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° {encoding} ã§ã®èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
                continue
        else:
            return {'success': False, 'error': 'ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ'}

        # æˆåŠŸã—ãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§å†åº¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã„ã¦å‡¦ç†
        with open(file_path, 'r', encoding=encoding) as f:
            reader = csv.reader(f)
            header = next(reader, None)  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—

            for row_num, row in enumerate(reader, start=2):  # è¡Œç•ªå·ã¯2ã‹ã‚‰é–‹å§‹ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼é™¤ãï¼‰
                import_stats['total_processed'] += 1

                try:
                    # ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºï¼ˆå®‰å…¨ãªå–å¾—ï¼‰
                    def safe_get_column(row, mapping_key, default=''):
                        """å®‰å…¨ã«åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
                        try:
                            col_index = column_mapping.get(mapping_key)
                            if col_index is not None and len(row) > col_index:
                                return row[col_index].strip()
                            return default
                        except Exception as e:
                            logger.error(f"åˆ—ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ ({mapping_key}): {e}")
                            return default

                    company_name = safe_get_column(row, 'company_name')
                    website = safe_get_column(row, 'website')
                    email_address = safe_get_column(row, 'email')
                    job_position = safe_get_column(row, 'job_position')

                    logger.debug(f"è¡Œ {row_num} ãƒ‡ãƒ¼ã‚¿æŠ½å‡º: ä¼æ¥­å='{company_name}', URL='{website}', ãƒ¡ãƒ¼ãƒ«='{email_address}', è·ç¨®='{job_position}'")

                    # å¿…é ˆé …ç›®ã®ãƒã‚§ãƒƒã‚¯
                    if not company_name or not job_position:
                        import_stats['errors'] += 1
                        import_stats['details'].append({
                            'row': row_num,
                            'status': 'error',
                            'message': 'å¿…é ˆé …ç›®ï¼ˆä¼æ¥­åã€å‹Ÿé›†è·ç¨®ï¼‰ãŒä¸è¶³ã—ã¦ã„ã¾ã™'
                        })
                        continue

                    # ğŸ†• ä¸å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã¨ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®ä¸¡æ–¹ãŒç©ºï¼‰
                    email_empty = not email_address or email_address.strip() in ['', 'æœªç™»éŒ²', '-', 'â€']
                    website_empty = not website or website.strip() in ['', 'â€', '-']

                    if email_empty and website_empty:
                        import_stats['errors'] += 1
                        import_stats['details'].append({
                            'row': row_num,
                            'status': 'excluded',
                            'company_name': company_name,
                            'message': 'ä¸å®Œå…¨ãƒ‡ãƒ¼ã‚¿: ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã¨ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®ä¸¡æ–¹ãŒç©ºã®ãŸã‚é™¤å¤–ã•ã‚Œã¾ã—ãŸ'
                        })
                        logger.info(f"ä¸å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–: {company_name} (è¡Œ {row_num})")
                        continue

                    # ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆãŒç©ºã¾ãŸã¯ã€Œâ€ã€ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                    if not website or website in ['â€', '-', '']:
                        website = 'â€'

                    # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒã€Œâ€ã€ã®å ´åˆã¯ç©ºæ–‡å­—åˆ—ã¨ã—ã¦æ‰±ã†ï¼ˆæ¤œè¨¼ã‚¨ãƒ©ãƒ¼ã‚’å›é¿ï¼‰
                    if email_address in ['â€', '-']:
                        email_address = ''

                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if skip_duplicates and is_company_duplicate(company_name, website):
                        import_stats['skipped'] += 1
                        import_stats['details'].append({
                            'row': row_num,
                            'status': 'skipped',
                            'company_name': company_name,
                            'message': 'é‡è¤‡ä¼æ¥­ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ'
                        })
                        continue

                    # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
                    validation_result = validate_company_data(company_name, website, email_address)
                    if not validation_result['valid']:
                        import_stats['errors'] += 1
                        import_stats['details'].append({
                            'row': row_num,
                            'status': 'error',
                            'company_name': company_name,
                            'website': website,
                            'email': email_address,
                            'message': validation_result['error']
                        })
                        logger.warning(f"è¡Œ {row_num} æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {validation_result['error']}, ä¼æ¥­å='{company_name}', URL='{website}', ãƒ¡ãƒ¼ãƒ«='{email_address}'")
                        continue

                    # æ–°ã—ã„IDã‚’ç”Ÿæˆ
                    current_max_id += 1
                    new_id = current_max_id

                    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½åŠ ï¼ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒç©ºã®å ´åˆã¯ã€Œâ€ã€ã«æˆ»ã™ï¼‰
                    csv_email_address = email_address if email_address else 'â€'
                    success = add_company_to_csv(new_id, company_name, website, csv_email_address, job_position)

                    if success:
                        import_stats['added'] += 1
                        import_stats['new_companies'].append({
                            'id': new_id,
                            'name': company_name,
                            'website': website,
                            'job_position': job_position
                        })
                        import_stats['details'].append({
                            'row': row_num,
                            'status': 'added',
                            'company_name': company_name,
                            'company_id': new_id,
                            'message': f'ä¼æ¥­ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼ˆID: {new_id}ï¼‰'
                        })
                    else:
                        import_stats['errors'] += 1
                        import_stats['details'].append({
                            'row': row_num,
                            'status': 'error',
                            'company_name': company_name,
                            'message': 'CSVãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®è¿½åŠ ã«å¤±æ•—ã—ã¾ã—ãŸ'
                        })

                except Exception as e:
                    import_stats['errors'] += 1
                    import_stats['details'].append({
                        'row': row_num,
                        'status': 'error',
                        'message': f'å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}',
                        'raw_data': row[:4] if len(row) >= 4 else row,  # ãƒ‡ãƒãƒƒã‚°ç”¨
                        'column_mapping': column_mapping,  # ãƒ‡ãƒãƒƒã‚°ç”¨
                        'company_name': row[column_mapping['company_name']] if column_mapping['company_name'] is not None and len(row) > column_mapping['company_name'] else 'ä¸æ˜'
                    })
                    logger.error(f"è¡Œ {row_num} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}, ãƒ‡ãƒ¼ã‚¿: {row[:4] if len(row) >= 4 else row}, åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°: {column_mapping}")

        # ğŸ†• ä¸å®Œå…¨ãƒ‡ãƒ¼ã‚¿é™¤å¤–æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        excluded_count = len([d for d in import_stats['details'] if d.get('status') == 'excluded'])
        import_stats['incomplete_excluded'] = excluded_count

        # çµæœã®ã‚µãƒãƒªãƒ¼
        if excluded_count > 0:
            import_stats['message'] = f'ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†: {import_stats["added"]}ç¤¾è¿½åŠ , {import_stats["skipped"]}ç¤¾ã‚¹ã‚­ãƒƒãƒ—, {excluded_count}ç¤¾é™¤å¤–ï¼ˆä¸å®Œå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰, {import_stats["errors"]}ä»¶ã‚¨ãƒ©ãƒ¼'
        else:
            import_stats['message'] = f'ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†: {import_stats["added"]}ç¤¾è¿½åŠ , {import_stats["skipped"]}ç¤¾ã‚¹ã‚­ãƒƒãƒ—, {import_stats["errors"]}ä»¶ã‚¨ãƒ©ãƒ¼'

        logger.info(f"CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†: {import_stats['message']}")
        return import_stats

    except Exception as e:
        logger.error(f"CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'success': False,
            'error': f'ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'
        }

@app.route('/api/daily-open-rates')
def api_daily_open_rates():
    """æ—¥åˆ¥é–‹å°ç‡API"""
    try:
        days = request.args.get('days', 30, type=int)
        stats = get_daily_open_rate_stats(days)
        return jsonify(stats)
    except Exception as e:
        logger.error(f"æ—¥åˆ¥é–‹å°ç‡API ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/unopened-emails')
def api_unopened_emails():
    """æœªé–‹å°ãƒ¡ãƒ¼ãƒ«API"""
    try:
        days_threshold = request.args.get('days', 7, type=int)
        limit = request.args.get('limit', 50, type=int)

        unopened = get_unopened_emails_list(days_threshold)
        return jsonify(unopened[:limit])
    except Exception as e:
        logger.error(f"æœªé–‹å°ãƒ¡ãƒ¼ãƒ«API ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/data-integrity-check')
def api_data_integrity_check():
    """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯API"""
    try:
        integrity_check = check_data_integrity()
        return jsonify(integrity_check)
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯API ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/bounce-open-inconsistency')
def api_bounce_open_inconsistency():
    """ãƒã‚¦ãƒ³ã‚¹ãƒ»é–‹å°çŸ›ç›¾ãƒ‡ãƒ¼ã‚¿API"""
    try:
        summary = get_bounce_open_inconsistency_summary()
        return jsonify(summary)
    except Exception as e:
        logger.error(f"ãƒã‚¦ãƒ³ã‚¹ãƒ»é–‹å°çŸ›ç›¾ãƒ‡ãƒ¼ã‚¿API ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/clean-open-tracking-data')
def api_clean_open_tracking_data():
    """é–‹å°è¿½è·¡ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—API"""
    try:
        result = clean_open_tracking_data()
        return jsonify(result)
    except Exception as e:
        logger.error(f"é–‹å°è¿½è·¡ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—API ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/remove-duplicate-opens')
def api_remove_duplicate_opens():
    """é‡è¤‡é–‹å°è¨˜éŒ²é™¤å»API"""
    try:
        result = remove_duplicate_open_records()
        return jsonify(result)
    except Exception as e:
        logger.error(f"é‡è¤‡é–‹å°è¨˜éŒ²é™¤å»API ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/daily-open-rate-stats')
def api_daily_open_rate_stats():
    """æ—¥åˆ¥é–‹å°ç‡çµ±è¨ˆAPI"""
    try:
        days = request.args.get('days', 30, type=int)
        daily_stats = get_daily_open_rate_stats(days)
        return jsonify(daily_stats)
    except Exception as e:
        logger.error(f"æ—¥åˆ¥é–‹å°ç‡çµ±è¨ˆAPI ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'error': str(e)}), 500

def start_high_quality_process(command, args, description):
    """é«˜å“è³ªä¿®å¾©ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ã™ã‚‹é–¢æ•°"""
    try:
        import subprocess
        import uuid

        # ãƒ—ãƒ­ã‚»ã‚¹IDã‚’ç”Ÿæˆ
        process_id = str(uuid.uuid4())[:8]

        # ã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰
        if command.endswith('.py'):
            full_command = f'python {command} {args}'
        else:
            full_command = f'{command} {args}'

        # ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ï¼ˆå‡ºåŠ›ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ç„¡åŠ¹åŒ–ï¼‰
        process = subprocess.Popen(
            full_command,
            shell=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,  # æ¨™æº–ã‚¨ãƒ©ãƒ¼ã‚’æ¨™æº–å‡ºåŠ›ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ
            text=True,
            bufsize=0,  # ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ç„¡åŠ¹åŒ–
            universal_newlines=True,
            cwd=os.getcwd()
        )

        # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±ã‚’ä¿å­˜
        process_info = {
            'id': process_id,
            'pid': process.pid,
            'command': command,
            'args': args,
            'description': description,
            'start_time': datetime.datetime.now(),
            'status': 'running',
            'process': process
        }

        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒ­ã‚»ã‚¹ãƒªã‚¹ãƒˆã«è¿½åŠ 
        running_processes[process_id] = process_info

        logger.info(f"é«˜å“è³ªãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹: {description} (ID: {process_id}, PID: {process.pid})")

        return process_id

    except Exception as e:
        logger.error(f"é«˜å“è³ªãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
        raise

# é«˜å“è³ªä¿®å¾©å‡¦ç†API
@app.route('/api/high_quality_repair', methods=['POST'])
def high_quality_repair():
    """é«˜å“è³ªä¿®å¾©å‡¦ç†ã‚’å®Ÿè¡Œ"""
    try:
        data = request.get_json()
        repair_type = data.get('repair_type')
        start_id = data.get('start_id', 2004)
        end_id = data.get('end_id', 2100)
        test_mode = data.get('test_mode', False)

        # ä¿®å¾©ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ã‚³ãƒãƒ³ãƒ‰ã‚’æ±ºå®š
        if repair_type == 'staged':
            # æ®µéšçš„ä¿®å¾©ï¼ˆ10ç¤¾ãšã¤ï¼‰
            command = 'high_quality_integrated_workflow.py'
            args = f'--start-id {start_id} --end-id {end_id} --batch-size 10 --timeout 600'
            description = f'é«˜å“è³ªæ®µéšçš„ä¿®å¾© (ID {start_id}-{end_id}, 10ç¤¾ãšã¤)'
        elif repair_type == 'ultra_safe':
            # è¶…å®‰å…¨ä¿®å¾©ï¼ˆ3ç¤¾ãšã¤ï¼‰
            command = 'high_quality_integrated_workflow.py'
            args = f'--start-id {start_id} --end-id {end_id} --batch-size 3 --timeout 600'
            description = f'é«˜å“è³ªè¶…å®‰å…¨ä¿®å¾© (ID {start_id}-{end_id}, 3ç¤¾ãšã¤)'
        elif repair_type == 'quality_test':
            # å“è³ªãƒ†ã‚¹ãƒˆï¼ˆID 2001-2003ï¼‰
            command = 'high_quality_integrated_workflow.py'
            args = '--start-id 2001 --end-id 2003 --batch-size 3 --timeout 600 --test-mode'
            description = 'é«˜å“è³ªãƒ†ã‚¹ãƒˆ (ID 2001-2003)'
        elif repair_type == 'email_extraction':
            # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºã®ã¿
            command = 'high_quality_email_extractor.py'
            args = f'--start-id {start_id} --end-id {end_id} --input-file new_input_utf8.csv --timeout 45 --retries 5'
            description = f'é«˜å“è³ªãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡º (ID {start_id}-{end_id})'
        elif repair_type == 'website_analysis':
            # ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æã®ã¿
            command = 'high_quality_website_analyzer.py'
            args = f'--start-id {start_id} --end-id {end_id} --input-file new_input_utf8.csv --timeout 60 --retries 3'
            description = f'é«˜å“è³ªã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ†æ (ID {start_id}-{end_id})'
        elif repair_type == 'consolidate':
            # çµæœçµ±åˆ
            command = 'python'
            args = '-c "import consolidate_high_quality_results; consolidate_high_quality_results.main()"'
            description = 'é«˜å“è³ªå‡¦ç†çµæœçµ±åˆ'
        else:
            return jsonify({
                'success': False,
                'message': f'ä¸æ˜ãªä¿®å¾©ã‚¿ã‚¤ãƒ—: {repair_type}'
            }), 400

        # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯å¼•æ•°ã«è¿½åŠ 
        if test_mode and repair_type in ['staged', 'ultra_safe']:
            args += ' --test-mode'

        # ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹
        process_id = start_high_quality_process(command, args, description)

        return jsonify({
            'success': True,
            'message': f'{description}ã‚’é–‹å§‹ã—ã¾ã—ãŸ',
            'process_id': process_id
        })

    except Exception as e:
        logger.error(f"é«˜å“è³ªä¿®å¾©å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'message': f'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'
        }), 500

@app.route('/api/high_quality_status')
def high_quality_status():
    """é«˜å“è³ªä¿®å¾©å‡¦ç†ã®çŠ¶æ³ã‚’å–å¾—"""
    try:
        import glob

        # é«˜å“è³ªå‡¦ç†çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
        email_files = glob.glob('high_quality_email_extraction_results_id20*.csv')
        website_files = glob.glob('high_quality_website_analysis_results_id20*.csv')

        # å‡¦ç†æ¸ˆã¿ä¼æ¥­IDã‚’å–å¾—
        processed_ids = set()

        for file in email_files + website_files:
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ä¼æ¥­IDç¯„å›²ã‚’æŠ½å‡º
            import re
            match = re.search(r'id(\d+)-(\d+)', file)
            if match:
                start_id = int(match.group(1))
                end_id = int(match.group(2))
                processed_ids.update(range(start_id, end_id + 1))

        # ä¼æ¥­ID 2001-2100ã®å‡¦ç†çŠ¶æ³ã‚’ç¢ºèª
        total_companies = 100  # 2001-2100
        processed_companies = len([id for id in processed_ids if 2001 <= id <= 2100])
        remaining_companies = total_companies - processed_companies

        # æ¬¡ã«å‡¦ç†ã™ã¹ãç¯„å›²ã‚’è¨ˆç®—
        next_start_id = 2001
        for id in range(2001, 2101):
            if id not in processed_ids:
                next_start_id = id
                break

        return jsonify({
            'success': True,
            'total_companies': total_companies,
            'processed_companies': processed_companies,
            'remaining_companies': remaining_companies,
            'progress_percent': round((processed_companies / total_companies) * 100, 1),
            'next_start_id': next_start_id,
            'processed_files': {
                'email_extraction': len(email_files),
                'website_analysis': len(website_files)
            }
        })

    except Exception as e:
        logger.error(f"é«˜å“è³ªä¿®å¾©çŠ¶æ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'message': f'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'
        }), 500

@app.route('/api/high_quality_logs')
def high_quality_logs():
    """é«˜å“è³ªä¿®å¾©å‡¦ç†ã®ãƒ­ã‚°ã‚’å–å¾—"""
    try:
        # é«˜å“è³ªå‡¦ç†é–¢é€£ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        log_files = [
            'high_quality_integrated_workflow.log',
            'high_quality_email_extraction.log',
            'high_quality_website_analysis.log',
            'consolidate_high_quality_results.log'
        ]

        logs = []
        for log_file in log_files:
            if os.path.exists(log_file):
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                        # æœ€æ–°ã®50è¡Œã‚’å–å¾—
                        recent_lines = lines[-50:] if len(lines) > 50 else lines
                        for line in recent_lines:
                            if line.strip():
                                logs.append({
                                    'timestamp': datetime.datetime.now().strftime('%H:%M:%S'),
                                    'source': log_file,
                                    'message': line.strip()
                                })
                except Exception as e:
                    logger.error(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {log_file}: {e}")

        # æ™‚ç³»åˆ—ã§ã‚½ãƒ¼ãƒˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
        logs = logs[-100:]  # æœ€æ–°100è¡Œã«åˆ¶é™

        return jsonify({
            'success': True,
            'logs': logs
        })

    except Exception as e:
        logger.error(f"é«˜å“è³ªãƒ­ã‚°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'message': f'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'
        }), 500

def check_port_availability(port=5001):
    """ãƒãƒ¼ãƒˆã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    import socket
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind(('127.0.0.1', port))
            return True
    except OSError:
        return False

def find_available_port(start_port=5001, max_attempts=10):
    """åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒˆã‚’è¦‹ã¤ã‘ã‚‹"""
    for port in range(start_port, start_port + max_attempts):
        if check_port_availability(port):
            return port
    return None

def cleanup_existing_processes():
    """æ—¢å­˜ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ—ãƒ­ã‚»ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    import psutil
    try:
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                if proc.info['name'] and 'python' in proc.info['name'].lower():
                    cmdline = ' '.join(proc.info['cmdline']) if proc.info['cmdline'] else ''
                    if 'new_dashboard.py' in cmdline:
                        logger.warning(f"æ—¢å­˜ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ‚äº†ã—ã¾ã™ (PID: {proc.info['pid']})")
                        proc = psutil.Process(proc.info['pid'])
                        proc.terminate()
                        proc.wait(timeout=5)
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired):
                continue
    except Exception as e:
        logger.warning(f"ãƒ—ãƒ­ã‚»ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

def save_pid_file(port):
    """PIDãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
    try:
        import os
        pid_data = {
            'pid': os.getpid(),
            'port': port,
            'start_time': datetime.datetime.now().isoformat(),
            'url': f'http://127.0.0.1:{port}/'
        }
        with open('new_dashboard.pid', 'w', encoding='utf-8') as f:
            json.dump(pid_data, f, ensure_ascii=False, indent=2)
        logger.info(f"PIDãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {pid_data}")
    except Exception as e:
        logger.warning(f"PIDãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

# HUGANJOBå°‚ç”¨APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ãƒ†ã‚¹ãƒˆãƒ¡ãƒ¼ãƒ«é€ä¿¡API - å‰Šé™¤æ¸ˆã¿ï¼ˆæœ¬ç•ªé€ä¿¡ã®ã¿ä½¿ç”¨ï¼‰

@app.route('/api/huganjob/send', methods=['POST'])
def api_huganjob_send():
    """HUGANJOB çµ±åˆãƒ¡ãƒ¼ãƒ«é€ä¿¡APIï¼ˆhuganjob_unified_sender.pyå°‚ç”¨ï¼‰"""
    try:
        data = request.get_json()
        start_id = data.get('start_id', 1)
        end_id = data.get('end_id', 10)

        # huganjob_unified_sender.pyå°‚ç”¨ã‚³ãƒãƒ³ãƒ‰
        command = 'huganjob_unified_sender.py'
        args = f'--start-id {start_id} --end-id {end_id} --email-format html_only'

        logger.info(f"HUGANJOBçµ±åˆãƒ¡ãƒ¼ãƒ«é€ä¿¡é–‹å§‹: ID {start_id}-{end_id}")

        process_id = run_process(command, args)
        if process_id:
            return jsonify({
                'success': True,
                'message': f'ID {start_id}-{end_id} ã®ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚’é–‹å§‹ã—ã¾ã—ãŸ',
                'process_id': process_id,
                'range': f'{start_id}-{end_id}',
                'command': f'python {command} {args}'
            })
        else:
            return jsonify({'success': False, 'message': 'ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã®é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸ'})

    except Exception as e:
        logger.error(f"HUGANJOBãƒ¡ãƒ¼ãƒ«é€ä¿¡API ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'success': False, 'message': str(e)})

@app.route('/api/huganjob/progress')
def api_huganjob_progress():
    """HUGANJOBçµ±åˆé€ä¿¡ã®é€²è¡ŒçŠ¶æ³ã‚’å–å¾—"""
    try:
        # å®Ÿè¡Œä¸­ã®HUGANJOBçµ±åˆé€ä¿¡ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ¤œç´¢
        huganjob_processes = []
        for process_id, process_info in running_processes.items():
            command = process_info.get('command', '')
            if 'huganjob_unified_sender' in command:
                progress_info = analyze_huganjob_progress(process_info)
                huganjob_processes.append({
                    'process_id': process_id,
                    'status': process_info.get('status', 'unknown'),
                    'start_time': process_info.get('start_time', '').strftime('%Y-%m-%d %H:%M:%S') if process_info.get('start_time') else 'N/A',
                    'duration': str(datetime.datetime.now() - process_info['start_time']).split('.')[0] if process_info.get('start_time') else 'N/A',
                    'progress': progress_info
                })

        return jsonify({
            'success': True,
            'active_processes': len(huganjob_processes),
            'processes': huganjob_processes,
            'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })

    except Exception as e:
        logger.error(f"HUGANJOBé€²è¡ŒçŠ¶æ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'message': f'ã‚¨ãƒ©ãƒ¼: {e}',
            'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })

@app.route('/api/huganjob/active_processes')
def api_huganjob_active_processes():
    """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªHUGANJOBé€ä¿¡ãƒ—ãƒ­ã‚»ã‚¹ä¸€è¦§"""
    try:
        active_processes = []

        # å®Ÿè¡Œä¸­ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰æ¤œç´¢
        for process_id, process_info in running_processes.items():
            command = process_info.get('command', '')
            if 'huganjob_unified_sender' in command:
                progress_info = analyze_huganjob_progress(process_info)
                active_processes.append({
                    'process_id': process_id,
                    'command': command,
                    'status': process_info.get('status', 'unknown'),
                    'start_time': process_info.get('start_time', '').strftime('%Y-%m-%d %H:%M:%S') if process_info.get('start_time') else 'N/A',
                    'duration': str(datetime.datetime.now() - process_info['start_time']).split('.')[0] if process_info.get('start_time') else 'N/A',
                    'progress': progress_info
                })

        return jsonify({
            'success': True,
            'count': len(active_processes),
            'processes': active_processes
        })

    except Exception as e:
        logger.error(f"ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ã‚»ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'success': False, 'message': f'ã‚¨ãƒ©ãƒ¼: {e}'})

@app.route('/api/get_active_processes')
def api_get_active_processes():
    """ä¸€èˆ¬çš„ãªã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ã‚»ã‚¹å–å¾—APIï¼ˆé€²è¡ŒçŠ¶æ³è¡¨ç¤ºç”¨ï¼‰"""
    try:
        active_processes = []

        for process_id, process_info in running_processes.items():
            active_processes.append({
                'id': process_id,
                'command': process_info.get('command', ''),
                'status': process_info.get('status', 'unknown'),
                'start_time': process_info.get('start_time', '').strftime('%Y-%m-%d %H:%M:%S') if process_info.get('start_time') else 'N/A',
                'duration': str(datetime.datetime.now() - process_info['start_time']).split('.')[0] if process_info.get('start_time') else 'N/A',
                'description': process_info.get('description', process_info.get('command', ''))
            })

        return jsonify(active_processes)

    except Exception as e:
        logger.error(f"ä¸€èˆ¬ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ã‚»ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify([])









# é…ä¿¡åœæ­¢ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
@app.route('/unsubscribe-management')
def unsubscribe_management():
    """é…ä¿¡åœæ­¢ç®¡ç†ãƒšãƒ¼ã‚¸"""
    try:
        # é…ä¿¡åœæ­¢ãƒ­ã‚°ã‚’èª­ã¿è¾¼ã¿
        unsubscribe_log_path = 'data/huganjob_unsubscribe_log.csv'
        unsubscribed_companies = []

        if os.path.exists(unsubscribe_log_path):
            with open(unsubscribe_log_path, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                unsubscribed_companies = list(reader)

        # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
        total_unsubscribed = len(unsubscribed_companies)

        # ç”³è«‹å…ƒåˆ¥çµ±è¨ˆ
        source_stats = {}
        for entry in unsubscribed_companies:
            source = entry.get('ç”³è«‹å…ƒ', 'unknown')
            source_stats[source] = source_stats.get(source, 0) + 1

        # æœ€è¿‘ã®é…ä¿¡åœæ­¢ï¼ˆç›´è¿‘10ä»¶ï¼‰
        recent_unsubscribes = sorted(
            unsubscribed_companies,
            key=lambda x: x.get('é…ä¿¡åœæ­¢æ—¥æ™‚', ''),
            reverse=True
        )[:10]

        # ç·ä¼æ¥­æ•°ã‚’å–å¾—
        total_companies = 0
        if os.path.exists('data/new_input_test.csv'):
            with open('data/new_input_test.csv', 'r', encoding='utf-8-sig') as f:
                reader = csv.reader(f)
                total_companies = sum(1 for _ in reader) - 1

        # é…ä¿¡åœæ­¢ç‡ã‚’è¨ˆç®—
        unsubscribe_rate = (total_unsubscribed / total_companies * 100) if total_companies > 0 else 0

        return render_template(
            'unsubscribe_management.html',
            unsubscribed_companies=unsubscribed_companies,
            recent_unsubscribes=recent_unsubscribes,
            total_unsubscribed=total_unsubscribed,
            total_companies=total_companies,
            unsubscribe_rate=unsubscribe_rate,
            source_stats=source_stats,
            last_updated=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        )

    except Exception as e:
        logger.error(f"é…ä¿¡åœæ­¢ç®¡ç†ãƒšãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e}")
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", 500

@app.route('/api/unsubscribe/process', methods=['POST'])
def api_process_unsubscribe():
    """é…ä¿¡åœæ­¢å‡¦ç†API"""
    try:
        # huganjob_unsubscribe_manager.pyã‚’å®Ÿè¡Œ
        import subprocess
        result = subprocess.run(
            ['python', 'huganjob_unsubscribe_manager.py', '--process'],
            capture_output=True,
            text=True,
            encoding='utf-8'
        )

        if result.returncode == 0:
            return jsonify({
                'success': True,
                'message': 'é…ä¿¡åœæ­¢å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ',
                'output': result.stdout
            })
        else:
            return jsonify({
                'success': False,
                'message': 'é…ä¿¡åœæ­¢å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ',
                'error': result.stderr
            })

    except Exception as e:
        logger.error(f"é…ä¿¡åœæ­¢å‡¦ç†API ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'message': f'å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}'
        }), 500

@app.route('/api/unsubscribe/check', methods=['POST'])
def api_check_unsubscribe():
    """é…ä¿¡åœæ­¢çŠ¶æ³ç¢ºèªAPI"""
    try:
        data = request.get_json()
        email = data.get('email', '').strip()

        if not email:
            return jsonify({
                'success': False,
                'message': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“'
            }), 400

        # é…ä¿¡åœæ­¢ãƒ­ã‚°ã‚’ç¢ºèª
        unsubscribe_log_path = 'data/huganjob_unsubscribe_log.csv'
        is_unsubscribed = False
        unsubscribe_info = None

        if os.path.exists(unsubscribe_log_path):
            with open(unsubscribe_log_path, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for entry in reader:
                    if entry.get('ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', '').lower().strip() == email.lower():
                        is_unsubscribed = True
                        unsubscribe_info = entry
                        break

        return jsonify({
            'success': True,
            'is_unsubscribed': is_unsubscribed,
            'unsubscribe_info': unsubscribe_info
        })

    except Exception as e:
        logger.error(f"é…ä¿¡åœæ­¢ç¢ºèªAPI ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'message': f'ç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}'
        }), 500

# Google Sheetsç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åˆ¶å¾¡
@app.route('/sheets-monitor')
def sheets_monitor_page():
    """Google Sheetsç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åˆ¶å¾¡ãƒšãƒ¼ã‚¸"""
    try:
        # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®çŠ¶æ…‹ç¢ºèª
        monitor_status = {
            'is_running': False,
            'last_check': None,
            'processed_count': 0,
            'credentials_configured': False
        }

        # èªè¨¼æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
        credentials_path = 'config/google_sheets_credentials.json'
        if os.path.exists(credentials_path):
            try:
                with open(credentials_path, 'r', encoding='utf-8') as f:
                    creds = json.load(f)
                if creds.get('type') == 'service_account' and 'your-project' not in creds.get('project_id', ''):
                    monitor_status['credentials_configured'] = True
            except:
                pass

        # å‡¦ç†æ¸ˆã¿ã‚¨ãƒ³ãƒˆãƒªæ•°ã®ç¢ºèª
        processed_file = 'data/huganjob_sheets_processed.json'
        if os.path.exists(processed_file):
            try:
                with open(processed_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    monitor_status['processed_count'] = len(data.get('processed_hashes', []))
                    monitor_status['last_check'] = data.get('last_updated')
            except:
                pass

        # å‡¦ç†ä¸å¯ã‚¨ãƒ³ãƒˆãƒªã®ç¢ºèª
        unprocessable_entries = []
        unprocessable_file = 'data/huganjob_unprocessable_entries.csv'
        if os.path.exists(unprocessable_file):
            try:
                with open(unprocessable_file, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    unprocessable_entries = list(reader)
            except:
                pass

        return render_template(
            'sheets_monitor.html',
            monitor_status=monitor_status,
            unprocessable_entries=unprocessable_entries,
            last_updated=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        )

    except Exception as e:
        logger.error(f"Google Sheetsç›£è¦–ãƒšãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e}")
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", 500

@app.route('/api/sheets-monitor/test', methods=['POST'])
def api_sheets_monitor_test():
    """Google Sheetsç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆAPI"""
    try:
        import subprocess
        result = subprocess.run(
            ['python', 'huganjob_google_sheets_monitor.py', '--test'],
            capture_output=True,
            text=True,
            encoding='utf-8'
        )

        return jsonify({
            'success': result.returncode == 0,
            'output': result.stdout,
            'error': result.stderr
        })

    except Exception as e:
        logger.error(f"Google Sheetsç›£è¦–ãƒ†ã‚¹ãƒˆAPI ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'message': f'ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}'
        }), 500

@app.route('/api/sheets-monitor/setup', methods=['POST'])
def api_sheets_monitor_setup():
    """Google Sheetsç›£è¦–ã‚·ã‚¹ãƒ†ãƒ è¨­å®šAPI"""
    try:
        import subprocess
        result = subprocess.run(
            ['python', 'setup_google_sheets_api.py', '--all'],
            capture_output=True,
            text=True,
            encoding='utf-8'
        )

        return jsonify({
            'success': result.returncode == 0,
            'output': result.stdout,
            'error': result.stderr
        })

    except Exception as e:
        logger.error(f"Google Sheetsç›£è¦–è¨­å®šAPI ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'message': f'è¨­å®šã‚¨ãƒ©ãƒ¼: {str(e)}'
        }), 500

@app.route('/api/huganjob/stats')
def api_huganjob_stats():
    """HUGANJOBçµ±è¨ˆæƒ…å ±API"""
    try:
        # HUGANJOBå°‚ç”¨çµ±è¨ˆã‚’å–å¾—
        stats = {
            'total_companies': 0,
            'email_resolved': 0,
            'emails_sent': 0,
            'delivery_success': 0,
            'bounced': 0,
            'unsubscribed': 0,
            'success_rate': 0.0,
            'unsubscribe_rate': 0.0
        }

        # new_input_test.csvã‹ã‚‰ä¼æ¥­æ•°ã‚’å–å¾—
        if os.path.exists('data/new_input_test.csv'):
            try:
                with open('data/new_input_test.csv', 'r', encoding='utf-8-sig') as f:
                    reader = csv.reader(f)
                    stats['total_companies'] = sum(1 for _ in reader) - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼é™¤ã
            except Exception as e:
                logger.error(f"ä¼æ¥­æ•°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                stats['total_companies'] = 0

        # é€ä¿¡çµæœã‹ã‚‰è©³ç´°çµ±è¨ˆã‚’å–å¾—
        if os.path.exists('new_email_sending_results.csv'):
            try:
                with open('new_email_sending_results.csv', 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    results = list(reader)

                # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªä¼æ¥­IDã‚’å–å¾—
                unique_companies = set()
                result_counts = {}

                for result in results:
                    try:
                        company_id = int(result['ä¼æ¥­ID'])
                        unique_companies.add(company_id)

                        send_result = result.get('é€ä¿¡çµæœ', 'unknown')
                        result_counts[send_result] = result_counts.get(send_result, 0) + 1
                    except:
                        continue

                stats['emails_sent'] = len(unique_companies)
                stats['delivery_success'] = result_counts.get('success', 0)
                stats['bounced'] = result_counts.get('bounced', 0)
                stats['unsubscribed'] = result_counts.get('unsubscribed', 0)

                # æˆåŠŸç‡ã‚’è¨ˆç®—
                if stats['emails_sent'] > 0:
                    stats['success_rate'] = (stats['delivery_success'] / stats['emails_sent']) * 100

            except Exception as e:
                logger.error(f"é€ä¿¡çµæœçµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

        # é…ä¿¡åœæ­¢æ•°ã‚’å–å¾—ï¼ˆãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚‚ç¢ºèªï¼‰
        unsubscribe_log_path = 'data/huganjob_unsubscribe_log.csv'
        if os.path.exists(unsubscribe_log_path):
            try:
                with open(unsubscribe_log_path, 'r', encoding='utf-8-sig') as f:
                    reader = csv.reader(f)
                    unsubscribe_count = sum(1 for _ in reader) - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼é™¤ã
                # é€ä¿¡çµæœã¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€å¤§å€¤ã‚’ä½¿ç”¨
                stats['unsubscribed'] = max(stats['unsubscribed'], unsubscribe_count)
            except Exception as e:
                logger.error(f"é…ä¿¡åœæ­¢ãƒ­ã‚°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

        # é…ä¿¡åœæ­¢ç‡ã‚’è¨ˆç®—
        if stats['total_companies'] > 0:
            stats['unsubscribe_rate'] = (stats['unsubscribed'] / stats['total_companies']) * 100

        # é€ä¿¡å±¥æ­´ã‹ã‚‰ã‚‚çµ±è¨ˆã‚’è£œå®Œ
        try:
            with open('huganjob_sending_history.json', 'r', encoding='utf-8') as f:
                history = json.load(f)

            history_count = len(history.get('sending_records', []))
            # é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã¨é€ä¿¡å±¥æ­´ã®æœ€å¤§å€¤ã‚’ä½¿ç”¨
            stats['emails_sent'] = max(stats['emails_sent'], history_count)

        except Exception as e:
            logger.error(f"é€ä¿¡å±¥æ­´èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        # å®Ÿè³ªçš„ãªæˆåŠŸç‡ã‚’è¨ˆç®—ï¼ˆé…ä¿¡åœæ­¢ã¨ãƒã‚¦ãƒ³ã‚¹ã‚’é™¤å¤–ï¼‰
        effective_sent = stats['emails_sent'] - stats['unsubscribed']
        effective_success = effective_sent - stats['bounced']

        if effective_sent > 0:
            stats['success_rate'] = (effective_success / effective_sent) * 100

        # æœ€çµ‚çš„ãªé…ä¿¡æˆåŠŸæ•°ã‚’è¨­å®š
        stats['delivery_success'] = effective_success

        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æ±ºå®šçµæœã‹ã‚‰çµ±è¨ˆã‚’å–å¾—
        if os.path.exists('huganjob_email_resolution_results.csv'):
            try:
                import pandas as pd
                df = pd.read_csv('huganjob_email_resolution_results.csv', encoding='utf-8-sig')
                stats['email_resolved'] = len(df[df['æ±ºå®šãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].notna()])
            except Exception as e:
                logger.warning(f"ãƒ¡ãƒ¼ãƒ«æ±ºå®šçµæœãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        return jsonify({
            'success': True,
            'stats': stats,
            'last_updated': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })

    except Exception as e:
        logger.error(f"HUGANJOBçµ±è¨ˆæƒ…å ±API ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'success': False, 'message': str(e)})

if __name__ == '__main__':
    import argparse

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ
    parser = argparse.ArgumentParser(description='HUGANJOBå–¶æ¥­ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚·ã‚¹ãƒ†ãƒ  ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰')
    parser.add_argument('--port', type=int, default=5002, help='ãƒãƒ¼ãƒˆç•ªå· (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5002)')
    parser.add_argument('--host', default='127.0.0.1', help='ãƒ›ã‚¹ãƒˆã‚¢ãƒ‰ãƒ¬ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 127.0.0.1)')
    parser.add_argument('--debug', action='store_true', help='ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•')
    parser.add_argument('--cleanup', action='store_true', help='æ—¢å­˜ãƒ—ãƒ­ã‚»ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¦ã‹ã‚‰èµ·å‹•')
    parser.add_argument('--auto-port', action='store_true', help='åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒˆã‚’è‡ªå‹•æ¤œç´¢')
    args = parser.parse_args()

    try:
        # åˆæœŸåŒ–å‡¦ç†
        logger.info("=" * 60)
        logger.info("HUGANJOBå–¶æ¥­ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚·ã‚¹ãƒ†ãƒ  ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰èµ·å‹•ä¸­...")
        logger.info("=" * 60)

        ensure_directories()
        initialize_config_files()

        # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèªï¼ˆè»½é‡åŒ–ï¼‰
        if not os.path.exists(INPUT_FILE):
            logger.error("âŒ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            sys.exit(1)

        logger.info("âœ… å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¾ã—ãŸ")

        # ãƒ—ãƒ­ã‚»ã‚¹å±¥æ­´ã‚’èª­ã¿è¾¼ã¿ï¼ˆé…å»¶èª­ã¿è¾¼ã¿å¯¾å¿œï¼‰
        if not STARTUP_LAZY_LOADING:
            load_process_history()
        else:
            logger.info("âš¡ é…å»¶èª­ã¿è¾¼ã¿ãƒ¢ãƒ¼ãƒ‰: ãƒ—ãƒ­ã‚»ã‚¹å±¥æ­´ã¯å¿…è¦æ™‚ã«èª­ã¿è¾¼ã¿ã¾ã™")

        # æ—¢å­˜ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if args.cleanup:
            logger.info("ğŸ§¹ æ—¢å­˜ãƒ—ãƒ­ã‚»ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¦ã„ã¾ã™...")
            cleanup_existing_processes()

        # ãƒãƒ¼ãƒˆã®ç¢ºèªã¨è¨­å®š
        target_port = args.port
        if args.auto_port:
            available_port = find_available_port(target_port)
            if available_port:
                target_port = available_port
                logger.info(f"ğŸ” åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒˆã‚’ç™ºè¦‹: {target_port}")
            else:
                logger.error(f"âŒ åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (é–‹å§‹ãƒãƒ¼ãƒˆ: {args.port})")
                sys.exit(1)
        elif not check_port_availability(target_port):
            logger.error(f"âŒ ãƒãƒ¼ãƒˆ {target_port} ã¯æ—¢ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™")
            logger.info("ğŸ’¡ --auto-port ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€--cleanup ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§æ—¢å­˜ãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ‚äº†ã—ã¦ãã ã•ã„")
            sys.exit(1)

        # PIDãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        save_pid_file(target_port)

        # HUGANJOBèµ·å‹•ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        logger.info("ğŸš€ HUGANJOBãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’èµ·å‹•ã—ã¾ã™...")
        logger.info(f"ğŸ“Š ã‚¢ã‚¯ã‚»ã‚¹URL: http://{args.host}:{target_port}/")
        logger.info(f"ğŸ”§ ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: {'æœ‰åŠ¹' if args.debug else 'ç„¡åŠ¹'}")
        logger.info(f"âš¡ é…å»¶èª­ã¿è¾¼ã¿: {'æœ‰åŠ¹' if STARTUP_LAZY_LOADING else 'ç„¡åŠ¹'}")
        logger.info(f"ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {CACHE_TIMEOUT_SECONDS}ç§’")
        logger.info(f"ğŸ“§ HUGANJOBå–¶æ¥­ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚·ã‚¹ãƒ†ãƒ å°‚ç”¨")
        logger.info("=" * 60)

        # å®šæœŸçš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’é–‹å§‹
        def periodic_memory_cleanup():
            """å®šæœŸçš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
            import time
            while True:
                time.sleep(300)  # 5åˆ†é–“éš”
                try:
                    optimize_memory()
                except Exception as e:
                    logger.warning(f"å®šæœŸãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")

        cleanup_thread = threading.Thread(target=periodic_memory_cleanup, daemon=True)
        cleanup_thread.start()
        logger.info("ğŸ§¹ å®šæœŸãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’é–‹å§‹ã—ã¾ã—ãŸï¼ˆ5åˆ†é–“éš”ï¼‰")

        # Flaskã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•
        app.run(
            host=args.host,
            port=target_port,
            debug=args.debug,
            threaded=True,
            use_reloader=False  # ãƒªãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ç„¡åŠ¹åŒ–ã—ã¦ãƒ—ãƒ­ã‚»ã‚¹é‡è¤‡ã‚’é˜²ã
        )

    except KeyboardInterrupt:
        logger.info("ğŸ‘‹ HUGANJOBãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãŒæ‰‹å‹•ã§åœæ­¢ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"ğŸ’¥ HUGANJOBãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰èµ·å‹•ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)
    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        try:
            if os.path.exists('huganjob_dashboard.pid'):
                os.remove('huganjob_dashboard.pid')
                logger.info("ğŸ§¹ HUGANJOBPIDãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        except:
            pass
