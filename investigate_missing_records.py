#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€ä¿¡è¨˜éŒ²ã®ä¸è¶³èª¿æŸ»ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
1859ç¤¾ã«é€ä¿¡ã—ãŸã¯ãšãŒ1770ç¤¾åˆ†ã—ã‹è¨˜éŒ²ã•ã‚Œã¦ã„ãªã„å•é¡Œã‚’èª¿æŸ»
"""

import csv
import pandas as pd
from collections import defaultdict

def main():
    print("=" * 60)
    print("ğŸ“Š é€ä¿¡è¨˜éŒ²ä¸è¶³èª¿æŸ»")
    print("=" * 60)
    
    # é€ä¿¡çµæœã‚’èª­ã¿è¾¼ã¿
    try:
        df_results = pd.read_csv('new_email_sending_results.csv', encoding='utf-8-sig')
        print(f"âœ… é€ä¿¡è¨˜éŒ²ç·æ•°: {len(df_results)}ä»¶")
        print(f"   æœ€å°ä¼æ¥­ID: {df_results['ä¼æ¥­ID'].min()}")
        print(f"   æœ€å¤§ä¼æ¥­ID: {df_results['ä¼æ¥­ID'].max()}")
        print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªä¼æ¥­IDæ•°: {df_results['ä¼æ¥­ID'].nunique()}")
    except Exception as e:
        print(f"âŒ é€ä¿¡çµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿
    try:
        df_companies = pd.read_csv('data/new_input_test.csv', encoding='utf-8-sig')
        print(f"âœ… ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç·æ•°: {len(df_companies)}ç¤¾")
        print(f"   ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€å°ID: {df_companies['ID'].min()}")
        print(f"   ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€å¤§ID: {df_companies['ID'].max()}")
    except Exception as e:
        print(f"âŒ ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # é€ä¿¡è¨˜éŒ²ã«ãªã„ä¼æ¥­IDã‚’ç‰¹å®š
    sent_ids = set(df_results['ä¼æ¥­ID'].tolist())
    all_ids = set(df_companies['ID'].tolist())
    missing_ids = all_ids - sent_ids
    
    print(f"\nğŸ” åˆ†æçµæœ:")
    print(f"   é€ä¿¡è¨˜éŒ²ã«ãªã„ä¼æ¥­IDæ•°: {len(missing_ids)}ç¤¾")
    
    if len(missing_ids) <= 50:
        print(f"   é€ä¿¡è¨˜éŒ²ã«ãªã„ä¼æ¥­ID: {sorted(missing_ids)}")
    else:
        missing_list = sorted(missing_ids)
        print(f"   é€ä¿¡è¨˜éŒ²ã«ãªã„ä¼æ¥­IDï¼ˆæœ€åˆã®20ä»¶ï¼‰: {missing_list[:20]}")
        print(f"   é€ä¿¡è¨˜éŒ²ã«ãªã„ä¼æ¥­IDï¼ˆæœ€å¾Œã®20ä»¶ï¼‰: {missing_list[-20:]}")
    
    # é€ä¿¡çµæœã®çŠ¶æ³ã‚’åˆ†æ
    print(f"\nğŸ“ˆ é€ä¿¡çµæœã®çŠ¶æ³:")
    result_counts = df_results['é€ä¿¡çµæœ'].value_counts()
    for result, count in result_counts.items():
        print(f"   {result}: {count}ä»¶")
    
    # é€ä¿¡è¨˜éŒ²ã«ãªã„ä¼æ¥­ã®è©³ç´°ã‚’ç¢ºèª
    if len(missing_ids) > 0:
        print(f"\nğŸ“‹ é€ä¿¡è¨˜éŒ²ã«ãªã„ä¼æ¥­ã®è©³ç´°ï¼ˆæœ€åˆã®10ç¤¾ï¼‰:")
        missing_companies = df_companies[df_companies['ID'].isin(sorted(missing_ids)[:10])]
        for _, company in missing_companies.iterrows():
            print(f"   ID {company['ID']}: {company['ä¼æ¥­å']} - {company.get('æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'N/A')}")
    
    # é‡è¤‡é€ä¿¡è¨˜éŒ²ã®ç¢ºèª
    duplicate_ids = df_results[df_results.duplicated(subset=['ä¼æ¥­ID'], keep=False)]
    if len(duplicate_ids) > 0:
        print(f"\nâš ï¸ é‡è¤‡é€ä¿¡è¨˜éŒ²:")
        print(f"   é‡è¤‡ã—ã¦ã„ã‚‹ä¼æ¥­IDæ•°: {duplicate_ids['ä¼æ¥­ID'].nunique()}ç¤¾")
        for company_id in duplicate_ids['ä¼æ¥­ID'].unique()[:5]:
            duplicates = df_results[df_results['ä¼æ¥­ID'] == company_id]
            print(f"   ID {company_id}: {len(duplicates)}å›é€ä¿¡")
    
    # é€ä¿¡å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç¢ºèª
    print(f"\nğŸ“ é€ä¿¡å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª:")
    try:
        import json
        with open('huganjob_sending_history.json', 'r', encoding='utf-8') as f:
            history = json.load(f)
        
        history_records = history.get('sending_records', [])
        print(f"   é€ä¿¡å±¥æ­´è¨˜éŒ²æ•°: {len(history_records)}ä»¶")
        
        if history_records:
            history_ids = set()
            for record in history_records:
                try:
                    history_ids.add(int(record['company_id']))
                except:
                    continue
            
            print(f"   é€ä¿¡å±¥æ­´ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ä¼æ¥­IDæ•°: {len(history_ids)}ç¤¾")
            
            # é€ä¿¡å±¥æ­´ã«ã‚ã‚‹ãŒé€ä¿¡çµæœã«ãªã„ä¼æ¥­ID
            history_only = history_ids - sent_ids
            if history_only:
                print(f"   é€ä¿¡å±¥æ­´ã«ã‚ã‚‹ãŒé€ä¿¡çµæœã«ãªã„ä¼æ¥­IDæ•°: {len(history_only)}ç¤¾")
                if len(history_only) <= 20:
                    print(f"   è©²å½“ä¼æ¥­ID: {sorted(history_only)}")
    
    except Exception as e:
        print(f"   é€ä¿¡å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ä»–ã®é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç¢ºèª
    print(f"\nğŸ“‚ ãã®ä»–ã®é€ä¿¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª:")
    import glob
    import os
    
    other_files = []
    for pattern in ['sent_emails_record_*.csv', '*sending_results*.csv', '*email_results*.csv']:
        other_files.extend(glob.glob(pattern))
    
    for file_path in other_files:
        if file_path != 'new_email_sending_results.csv' and os.path.exists(file_path):
            try:
                df_other = pd.read_csv(file_path, encoding='utf-8-sig')
                print(f"   {file_path}: {len(df_other)}ä»¶")
                
                # ä¼æ¥­IDãŒã‚ã‚‹å ´åˆã¯è©³ç´°ç¢ºèª
                if 'ä¼æ¥­ID' in df_other.columns:
                    other_ids = set(df_other['ä¼æ¥­ID'].tolist())
                    overlap_with_main = len(sent_ids & other_ids)
                    unique_to_other = len(other_ids - sent_ids)
                    print(f"     ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã®é‡è¤‡: {overlap_with_main}ä»¶")
                    print(f"     ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ç‹¬è‡ª: {unique_to_other}ä»¶")
                    
                    # ä¸è¶³åˆ†ã‚’è£œå®Œã§ãã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if unique_to_other > 0:
                        complementary_ids = (other_ids - sent_ids) & missing_ids
                        if complementary_ids:
                            print(f"     ä¸è¶³åˆ†ã‚’è£œå®Œå¯èƒ½: {len(complementary_ids)}ç¤¾")
                            
            except Exception as e:
                print(f"   {file_path}: èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ - {e}")

if __name__ == "__main__":
    main()
